# FREE CLI Access to the worlds most popular AI Models
Source: https://servicestack.net/posts/okai-chat

As part of the development of [okai](/posts/okai-models) for generating [Blazor CRUD Apps from a text prompt](/posts/text-to-blazor) 
using your preferred AI Models, we've also made available a generic **chat** prompt that can be used as a 
convenient way to conduct personal research against many of the worlds most popular Large Language Models - for Free!

![](/img/posts/okai-chat/okai-chat.webp)

No API Keys, no Signups, no installs, no cost, you can just start immediately using the `npx okai chat` script to ask LLMs 
for assistance:

:::sh
npx okai chat "command to copy a folder with rsync?"
:::

This will use the default model (currently codestral:22b) to answer your question.

### Select Preferred Model

You can also use your preferred model with the `-m <model>` flag with either the model **name** or its **alias**, 
e.g you can use 
[Microsoft's PHI-4 14B](https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090) model with:

:::sh
npx okai -m phi chat "command to copy folder with rsync?"
:::

### List Available Models

We're actively adding more great performing and leading experimental models as they're released. 
You can view the list of available models with `ls models`:

:::sh
npx okai ls models
:::

Which at this time will return the following list of available models along with instructions for how to use them:

```txt
USAGE (5 models max):
a) OKAI_MODELS=codestral,llama3.3,flash
b) okai -models codestral,llama3.3,flash <prompt>
c) okai -m flash chat <prompt>

FREE MODELS:
claude-3-haiku            (alias haiku)
codestral:22b             (alias codestral)
deepseek-r1:32b
deepseek-r1:70b
deepseek-r2:32b
deepseek-v3:671b          (alias deepseek)
gemini-flash-1.5
gemini-flash-1.5-8b       (alias flash-8b)
gemini-flash-2.0          (alias flash)
gemini-flash-lite-2.0     (alias flash-lite)
gemini-flash-thinking-2.0 (alias flash-thinking)
gemini-pro-2.0            (alias gemini-pro)
gemma2:9b                 (alias gemma)
gpt-3.5-turbo             (alias gpt-3.5)
gpt-4o-mini
llama3.1:70b              (alias llama3.1)
llama3.3:70b              (alias llama3.3)
llama3:8b                 (alias llama3)
mistral-nemo:12b          (alias mistral-nemo)
mistral-small:24b         (alias mistral-small)
mistral:7b                (alias mistral)
mixtral:8x22b
mixtral:8x7b              (alias mixtral)
nova-lite
nova-micro
phi-4:14b                 (alias phi,phi-4)
qwen-plus
qwen-turbo
qwen2.5-coder:32b         (alias qwen2.5-coder)
qwen2.5:32b
qwen2.5:72b               (alias qwen2.5)
qwq:32b                   (alias qwq)
qwq:72b

PREMIUM MODELS: *
claude-3-5-haiku
claude-3-5-sonnet
claude-3-7-sonnet         (alias sonnet)
claude-3-sonnet
deepseek-r1:671b          (alias deepseek-r1)
gemini-pro-1.5
gpt-4
gpt-4-turbo
gpt-4o
mistral-large:123b
nova-pro
o1-mini
o1-preview
o3-mini
qwen-max

 * requires valid license:
a) SERVICESTACK_LICENSE=<key>
b) SERVICESTACK_CERTIFICATE=<LC-XXX>
c) okai -models <premium,models> -license <license> <prompt>
```

Where you'll be able to use any of the great performing inexpensive models listed under `FREE MODELS` for Free.
Whilst ServiceStack customers with an active commercial license can also use any of the  more expensive
and better performing models listed under `PREMIUM MODELS` by either:

 a) Setting the `SERVICESTACK_LICENSE` Environment Variable with your **License Key**
 b) Setting the `SERVICESTACK_CERTIFICATE` Variable with your **License Certificate**
 c) Inline using the `-license` flag with either the **License Key** or **Certificate**

### FREE for Personal Usage

To be able to maintain this as a free service we're limiting usage as a tool that developers can use for personal
assistance and research by limiting usage to **60 requests /hour** which should be more than enough for most 
personal usage and research whilst deterring usage in automated tools.

:::tip info
Rate limiting is implemented with a sliding [Token Bucket algorithm](https://en.wikipedia.org/wiki/Token_bucket) 
that replenishes 1 additional request every 60s
:::


# New okai tool for Rapid App Development
Source: https://servicestack.net/posts/okai-models

## AI powered Rapid App Development Workflow

The new `okai` npm tool works similar to the online [Text to Blazor App](/posts/text-to-blazor) generator
except it's a local tool that can add additional functionality to an existing project:

<ascii-cinema src="/img/posts/okai-models/okai-prompt-jobs.cast"
    loop="true" poster="npt:00:20" theme="dracula" rows="24" />

The syntax for adding a new feature to your Web App is `npx okai <prompt>`, e.g:

:::sh
npx okai "The kind of Feature you would like to add"
:::

Where it will generate the Data Models, AutoQuery CRUD APIs, DB Migrations and Admin UI for the 
selected feature which you'll see after selecting the LLM Data Models you want to use, e.g:

```sh
Selected 'deepseek-r1:70b' data models

Saved: /home/mythz/src/MyApp/MyApp.ServiceModel/Jobs.d.ts
Saved: /home/mythz/src/MyApp/MyApp.ServiceModel/Jobs.cs
Saved: /home/mythz/src/MyApp/wwwroot/admin/sections/Jobs.mjs
Saved: /home/mythz/src/MyApp/wwwroot/admin/sections/index.mjs
Saved: /home/mythz/src/MyApp/Migrations/Migration1001.cs

Run 'dotnet run --AppTasks=migrate' to apply new migration and create tables

To regenerate classes, update 'Jobs.d.ts' then run:
$ okai Jobs.d.ts
```

Where okai will generate everything needed to support the feature in your App, including:

- `MyApp.ServiceModel/Jobs.d.ts` - TypeScript Data Models
- `MyApp.ServiceModel/Jobs.cs` - AutoQuery CRUD APIs and Data Models
- `wwwroot/admin/sections/Jobs.mjs` - Admin UI Section
  - requires `blazor-admin` or `blazor-vue` template
- `MyApp/Migrations/Migration1001.cs` - DB Migrations
  - requires project with [OrmLite DB Migrations](https://docs.servicestack.net/ormlite/db-migrations) 

Then to apply the migration and create the tables you can run:

:::sh
npm run migrate
:::

## Declarative AI powered Features

The approach okai uses is very different from most AI tools which instead of using AI to generate an 
entire App or source code for a feature it's only used to generate the initial Data Models within 
a TypeScript Declaration file which we've found is best format supported by AI models that's also the 
best typed DSL for defining data models with minimal syntax that's easy for humans to read and write.

This is possible for ServiceStack Apps since a significant portion of an App's functionality can be
[declaratively applied](https://docs.servicestack.net/locode/declarative) including all 
[AutoQuery CRUD APIs](https://docs.servicestack.net/autoquery/crud) which can be implemented just
using typed Request DTOs to define the shape of the API AutoQuery should implement.

From the Data Models, the rest of the feature is generated using declarative code-first APIs depending
on the template used. 

### Generated Admin UI

To have okai generate an Admin UI you'll need to use it within a new Blazor Admin project or 
Blazor Vue ([blazor-vue](https://blazor-vue.web-templates.io)) project:

:::sh
x new blazor-admin Acme
:::

Which both support a "Modular no-touch" Admin UI which will appear under a new group in the Admin Sidebar:

![](/img/posts/text-to-blazor/okai-blazor-admin.webp)

## Customize Data Models

The data models defined in the TypeScript Declaration file e.g. `Jobs.d.ts` is what drives the
generation of the Data Models, APIs, DB Migrations and Admin UIs.

This can be further customized by editing the TypeScript Declaration file and re-running the `okai` tool
with the name of the TypeScript Declaration file, e.g. `Jobs.d.ts`:

:::sh
npx okai Jobs.d.ts
:::

Which will re-generate the Data Models, APIs, DB Migrations and Admin UIs based on the updated Data Models.

![](/img/posts/text-to-blazor/okai-Employees.webp)

:::tip
You only need to specify the `Jobs.d.ts` TypeScript filename (i.e. not the filepath) from
anywhere within your .NET solution
:::

### Live Code Generation

If you'd prefer to see the generated code in real-time you can add the `--watch` flag to watch the 
TypeScript Declaration file for changes and automatically re-generate the generated files on Save:

:::sh
npx okai Jobs.d.ts --watch
:::

<video autoplay="autoplay" loop="loop" controls>
    <source src="https://media.servicestack.com/videos/okai-watch.mp4" type="video/mp4">
</video>


# Text to Blazor Vue CRUD Apps
Source: https://servicestack.net/posts/text-to-blazor

Text to Blazor is our first initiative for harnessing AI to help to rapidly generate new Blazor Admin CRUD 
Apps from just a text description.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="Bd283EYJKxM" style="background-image: url('https://img.youtube.com/vi/Bd283EYJKxM/maxresdefault.jpg')"></lite-youtube>
</div>

[![](/img/posts/text-to-blazor/text-to-blazor-prompt.webp)](/text-to-blazor)

<div class="pb-4 not-prose flex justify-center">
<a href="https://servicestack.net/text-to-blazor" class="text-3xl text-indigo-600 hover:text-indigo-800">https://servicestack.net/text-to-blazor</a>
</div>

This will query 5 different high quality AI models to generate 5 different Data Models, APIs, DB Migrations 
and Admin UIs which you can browse to find the one that best matches your requirements.

[![](/img/posts/text-to-blazor/text-to-blazor-gen.webp)](/text-to-blazor)

### Using AI to only generate Data Models

Whilst the result is a working CRUD App, the approach taken is very different from most AI tools
which uses AI to generate the entire App that ends up with a whole new code-base developers didn't write
which they'd now need to maintain.

Instead AI is only used to generate the initial Data Models within a **TypeScript Declaration file**
which we've found is the best format supported by AI models that's also the best typed DSL for defining
data models with minimal syntax that's easy for humans to read and write.

### Download preferred Blazor Vue CRUD App

Once you've decided on the Data Models that best matches your requirements, you can download your preferred 
generated Blazor Vue CRUD App:

[![](/img/posts/text-to-blazor/text-to-blazor-download.webp)](/text-to-blazor)

### Blazor Admin App

**Admin Only** - is ideal for internal Admin Apps where the Admin UI is the Primary UI

![](/img/posts/text-to-blazor/okai-blazor-admin.webp)

### Blazor Vue App

**UI + Admin** - Creates a new [blazor-vue](https://blazor-vue.web-templates.io) template that's ideal
for Internet or public facing Apps, sporting a full-featured public facing UI for a Web App's
users whilst enabling a back-office CRUD UI for Admin Users to manage their App's data.

![](/img/posts/text-to-blazor/okai-blazor-vue.webp)

Clicking on the **Admin UI** button will take you to the Admin UI at `/admin`:

![](/img/posts/text-to-blazor/okai-blazor-vue-admin.webp)

## Modular Code Generation

Instead of unleashing AI on your code-base unabated, we're only using AI to generate isolated functionality 
into grouped "no touch" source files that can be easily maintained and extended.

Creating a new Project with a similar prompt above would create a new project with the new source files
(marked with `*`) added to the existing project:

### APIs

```files
/MyApp.ServiceModel
    Bookings.cs
    api.d.ts*
    Employees.cs*
    Employees.d.ts*
```

### Migration

```files
/MyApp/Migrations
    Migration1000.cs
    Migration1001.cs*
```

### UI

```files
/MyApp/wwwroot/admin
    /sections
        Bookings.mjs
        Employees.mjs*
        index.mjs*
    index.html
```

Which after downloading a new project just needs to run the [DB Migrations](https://docs.servicestack.net/ormlite/db-migrations)
to create the tables required for any new functionality:

:::sh
npm run migrate
:::

## Run Migrations

In order to create the necessary tables for the new functionality, you'll need to run the DB Migrations.

If migrations have never been run before, you can run the `migrate` npm script to create the initial database:

:::sh
npm run migrate
:::

If you've already run the migrations before, you can run the `rerun:last` npm script to drop and re-run the last migration:

:::sh
npm run rerun:last
:::

Alternatively you can nuke the App's database (e.g. `App_Data/app.db`) and recreate it from scratch with `npm run migrate`.

## Instant CRUD UI

After running the DB migrations, you can hit the ground running and start using the Admin UI to manage the new 
Data Model RDBMS Tables:

:::youtube 8buo_ce3SNM
Using AutoQuery CRUD UI in a Text to Blazor App
:::

### Create new Records from Search Dialog

We're continually improving the UX of the [AutoQueryGrid Component](/vue/autoquerygrid) used in generating CRUD UIs to enable a more productive and seamless workflow. A change added to that end that you can see in the above video is the ability to add new Records from a Search dialog:

![](/img/posts/text-to-blazor/autoquerygrid-new2.webp)

This now lets you start immediately creating new records without needing to create any lookup entries beforehand.

## Audited Data Models

The TypeScript Data Models enable a rapid development experience for defining an App's Data Models which are used
to generate the necessary AutoQuery CRUD APIs to support an Admin UI.

An example of the productivity of this approach is the effortless support for maintaining a detailed audit history for changes to select tables by inheriting from the `AuditBase` base class, e.g:

```ts
export class Job extends AuditBase {
    ...
}
```

Which can then be regenerated using the name of the TypeScript Model definitions:

:::sh
npx okai Jobs.d.ts
:::

This will include additional `CreatedBy`, `CreatedDate`, `ModifiedBy`, `ModifiedDate`, `DeletedBy` and `DeletedDate`
properties to the specified Table and also generates the necessary
[Audit Behaviors](https://docs.servicestack.net/autoquery/crud#apply-generic-crud-behaviors)
on the AutoQuery APIs to maintain the audit history for each CRUD operation.

### AutoQuery CRUD Audit Log

As the **blazor-admin** and **blazor-vue** templates are configured to use the [AutoQuery CRUD Executable Audit Log](https://docs.servicestack.net/autoquery/audit-log)
in its [Configure.AutoQuery.cs](https://github.com/NetCoreTemplates/blazor-admin/blob/main/MyApp/Configure.AutoQuery.cs)
the Audit Behaviors will also maintain an Audit Trail of all CRUD operations which can be viewed in the Admin UI:

![](/img/posts/text-to-blazor/okai-audit-form.webp)

## TypeScript Schema

In addition to being a great DSL for defining Data Models, using TypeScript also lets us define a schema
containing all the C# Types, interfaces, and attributes used in defining APIs, DTOs and Data Models in 
the accompanying [api.d.ts](https://okai.servicestack.com/api.d.ts) file.

This now lets us use TypeScript to define the [Bookings.cs](https://github.com/NetCoreTemplates/blazor-vue/blob/main/MyApp.ServiceModel/Bookings.cs) 
AutoQuery APIs and Data Models which blazor-admin uses instead in its [Bookings.d.ts](https://github.com/NetCoreTemplates/blazor-admin/blob/main/MyApp.ServiceModel/Bookings.d.ts):

```ts
/// <reference path="./api.d.ts" />
export type Config = {
    prompt:    "New Booking"
    api:       "~/MyApp.ServiceModel/Bookings.cs"
    migration: "~/MyApp/Migrations/Migration1001.cs"
    uiMjs:     "~/MyApp/wwwroot/admin/sections/Bookings.mjs"
}

export enum RoomType {
  Single,
  Double,
  Queen,
  Twin,
  Suite,
}

@Read.route("/bookings","GET")
@Read.route("/bookings/{Id}","GET")
@Read.description("Find Bookings")
@Create.route("/bookings","POST")
@Create.description("Create a new Booking")
@Update.notes("Find out how to quickly create a <a href='https://youtu.be/nhc4MZufkcM'>C# Bookings App from Scratch</a>")
@Update.route("/booking/{Id}","PATCH")
@Update.description("Update an existing Booking")
@Delete.route("/booking/{Id}","DELETE")
@Delete.description("Delete a Booking")
@tag("Bookings")
@icon({svg:"<svg>...</svg>"})
@notes("Captures a Persons Name & Room Booking information")
@description("Booking Details")
@validateHasRole("Employee")
export class Booking extends AuditBase {
  @autoIncrement()
  id: number
  @Create.description("Name this Booking is for")
  @validateNotEmpty()
  name: string
  roomType: RoomType
  @validateGreaterThan(0)
  roomNumber: number
  @intlDateTime(DateStyle.Long)
  bookingStartDate: Date
  @intlRelativeTime()
  bookingEndDate?: Date
  @intlNumber({currency:"USD"})
  @validateGreaterThan(0)
  cost: decimal
  @ref({model:"nameof(Coupon)",refId:"nameof(Coupon.Id)",refLabel:"nameof(Coupon.Description)"})
  @references("typeof(Coupon)")
  couponId?: string
  @reference()
  discount?: Coupon
  @input({type:"textarea"})
  notes?: string
  cancelled?: boolean
  @reference({selfId:"nameof(CreatedBy)",refId:"nameof(User.UserName)",refLabel:"nameof(User.DisplayName)"})
  employee: User
}

@tag("Bookings")
@icon({svg:"<svg>...</svg>"})
export class Coupon extends AuditBase {
  id: string
  description: string
  discount: number
  expiryDate: Date
}
```

The benefit of this approach being that you can make a change to the Data Models and rerun the okai tool
to regenerate the AutoQuery APIs, DB Migrations and Admin UIs.

:::sh
npx okai Bookings.d.ts
:::

Which will regenerate its:
- APIs: [MyApp.ServiceModel/Bookings.cs](https://github.com/NetCoreTemplates/blazor-admin/blob/main/MyApp.ServiceModel/Bookings.cs)
- DB Migration: [MyApp/Migrations/Migration1000.cs](https://github.com/NetCoreTemplates/blazor-admin/blob/main/MyApp/Migrations/Migration1000.cs)
- Admin UI: [/wwwroot/admin/sections/Bookings.mjs](https://github.com/NetCoreTemplates/blazor-admin/blob/main/MyApp/wwwroot/admin/sections/Bookings.mjs)

What files will be generated is controlled in the `Config` section: 

```ts
export type Config = {
    prompt:    "New Booking"
    api:       "~/MyApp.ServiceModel/Bookings.cs"
    migration: "~/MyApp/Migrations/Migration1001.cs"
    uiMjs:     "~/MyApp/wwwroot/admin/sections/Bookings.mjs"
}
```

So if you no longer want the code regeneration to update the DB Migration for it, you can just remove it
from the Config.

## Customize Data Models

The data models defined in the TypeScript Declaration file e.g. `Bookings.d.ts` is what drives the
generation of the Data Models, APIs, DB Migrations and Admin UIs.

This can be further customized by editing the TypeScript Declaration file and re-running the `okai` tool
with the name of the TypeScript Declaration file, e.g. `Bookings.d.ts`:

:::sh
npx okai Bookings.d.ts
:::

Which will re-generate the Data Models, APIs, DB Migrations and Admin UIs based on the updated Data Models.

![](/img/posts/text-to-blazor/okai-Employees.webp)

Or add `--watch` to watch the TypeScript Declaration file for changes and automatically re-generate the generated files on Save:

:::sh
npx okai Bookings.d.ts --watch
:::

:::tip
You only need to specify the `Bookings.d.ts` TypeScript filename (i.e. not the filepath) from 
anywhere within your .NET solution
:::

One challenge with this approach is that we only have a single class to use to define our
attributes for both Request and Response DTOs for all AutoQuery CRUD APIs and Data Models.

### API and Data Model attributes 

The okai tool resolves some of these issues with smart generation of attributes where "Data Model Attributes" 
like `[Icon]` class attribute and `[AutoIncrement]` property attributes are only generated on the Data Model:

```ts
@icon({svg:"<svg>...</svg>"})
export class Booking {
    @autoIncrement()
    id: number
    @intlNumber({currency:"USD"})
    cost: decimal
}
```

Whilst "API Attributes" like `[Tag]` and `[ValidateHasRole]` class attribute and `[ValidateGreaterThan]` 
property attributes and are only generated on the APIs Request DTOs:

```ts
@tag("Bookings")
@validateHasRole("Employee")
export class Booking {
    @validateGreaterThan(0)
    cost: decimal
}
```

### C# Types

As JavaScript only has a limited set of types, the TypeScript **api.d.ts** schema also includes the 
built-in C# Types used when defining APIs, DTOs and Data Models which you'll be able to use when your
APIs need to use a specific .NET type, e.g:

```ts
export class Booking extends AuditBase {
  id: number
  name: string
  roomNumber: number
  bookingStartDate: Date
  bookingEndDate?: DateOnly
  cost: decimal
  cancelled?: boolean
}
```
 
Which uses the `DateOnly` and `decimal` .NET Types to generate:

```csharp
public class Booking : AuditBase
{
    [AutoIncrement]
    public int Id { get; set; }
    public string Name { get; set; }
    public int RoomNumber { get; set; }
    public DateTime BookingStartDate { get; set; }
    public DateOnly? BookingEndDate { get; set; }
    public decimal Cost { get; set; }
    public bool? Cancelled { get; set; }
}
```

### API Targeted Attributes

When you need to add attributes to a specific API Request DTO you can use a CRUD prefix to have it only
applied to that specific AutoQuery API, e.g:

```ts
@Read.route("/bookings","GET")
@Read.route("/bookings/{Id}","GET")
@Create.route("/bookings","POST")
```

Where it would only the generated on the AutoQuery API that it targets, e.g:

```csharp
[Route("/bookings", "GET")]
[Route("/bookings/{Id}", "GET")]
public class QueryBookings : QueryDb<Booking> { ... }

[Route("/bookings", "POST")]
public class CreateBooking : ICreateDb<Booking>, IReturn<IdResponse> { ... }
```

In addition to `Create.`, `Read.`, `Update.`, `Delete.` attributes to target specific AutoQuery CRUD APIs, 
you can also use `Write.` to target all `Create.`, `Update.`, `Delete.` Write APIs.

### Ambiguous Attributes

Attributes that can be annotated on both the Data Model and API Request DTOs like `[Notes]` and `[Description]` 
are only generated on the Data Model and require using targeted attributes to apply to them to 
API Request DTOs, e.g:

```ts
@Read.description("Find Bookings")
@Create.description("Create a new Booking")
@Update.notes("Find out how to quickly create a <a href='https://youtu.be/nhc4MZufkcM'>C# Bookings App from Scratch</a>")
@Update.description("Update an existing Booking")
@Delete.description("Delete a Booking")
@notes("Captures a Persons Name & Room Booking information")
@description("Booking Details")
export class Booking extends AuditBase { ... }
```

Where the naked `@notes` and `@description` attributes are only generated on the Data Model whilst the 
targeted attributes are generated on their respective DTOs, e.g: 

```csharp
[Description("Find Bookings")]
public class QueryBookings : QueryDb<Booking> { ... }

[Description("Create a new Booking")]
public class CreateBooking : ICreateDb<Booking>, IReturn<IdResponse> { ... }

[Notes("Find out how to quickly create a <a href='https://youtu.be/nhc4MZufkcM'>C# Bookings App from Scratch</a>")]
[Description("Update an existing Booking")]
public class UpdateBooking : IPatchDb<Booking>, IReturn<IdResponse> { ... }

[Description("Delete a Booking")]
public class DeleteBooking : IDeleteDb<Booking>, IReturnVoid { ... }
    
[Description("Booking Details")]
[Notes("Captures a Persons Name & Room Booking information")]
public class Booking : AuditBase { ... }
```

### Special Attribute Values

There's special behavior for `"nameof(...)"` and `"typeof(...)"` string attribute values where: 

```ts
export class Booking extends AuditBase {
    @ref({model: "nameof(Coupon)", refId: "nameof(Coupon.Id)", refLabel: "nameof(Coupon.Description)"})
    @references("typeof(Coupon)")
    couponId?: string
}
```

Will be generated with native C# syntax, i.e. instead of as strings:

```csharp
public class Booking : AuditBase
{
    [Ref(Model=nameof(Coupon),RefId=nameof(Coupon.Id),RefLabel=nameof(Coupon.Description))]
    [References(typeof(Coupon))]
    public string? CouponId { get; set; }
}
```

### Changing Default Attributes

To improve the default out-of-the-box experience some attributes are included by default, including:

 - `[Icon]` attribute on Data Models based on the Data Model name
   - prevent by adding empty `@icon()` attribute
 - `[AutoIncrement]` on `id` number properties if no other `[PrimaryKey]` attribute is defined
   - prevent by adding `@primaryKey()` or `@autoId()`
 - `[Validate*]` attributes added to Create/Update APIs on non-nullable properties
   - prevent by adding empty `@validate()` attribute

Here's an example which changes the default behavior for the default attributes above:

```ts
@icon()
export class Todo {
    @primaryKey()
    id: number
    @validate()
    name: string
}
```

Which will generate the C# APIs without the `[Icon]` and `[Validate]` attributes and replace `[AutoIncrement]`
with `[PrimaryKey]`, e.g:

```csharp
public class CreateTodo : ICreateDb<Todo>, IReturn<IdResponse>
{
    [ValidateGreaterThan(0)]
    public int Id { get; set; }
    public string Name { get; set; }
}

public class Todo
{
    [PrimaryKey]
    public int Id { get; set; }
    public string Name { get; set; }
}
```

### Modifying ApplicationUser

In many cases the AI Models will want to generate a `User` class for their AI models. But as Blazor Apps
are already configured to use an ApplicationUser Identity Auth User class, the C# code generation only generates
the User class in a comment so you can merge it with your existing `User` class, e.g:

```csharp
/* merge with User DTO
/// <summary>
/// Interface defining the structure for a JobApplication.
/// Represents a user's application to a specific job.
/// </summary>
public class User
{
    [AutoIncrement]
    public int Id { get; set; }
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public string Email { get; set; }
    /// <summary>
    /// Optional URL to the user's resume
    /// </summary>
    public string? ResumeUrl { get; set; }
}
*/
```

If you wish to add additional properties, you'll first need to add it your `ApplicationUser` class, e.g:

```csharp
public class ApplicationUser : IdentityUser
{
    public string? FirstName { get; set; }
    public string? LastName { get; set; }
    public string? DisplayName { get; set; }
    public string? ProfileUrl { get; set; }
    /// <summary>
    /// Optional URL to the user's resume
    /// </summary>
    public string? ResumeUrl { get; set; }
}
```

You'll then need to regenerate the EF Migration to update the `AspNetUsers` table with the new columns by
running the `init-ef` npm script:

:::sh
npm run init-ef
:::

Which will delete the existing Migrations and create a new Migration to update the Identity Auth tables:

```json
{
    "scripts": {
        "init-ef": "node -e 'fs.readdirSync(`Migrations`).filter(x => !x.startsWith(`Migration`)).forEach(x => fs.rmSync(`Migrations/${x}`))' && dotnet ef migrations add CreateIdentitySchema",
    }
}
```

You can then delete your Primary Database (e.g. App_Data/app.db) and re-run the `migrate` npm script to recreate it:

:::sh
npm run migrate
:::

If you want the additional property to be included in API Responses you'll also need to add it to your `User` DTO, e.g:

```csharp
/// <summary>
/// Public User DTO
/// </summary>
[Alias("AspNetUsers")]
public class User
{
    public string Id { get; set; }
    public string UserName { get; set; }
    public string? FirstName { get; set; }
    public string? LastName { get; set; }
    public string? DisplayName { get; set; }
    public string? ProfileUrl { get; set; }
    public string? ResumeUrl { get; set; }
}
```

Which OrmLite and AutoQuery will use to query Identity Auth's `AspNetUsers` table.

### Custom APIs

When you need more fine-grained control over the generated APIs, you can "takeover" the generation of
an AutoQuery API by explicitly defining it yourself.

So if you prefer to use explicit API Request DTOs instead of targeting attributes or need to control
the exact properties that are generated in each API, you can define the API Request DTOs yourself
where when exists will skip generation for that API.

To showcase the differences between the single class approach, you can rewrite the above single class
approach with an explicit class for each API:

```ts
export enum RoomType {
  Single,
  Double,
  Queen,
  Twin,
  Suite,
}

@tag("Bookings")
@notes("Captures a Persons Name & Room Booking information")
@route("/bookings","GET")
@route("/bookings/{Id}","GET")
@autoApply(Behavior.AuditQuery)
@description("Find Bookings")
export class QueryBookings extends QueryDb<Booking> {
  id?: number
}

@tag("Bookings")
@route("/bookings","POST")
@autoApply(Behavior.AuditCreate)
@description("Create a new Booking")
@validateHasRole("Employee")
export class CreateBooking implements ICreateDb<Booking>, IReturn<IdResponse> {
  name?: string
  roomType?: RoomType
  @validateGreaterThan(0)
  roomNumber?: number
  bookingStartDate?: Date
  bookingEndDate?: Date
  @validateGreaterThan(0)
  cost?: decimal
  couponId?: string
  discount?: Coupon
  @input({type:"textarea"})
  notes?: string
  cancelled?: boolean
}

@tag("Bookings")
@route("/bookings","PATCH")
@autoApply(Behavior.AuditModify)
@description("Create a new Booking")
@validateHasRole("Employee")
export class UpdateBooking implements IPatchDb<Booking>, IReturn<IdResponse> {
  name?: string
  roomType?: RoomType
  @validateGreaterThan(0)
  roomNumber?: number
  bookingStartDate?: Date
  bookingEndDate?: Date
  @validateGreaterThan(0)
  cost?: decimal
  couponId?: string
  discount?: Coupon
  @input({type:"textarea"})
  notes?: string
  cancelled?: boolean
}

@tag("Bookings")
@route("/bookings/{Id}","DELETE")
@autoApply(Behavior.AuditSoftDelete)
@description("Delete a Booking")
@validateHasRole("Manager")
export class DeleteBookings implements IDeleteDb<Booking>, IReturnVoid {
  id?: number
}

@tag("Bookings")
@notes("Captures a Persons Name & Room Booking information")
@route("/bookings","GET")
@route("/bookings/{Id}","GET")
@description("Find Bookings")
export class Booking extends AuditBase {
  @autoIncrement()
  id: number
  @Create.description("Name this Booking is for")
  @Create.validateNotEmpty()
  name: string
  roomType: RoomType
  roomNumber: number
  @intlDateTime(DateStyle.Long)
  bookingStartDate: Date
  @intlRelativeTime()
  bookingEndDate?: Date
  @intlNumber({currency:"USD"})
  cost: decimal
  @ref({model:"nameof(Coupon)",refId:"nameof(Coupon.Id)",refLabel:"nameof(Coupon.Description)"})
  @references("typeof(Coupon)")
  couponId?: string
  @reference()
  discount?: Coupon
  @Write.input({type:"textarea"})
  notes?: string
  cancelled?: boolean
  @reference({selfId:"nameof(CreatedBy)",refId:"nameof(User.UserName)",refLabel:"nameof(User.DisplayName)"})
  employee: User
}

@description("Discount Coupons")
export class Coupon extends AuditBase {
  id: string
  description: string
  discount: number
  expiryDate: Date
}

@tag("Bookings")
@route("/coupons","GET")
@autoApply(Behavior.AuditQuery)
@description("Find Coupons")
export class QueryCoupons extends QueryDb<Coupon> {
  id?: string
}

@tag("Bookings")
@route("/coupons","POST")
@autoApply(Behavior.AuditCreate)
@description("Create a new Create")
@validateHasRole("Employee")
export class CreateCoupon implements ICreateDb<Coupon>, IReturn<IdResponse> {
  id: string
  description: string
  discount: number
  expiryDate: Date
}

@tag("Bookings")
@route("/coupons","PATCH")
@autoApply(Behavior.AuditModify)
@description("Create a new Coupon")
@validateHasRole("Employee")
export class UpdateCoupon implements IPatchDb<Coupon>, IReturnVoid {
  id: string
  description?: string
  discount?: number
  expiryDate?: Date
}

@tag("Bookings")
@route("/coupons/{Id}","DELETE")
@autoApply(Behavior.AuditSoftDelete)
@description("Delete a Coupon")
@validateHasRole("Manager")
export class DeleteCoupon implements IDeleteDb<Coupon>, IReturnVoid {
  id?: string
}
```


# Self Hosted AI Server for LLMs, Ollama, Comfy UI & FFmpeg
Source: https://servicestack.net/posts/ai-server

## AI Server now ready to serve!

We're excited to announce the first release of AI Server - a Free OSS self-hosted Docker private gateway to 
manage API access to multiple LLM APIs, Ollama endpoints, Media APIs, Comfy UI and FFmpeg Agents.

:::youtube Ojo80oFQte8
Introducing AI Server
:::

### Centralized Management

Designed as a one-stop solution to manage an organization's AI integrations for all their System Apps,
by utilizing developer friendly HTTP JSON APIs that supports any programming language or framework.

[![](/img/svgs/ai-server-overview.svg)](https://openai.servicestack.net)

### Distribute load across multiple Ollama, Open AI Gateway and Comfy UI Agents

It works as a private gateway to process LLM, AI and image transformations requests
that any of our Apps need where it dynamically load balances requests across our local GPU Servers, Cloud GPU instances
and API Gateways running multiple instances of Ollama, Open AI Chat, LLM Gateway, Comfy UI, Whisper
and ffmpeg providers.

In addition to maintaining a history of AI Requests, it also provides file storage
for its CDN-hostable AI generated assets and on-the-fly, cacheable image transformations.

### Native Typed Integrations

Uses [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference) to enable
simple, native typed integrations for most popular Web, Mobile and Desktop languages including: 
C#, TypeScript, JavaScript, Python, Java, Kotlin, Dart, PHP, Swift, F# and VB.NET.

Each AI Feature supports multiple call styles for optimal integration of different usages:

- **Synchronous API** · Simplest API ideal for small workloads where the Response is returned in the same Request
- **Queued API** · Returns a reference to the queued job executing the AI Request which can be used to poll for the API Response
- **Reply to Web Callback** · Ideal for reliable App integrations where responses are posted back to a custom URL Endpoint

### Live Monitoring and Analytics

Monitor performance and statistics of all your App's AI Usage, real-time logging of executing APIs with auto archival 
of completed AI Requests into monthly rolling SQLite databases.

### Protected Access with API Keys

AI Server utilizes [Simple Auth with API Keys](https://docs.servicestack.net/auth/admin-apikeys)
letting Admins create and distribute API Keys to only allow authorized clients to access their
AI Server's APIs, which can be optionally further restricted to only
[allow access to specific APIs](https://docs.servicestack.net/auth/apikeys#creating-user-api-keys).

## Install

AI Server can be installed on macOS and Linux with Docker by running [install.sh](https://github.com/ServiceStack/ai-server/blob/main/install.sh):

1. Clone the AI Server repository from GitHub:

:::sh
`git clone https://github.com/ServiceStack/ai-server`
:::

2. Run the Installer

:::sh
`cd ai-server && cat install.sh | bash`
:::

The installer will detect common environment variables for the supported AI Providers like OpenAI, Google, Anthropic, 
and others, and prompt ask you if you want to add them to your AI Server configuration.

<ascii-cinema src="https://docs.servicestack.net/pages/ai-server/ai-server-install.cast"
  loop="true" poster="npt:00:21" theme="dracula" rows="12" />

### Optional - Install ComfyUI Agent

If your server also has a GPU you can ask the installer to also install the [ComfyUI Agent](/ai-server/comfy-extension):

<ascii-cinema src="https://docs.servicestack.net/pages/ai-server/agent-comfy-install.cast"
    loop="true" poster="npt:00:21" theme="dracula" rows="16" />

The ComfyUI Agent is a separate Docker agent for running [ComfyUI](https://www.comfy.org), 
[Whisper](https://github.com/openai/whisper) and [FFmpeg](https://www.ffmpeg.org) on servers with GPUs to handle 
AI Server's [Image](https://docs.servicestack.net/ai-server/transform/image) and 
[Video transformations](https://docs.servicestack.net/ai-server/transform/video) and Media Requests, including:

- [Text to Image](https://docs.servicestack.net/ai-server/text-to-image)
- [Image to Text](https://docs.servicestack.net/ai-server/image-to-text)
- [Image to Image](https://docs.servicestack.net/ai-server/image-to-image)
- [Image with Mask](https://docs.servicestack.net/ai-server/image-with-mask)
- [Image Upscale](https://docs.servicestack.net/ai-server/image-upscale)
- [Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text)
- [Text to Speech](https://docs.servicestack.net/ai-server/text-to-speech)

#### Comfy UI Agent Installer

To install the ComfyUI Agent on a separate server (with a GPU), you can clone and run the ComfyUI Agent installer 
on that server instead:

1. Clone the Comfy Agent

:::sh
`git clone https://github.com/ServiceStack/agent-comfy.git`
:::

2. Run the Installer

:::sh
`cd agent-comfy && cat install.sh | bash`
:::

## Running in Production

We've been developing and running AI Server for several months now, processing millions of LLM and Comfy UI Requests
to generate Open AI Chat Answers and Generated Images used to populate the
[pvq.app](https://pvq.app) and [blazordiffusion.com](https://blazordiffusion.com) websites.

Our production instance with more info about AI Server is available at:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
https://openai.servicestack.net
:::

[![](/img/posts/ai-server/ai-server-languages.png)](https://openai.servicestack.net)

## API Explorer

Whilst our production instance is protected by API Keys, you can still use it to explore available APIs in its API Explorer:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
[https://openai.servicestack.net/ui/](https://openai.servicestack.net/ui/OpenAiChatCompletion)
:::

## Documentation

The documentation for AI Server is being maintained at:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
https://docs.servicestack.net/ai-server/
:::

## Built-in UIs

Built-in UIs allow users with API Keys access to custom UIs for different AI features

[![](/img/posts/ai-server/ai-server-builtin-uis.png)](https://openai.servicestack.net)

## Admin UIs

Use Admin UI to manage API Keys that can access AI Server APIs and Features

[![](/img/posts/ai-server/ai-server-admin-uis.png)](https://openai.servicestack.net)

## Features

The current release of AI Server supports a number of different modalities, including:

### Large Language Models
- [Open AI Chat](https://docs.servicestack.net/ai-server/chat)
    - Support for Ollama endpoints
    - Support for Open Router, Anthropic, Open AI, Mistral AI, Google and Groq API Gateways

### Comfy UI Agent / Replicate / DALL-E 3
 
- [Text to Image](https://docs.servicestack.net/ai-server/text-to-image)

### Comfy UI Agent
 
- [Image to Image](https://docs.servicestack.net/ai-server/image-to-image)
    - [Image Upscaling](https://docs.servicestack.net/ai-server/image-upscale)
    - [Image with Mask](https://docs.servicestack.net/ai-server/image-with-mask)
- [Image to Text](https://docs.servicestack.net/ai-server/image-to-text)
- [Text to Speech](https://docs.servicestack.net/ai-server/text-to-speech)
- [Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text)
 
### FFmpeg

- [Image Transformations](https://docs.servicestack.net/ai-server/transform/image)
  - **Crop Image** - Crop an image to a specific size
  - **Convert Image** - Convert an image to a different format
  - **Scale Image** - Scale an image to a different resolution
  - **Watermark Image** - Add a watermark to an image

- [Video Transformations](https://docs.servicestack.net/ai-server/transform/video)
  - **Crop Video** - Crop a video to a specific size
  - **Convert Video** - Convert a video to a different format
  - **Scale Video** - Scale a video to a different resolution
  - **Watermark Video** - Add a watermark to a video
  - **Trim Video** - Trim a video to a specific length

### Managed File Storage

- Blob Storage - isolated and restricted by API Key

## AI Server API Examples

To simplify integrations with AI Server each API Request can be called with 3 different call styles to better
support different use-cases and integration patterns.

### Synchronous Open AI Chat Example

The **Synchronous API** is the simplest API ideal for small workloads where the Response is returned in the same Request:

```csharp
var client = new JsonApiClient(baseUrl);
client.BearerToken = apiKey;

var response = client.Post(new OpenAiChatCompletion {
    Model = "mixtral:8x22b",
    Messages = [
        new() {
            Role = "user",
            Content = "What's the capital of France?"
        }
    ],
    MaxTokens = 50
});

var answer = response.Choices[0].Message.Content;
```

### Synchronous Media Generation Request Example

Other AI Requests can be called synchronously in the same way where its API is named after the modality
it implements, e.g. you'd instead call `TextToImage` to generate an Image from a Text description:

```csharp
var response = client.Post(new TextToImage
    PositivePrompt = "A serene landscape with mountains and a lake",
    Model = "flux-schnell",
    Width = 1024,
    Height = 1024,
    BatchSize = 1
});

File.WriteAllBytes(saveToPath, response.Results[0].Url.GetBytesFromUrl());
```

### Queued Open AI Chat Example

The **Queued API** immediately Returns a reference to the queued job executing the AI Request:

```csharp
var response = client.Post(new QueueOpenAiChatCompletion
{
    Request = new()
    {
        Model = "gpt-4-turbo",
        Messages = [
            new() { Role = "system", Content = "You are a helpful AI assistant." },
            new() { Role = "user", Content = "How do LLMs work?" }
        ],
        MaxTokens = 50
    }
});
```

Which can be used to poll for the API Response of any Job by calling `GetOpenAiChatStatusResponse`
and checking when its state has finished running to get the completed `OpenAiChatResponse`:

```csharp
GetOpenAiChatStatusResponse status = new();
while (status.JobState is BackgroundJobState.Started or BackgroundJobState.Queued)
{
    status = await client.GetAsync(new GetOpenAiChatStatus { RefId = response.RefId });
    await Task.Delay(1000);
}

var answer = status.Result.Choices[0].Message.Content;
```

### Queued Media Artifact Generation Request Example

Most other AI Server Requests are Artifact generation requests which would instead call 
`GetArtifactGenerationStatus` to get the artifacts response of a queued job, e.g:

```csharp
var response = client.Post(new QueueTextToImage {
    PositivePrompt = "A serene landscape with mountains and a lake",
    Model = "flux-schnell",
    Width = 1024,
    Height = 1024,
    BatchSize = 1
});

// Poll for Job Completion Status
GetArtifactGenerationStatusResponse status = new();
while (status.JobState is BackgroundJobState.Queued or BackgroundJobState.Started)
{
    status = client.Get(new GetArtifactGenerationStatus { JobId = response.JobId });
    Thread.Sleep(1000);
}

File.WriteAllBytes(saveToPath, status.Results[0].Url.GetBytesFromUrl());
```

### Queued Media Text Generation Request Example

Whilst the Media API Requests that generates text like `SpeechToText` or `ImageToText` would instead call
`GetTextGenerationStatus` to get the text response of a queued job, e.g:

```csharp
using var fsAudio = File.OpenRead("files/test_audio.wav");
var response = client.PostFileWithRequest(new QueueSpeechToText(),
    new UploadFile("test_audio.wav", fsAudio, "audio"));

// Poll for Job Completion Status
GetTextGenerationStatusResponse status = new();
while (status.JobState is BackgroundJobState.Started or BackgroundJobState.Queued)
{
    status = client.Get(new GetTextGenerationStatus { RefId = response.RefId });
    Thread.Sleep(1000);
}

var answer = status.Results[0].Text;
```

### Open AI Chat with Callback Example

The Queued API also accepts a **Reply to Web Callback** for a more reliable push-based App integration
where responses are posted back to a custom URL Endpoint:

```csharp
var correlationId = Guid.NewGuid().ToString("N");
var response = client.Post(new QueueOpenAiChatCompletion
{
    //...
    ReplyTo = $"https://example.org/api/OpenAiChatResponseCallback?CorrelationId=${correlationId}"
});
```

Your callback can add any additional metadata on the callback to assist your App in correlating the response with 
the initiating request which just needs to contain the properties of the `OpenAiChatResponse` you're interested in
along with any metadata added to the callback URL, e.g:

```csharp
public class OpenAiChatResponseCallback : IPost, OpenAiChatResponse, IReturnVoid
{
    public Guid CorrelationId { get; set; }
}

public object Post(OpenAiChatResponseCallback request)
{
    // Handle OpenAiChatResponse callabck
}
```

Unless your callback API is restricted to only accept requests from your AI Server, you should include a 
unique Id like a `Guid` in the callback URL that can be validated against an initiating request to ensure 
the callback can't be spoofed.

## Feedback

Feel free to reach us at [ai-server/discussions](https://github.com/ServiceStack/ai-server/discussions)
with any AI Server questions.


# .NET 8 Templates migrated to use Kamal for deployments
Source: https://servicestack.net/posts/kamal-deployments

Since introducing [GitHub Actions support to our templates](https://docs.servicestack.net/ssh-docker-compose-deploment), we've promoted the simplified deployments, focusing on tooling like SSH and Docker Compose to give the most portability to projects by default. This was partly inspired by the fact that cloud providers value offerings have been decreasing, especially over the last 5 years. We've previously showed [the significant savings](https://servicestack.net/posts/hetzner-cloud) available by utilizing of hosting providers like Hetzner (who we've been using for several years), and moved all our templates and live demos to Hetzner resulting in a roughly **$0.50 per month** cost per .NET App.

Along with this decreasing in value from the major cloud vendors, and the general hardware improvements, we've also been leaning into [using SQLite for server .NET Apps](/posts/scalable-sqlite), using it as the primary database for some of our larger example applications like [pvq.app](https://pvq.app), [blazordiffusion.com](https://blazordiffusion.com), and most recently, [AI Server](https://openai.servicestack.net).

We're delighted to see that the folks at BaseCamp are estimating to [save millions from their cloud exit](https://world.hey.com/dhh/our-cloud-exit-savings-will-now-top-ten-million-over-five-years-c7d9b5bd) and have doubled down on their general purpose Docker deployment solutions with their initial MRSK project, that's now known as Kamal.

### Use Kamal to deploy .NET Apps to any Linux server

:::youtube -mDJfRG8mLQ
Use Kamal with GitHub Actions to deploy .NET Apps to any Linux server
:::

## What is [Kamal](https://kamal-deploy.org/)?

Kamal is a tool that offers the same flexibility by wrapping up the use of fundamental tooling like SSH and Docker into a great CLI tool that tries to make the management of containerized applications, enabling them to be deployed anywhere there is a Linux host that is accessible via SSH. It handles reverse proxy of web traffic automatically, as well as even the initial setup of the reverse proxy and related tooling to any target Linux host. 

This means you get the same great ergonomics of just pointing your DNS and configuration file to a server, and *Kamal takes care of the rest*, including TLS certificates via LetsEncrypt.

It even has commands that allow you to check on your running applications, view logs etc and all you need to do is run the commands from your local repository directory.

While our own templates have used the same approach for GitHub Actions, the usage was always awkward and lacked any dedicated CLI tooling you could run locally to check on your running applications.

## What's in the templates?

We still believe that having a CI process is important, and while Kamal deployments are repeatable from your local machine and uses locking to avoid multiple developers deploying changes, the single consistent process of a CI is hard to beat. So while we have moved the templates to use Kamal, we've incorporated GitHub Actions by default so you can still get the benefits of running commands like `kamal app logs` locally from your development machine when looking at production issues, but have that consistent workflow for deployment on your repositories GitHub Actions.

## How it works

One of the big benefits of Kamal is the focus on ergonomics and the really well done documentation that the BaseCamp team has put together. So if you need to know more about Kamal, [checkout their docs](https://kamal-deploy.org/docs/). For the ServiceStack templates, you will need to add a valid `PRIVATE_SSH_KEY` as a GitHub Actions secret to get it working along with the customization of your `config/deploy.yml` file which is a part of any Kamal setup. In short, you will need:

- Get a Linux host running with SSH access
- Update your DNS configuration with an A record pointing to that hosts IP address
- Create a new project using one of our updated templates using a command like:

:::sh
x new blazor-vue MyApp
:::

Update the `config/deploy.yml` with the following details:

### GitHub Container Registry Image

Update with your preferred container image name:

```yml
# Name of the container image
image: my-user/myapp
```

### Server Web

Configure with your Linux Host IP Address:

```yml
servers:
  # IP address of server, optionally use env variable
  web:
    - 123.123.123.123
```

Alternatively, you can use an environment variable for the server IP address, e.g:

```yml
  web:
    - <%= ENV['KAMAL_DEPLOY_IP'] %>
```

### Proxy Host

Configure with your domain pointing to the same IP as your host:

```yml
proxy:
  ssl: true
  host: myapp.example.com
```

### Health Checks

The template includes the use of ASP.NET Core Health Checks, that use the default Kamal path of `/up` to check if the application is running before deploying.

```csharp
public class HealthCheck : IHealthCheck
{
    public async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context, CancellationToken token = default)
    {
        // Perform health check logic here
        return HealthCheckResult.Healthy();
    }
}
```

Kamal checks this path before deploying your application, so you can add any custom health checks to this path to ensure your application is ready to receive traffic.

## GitHub Repository

With your application created and configured for deployment, you can create a new GitHub Repository and add the GitHub Actions Secret of `PRIVATE_SSH_KEY` which should be a separate SSH key for deployments that has access to your Linux host.

You can use the GitHub CLI to do of these steps.

```bash
gh repo create
```

When prompted, create an empty repository.

Then add the `PRIVATE_SSH_KEY` secret.

```
gh secret set PRIVATE_SSH_KEY < deploy-key
```

Where `deploy-key` is your deployment specific SSH key file.

Once created, you can follow the steps in your empty repository to init your templated `MyApp` project and push your initial commit. If you're deploy.yml config and DNS was setup correctly, the GitHub Action will do the following:

- Build and test your application running the MyApp.Tests project by default
- Publish your application as a Docker container to GitHub's `ghcr.io` repository
- Use Kamal to initialize your Linux host to be able to run Kamal applications and use their default `kamal-proxy`
- Fix volume permissions your for application due to ASP.NET containerization not running as root user in the container.
- Run your `AppTasks=migrate` command before running your application initializing the SQLite database
- Run your AppHost using `kamal deploy -P --version latest` command.

## Summary

We're excited to be moving our templates to Kamal for deployments as it has distilled the simple approach we have baked in our templates for a number of years while massively improving the ergonomics. We're excited to see what the BaseCamp team does with the project, and we're looking forward to seeing the community grow around it. If you have any questions about the templates or Kamal, feel free to reach out to us on our Discord, GitHub Discussions or Customer Forums.


# DTOs in all languages downloadable without .NET
Source: https://servicestack.net/posts/npx-get-dtos

To make it easier to consume ServiceStack APIs in any language, we've added the ability to download 
Typed DTOs from any ServiceStack API in all languages without needing .NET installed with the new `npx get-dtos` npm script.

It has the same syntax and functionality as the `x` dotnet tool for adding and updating ServiceStack References where
in most cases you can replace `x <lang>` with `npx get-dtos <lang>` to achieve the same result.

Running `npx get-dtos` without any arguments will display the available options:

    get-dtos <lang>                  Update all ServiceStack References in directory (recursive)
    get-dtos <file>                  Update existing ServiceStack Reference (e.g. dtos.cs)
    get-dtos <lang>     <url> <file> Add ServiceStack Reference and save to file name
    get-dtos csharp     <url>        Add C# ServiceStack Reference            (Alias 'cs')
    get-dtos typescript <url>        Add TypeScript ServiceStack Reference    (Alias 'ts')
    get-dtos javascript <url>        Add JavaScript ServiceStack Reference    (Alias 'js')
    get-dtos python     <url>        Add Python ServiceStack Reference        (Alias 'py')
    get-dtos dart       <url>        Add Dart ServiceStack Reference          (Alias 'da')
    get-dtos php        <url>        Add PHP ServiceStack Reference           (Alias 'ph')
    get-dtos java       <url>        Add Java ServiceStack Reference          (Alias 'ja')
    get-dtos kotlin     <url>        Add Kotlin ServiceStack Reference        (Alias 'kt')
    get-dtos swift      <url>        Add Swift ServiceStack Reference         (Alias 'sw')
    get-dtos fsharp     <url>        Add F# ServiceStack Reference            (Alias 'fs')
    get-dtos vbnet      <url>        Add VB.NET ServiceStack Reference        (Alias 'vb')
    get-dtos tsd        <url>        Add TypeScript Definition ServiceStack Reference
    
    Options:
        -h, --help, ?             Print this message
        -v, --version             Print tool version version
            --include <tag>       Include all APIs in specified tag group
            --qs <key=value>      Add query string to Add ServiceStack Reference URL
            --verbose             Display verbose logging
            --ignore-ssl-errors   Ignore SSL Errors

## Reusable DTOs and Reusable Clients in any language

A benefit of [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference) is that only an 
API DTOs need to be generated which can then be used to call any remote instance running that API. E.g. DTOs generated
for our deployed AI Server instance at [openai.servicestack.net](https://openai.servicestack.net) can be used to call
any self-hosted AI Server instance, likewise the same generic client can also be used to call any other ServiceStack API.

### Typed Open AI Chat & Ollama APIs in 11 Languages

A good example of its versatility is in the [Typed OpenAI Chat & Ollama APIs](/posts/typed-openai-chat-ollama-apis) 
in which AI Server's Typed DTOs can be used to call **any Open AI Chat compatible API** in its 11 supported languages.

### TypeScript Example

For example you can get the TypeScript DTOs for the just released [AI Server](/posts/ai-server) by:

1. Installing the `@servicestack/client` npm package:

:::copy
npm install @servicestack/client
:::

2. Download AI Server's TypeScript DTOs:

:::copy
`npx get-dtos typescript https://openai.servicestack.net`
:::

Which just like the `x` tool will add the TypeScript DTOs to the `dtos.ts` file

### Calling Ollama from TypeScript

Call Ollama by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```ts
import { JsonServiceClient } from "@servicestack/client"
import { OpenAiChatCompletion } from "./dtos"

const client = new JsonServiceClient(baseUrl)

const response = await client.postToUrl("/v1/chat/completions",
    new OpenAiChatCompletion({
        model: "mixtral:8x22b",
        messages: [
            { role: "user", content: "What's the capital of France?" }
        ],
        max_tokens: 50
    })
)

const answer = response.choices[0].message.content
```

### Update TypeScript DTOs

And later update all TypeScript ServiceStack References in the current directory with:

:::sh
`npx get-dtos typescript`
:::

### Install and Run in a single command

This can be used as a more flexible alternative to the `x` tool where it's often easier to install node in CI environments
than a full .NET SDK and easier to use npx scripts than global dotnet tools. For example you can use the `--yes` flag
to implicitly install (if needed) and run the `get-dtos` script in a single command, e.g:

:::sh
`npx --yes get-dtos typescript`
:::

### C# Example

As such you may want want to replace the `x` dotnet tool with `npx get-dtos` in your C#/.NET projects as well which
can either use the language name or its more wrist-friendly shorter alias, e.g:

:::sh
`npx get-dtos cs https://openai.servicestack.net`
:::

Then later update all C# DTOs in the current directory (including sub directories) with:

:::sh
`npx get-dtos cs`
:::


# ServiceStack.Swift client library rewritten for Swift 6
Source: https://servicestack.net/posts/swift6-upgrade

![](https://docs.servicestack.net/img/pages/servicestack-reference/swift-logo-banner.jpg)

As part of the release of [AI Server](/posts/ai-server) we've upgraded all generic service client libraries 
to support multiple file uploads with API requests to take advantage of AI Server APIs
that accept file uploads like [Image to Image](https://docs.servicestack.net/ai-server/image-to-image), 
[Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text) or its 
[FFmpeg Image](https://docs.servicestack.net/ai-server/transform/image) and 
[Video Transforms](https://docs.servicestack.net/ai-server/transform/video).

## ServiceStack.Swift rewritten for Swift 6

[ServiceStack.Swift](https://github.com/ServiceStack/ServiceStack.Swift) received the biggest upgrade, 
which was also rewritten to take advantage of Swift 6 features, including Swift promises which replaced the previous 
[PromiseKit](https://github.com/mxcl/PromiseKit) dependency - making it now dependency-free! 

For example you can request a [Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text) 
transcription by sending an audio file to the `SpeechToText` API using the new `postFilesWithRequest` method:

### Calling AI Server to transcribe an Audio Recording

```swift
let client = JsonServiceClient(baseUrl: "https://openai.servicestack.net")
client.bearerToken = apiKey

let request = SpeechToText()
request.refId = "uniqueUserIdForRequest"

let response = try client.postFilesWithRequest(request:request, 
    file:UploadFile(fileName:"audio.mp3", data:mp3Data, fieldName:"audio"))

Inspect.printDump(response)
``` 

### Async Upload Files with API Example

Alternatively use the new `postFileWithRequestAsync` method to call the API asynchronously
using [Swift 6 Concurrency](https://docs.swift.org/swift-book/documentation/the-swift-programming-language/concurrency/)
new **async/await** feature:

```swift
let response = try await client.postFileWithRequestAsync(request:request, 
    file:UploadFile(fileName:"audio.mp3", data:mp3Data, fieldName:"audio"))
    
Inspect.printDump(response)
```

### Multiple file upload with API Request examples

Whilst the `postFilesWithRequest` methods can be used to upload multiple files with an API Request. e.g:

```swift
let request = WatermarkVideo()
request.position = .BottomRight

let response = try client.postFilesWithRequest(request: request,
    files: [
        UploadFile(fileName: "video.mp4", data:videoData, fieldName:"video"),
        UploadFile(fileName: "mark.jpg", data:imgData, fieldName:"watermark")
    ])
```

Async Example:

```swift
let response = try await client.postFilesWithRequestAsync(request: request,
    files: [
        UploadFile(fileName: "video.mp4", data:videoData, fieldName:"video"),
        UploadFile(fileName: "mark.jpg", data:imgData, fieldName:"watermark")
    ])
```

### Sending typed Open AI Chat Ollama Requests with Swift

Even if you're not running AI Server you can still use its typed DTOs to call any compatible
Open AI Chat Compatible API like a self-hosted [Ollama](https://ollama.com) API. 

To call an Ollama endpoint from Swift:

1. Include `ServiceStack` package in your projects `Package.swift`

```swift
dependencies: [
    .package(url: "https://github.com/ServiceStack/ServiceStack.Swift.git",
        Version(6,0,0)..<Version(7,0,0)),
],
```

2. Download AI Server's Swift DTOs:

:::copy
`npx get-dtos swift https://openai.servicestack.net`
:::

You'll then be able to call Ollama by sending the OpenAI Chat compatible `OpenAiChatCompletion` 
Request DTO with the `JsonServiceClient`:

```swift
import Foundation
import ServiceStack

let ollamaBaseUrl = "http://localhost:11434"
let client = JsonServiceClient(baseUrl:ollamaBaseUrl)

let request = OpenAiChatCompletion()
request.model = "mixtral:8x22b"
let msg =  OpenAiMessage()
msg.role = "user"
msg.content = "What's the capital of France?"
request.messages = [msg]
request.max_tokens = 50

let result:OpenAiChatResponse = try await client.postAsync(
    "/v1/chat/completions", request:request)
```


# Typed Open AI Chat & Ollama APIs in 11 Languages
Source: https://servicestack.net/posts/typed-openai-chat-ollama-apis

A happy consequence of the release of [AI Server](/posts/ai-server) is that its 
[OpenAiChatCompletion](https://openai.servicestack.net/ui/OpenAiChatCompletion?tab=details) API is an 
**Open AI Chat compatible API** that can be used to access other LLM API Gateways, like Open AI's Chat GPT, Open Router, 
Mistral AI, GroqCloud as well as self-hosted Ollama instances directly in 11 of the most popular Web, Mobile & Desktop languages. 

This is a great opportunity to showcase the simplicity and flexibility of the
[Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference) feature where invoking APIs are 
all done the same way in all languages where the same generic Service Client can be used to call any ServiceStack API by 
downloading their typed API DTOs and sending its populated Request DTO.

Typically your `baseUrl` would be the URL of the remote ServiceStack API, but in this case we're using the
generic JSON Service Client and Typed DTOs to call an external Open AI Chat API directly, e.g. to call your 
local self-hosted [Ollama Server](https://ollama.com) you'd use:

```csharp
var baseUrl = "http://localhost:11434";
```

We'll use this to show how to call Open AI Chat APIs in 11 different languages:

## C#

Install the `ServiceStack.Client` NuGet package:

:::copy
`<PackageReference Include="ServiceStack.Client" Version="8.*" />`
:::

Download AI Server's C# DTOs with [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::copy
`x csharp https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with `JsonApiClient`:

```csharp
using ServiceStack;

var client = new JsonApiClient(baseUrl);

var result = await client.PostAsync<OpenAiChatResponse>("/v1/chat/completions",
    new OpenAiChatCompletion {
        Model = "mixtral:8x22b",
        Messages = [
            new () { Role = "user", Content = "What's the capital of France?" }
        ],
        MaxTokens = 50
    });
```

## TypeScript

Install the `@servicestack/client` npm package:

:::copy
npm install @servicestack/client
:::

Download AI Server's TypeScript DTOs:

:::copy
`npx get-dtos typescript https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient: 

```ts
import { JsonServiceClient } from "@servicestack/client"
import { OpenAiChatCompletion } from "./dtos"

const client = new JsonServiceClient(baseUrl)

const result = await client.postToUrl("/v1/chat/completions",
    new OpenAiChatCompletion({
        model: "mixtral:8x22b",
        messages: [
            { role: "user", content: "What's the capital of France?" }
        ],
        max_tokens: 50
    })
)
```

## JavaScript

Save [servicestack-client.mjs](https://unpkg.com/@servicestack/client@2/dist/servicestack-client.mjs) to your project

Define an Import Map referencing its saved location

```html
<script type="importmap">
    {
        "imports": {
            "@servicestack/client": "/js/servicestack-client.mjs"
        }
    }
</script>
```

Download AI Server's ESM JavaScript DTOs:

:::copy
`npx get-dtos mjs https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```js
import { JsonServiceClient } from "@servicestack/client"
import { OpenAiChatCompletion } from "./dtos.mjs"

const client = new JsonServiceClient(baseUrl)

const result = await client.postToUrl("/v1/chat/completions",
    new OpenAiChatCompletion({
        model: "mixtral:8x22b",
        messages: [
            { role: "user", content: "What's the capital of France?" }
        ],
        max_tokens: 50
    })
)
```

## Python

Install the `servicestack` PyPI package:

:::copy
pip install servicestack
:::

Download AI Server's Python DTOs:

:::copy
`npx get-dtos python https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```py
from servicestack import JsonServiceClient
from my_app.dtos import *

client = JsonServiceClient(baseUrl)

result = client.post_url("/v1/chat/completions",OpenAiChatCompletion(
    model="mixtral:8x22b",
    messages=[
        OpenAiMessage(role="user",content="What's the capital of France?")
    ],
    max_tokens=50
))
```

## Dart

Include `servicestack` package in your projects `pubspec.yaml`:

:::copy
servicestack: ^3.0.1
:::

Download AI Server's Dart DTOs:

:::copy
`npx get-dtos dart https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```dart
import 'dart:io';
import 'dart:typed_data';
import 'package:servicestack/client.dart';

var client = JsonServiceClient(baseUrl);

var result = await client.postToUrl('/v1/chat/completions', 
    OpenAiChatCompletion()
      ..model = 'mixtral:8x22b'
      ..max_tokens = 50
      ..messages = [
        OpenAiMessage()
          ..role = 'user'
          ..content = "What's the capital of France?"
      ]);
```

### PHP

Include `servicestack/client` package in your projects `composer.json`:

:::copy
"servicestack/client": "^1.0"
:::

Download AI Server's PHP DTOs:

:::copy
`npx get-dtos php https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```php
use ServiceStack\JsonServiceClient;
use dtos\OpenAiChatCompletion;
use dtos\OpenAiMessage;

$client = new JsonServiceClient(baseUrl);
$client->bearerToken = apiKey;

/** @var {OpenAiChatCompletionResponse} $response */
$result = $client->postUrl('/v1/chat/completions', 
    body: new OpenAiChatCompletion(
        model: "mixtral:8x22b",
        messages: [
            new OpenAiMessage(
                role: "user",
                content: "What's the capital of France?"
            )
        ],
        max_tokens: 50
    ));
```

## Java

Include `net.servicestack:client` package in your projects `build.gradle`:

:::copy
implementation 'net.servicestack:client:1.1.3'
:::

Download AI Server's Java DTOs:

:::copy
`npx get-dtos java https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```java
import net.servicestack.client.*;
import java.util.Collections;

var client = new JsonServiceClient(baseUrl);

OpenAiChatResponse result = client.post("/v1/chat/completions", 
    new OpenAiChatCompletion()
        .setModel("mixtral:8x22b")
        .setMaxTokens(50)
        .setMessages(Utils.createList(new OpenAiMessage()
                .setRole("user")
                .setContent("What's the capital of France?")
        )),
    OpenAiChatResponse.class);
```

## Kotlin

Include `net.servicestack:client` package in your projects `build.gradle`:

:::copy
implementation 'net.servicestack:client:1.1.3'
:::

Download AI Server's Kotlin DTOs:

:::copy
`npx get-dtos kotlin https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```kotlin
package myapp
import net.servicestack.client.*

val client = JsonServiceClient(baseUrl)

val result: OpenAiChatResponse = client.post("/v1/chat/completions", 
    OpenAiChatCompletion().apply {
        model = "mixtral:8x22b"
        messages = arrayListOf(OpenAiMessage().apply {
            role = "user"
            content = "What's the capital of France?"
        })
        maxTokens = 50
    }, 
    OpenAiChatResponse::class.java)
```

## Swift

Include `ServiceStack` package in your projects `Package.swift`

```swift
dependencies: [
    .package(url: "https://github.com/ServiceStack/ServiceStack.Swift.git",
        Version(6,0,0)..<Version(7,0,0)),
],
```

Download AI Server's Swift DTOs:

:::copy
`npx get-dtos swift https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with JsonServiceClient:

```swift
import Foundation
import ServiceStack

let client = JsonServiceClient(baseUrl:baseUrl)

let request = OpenAiChatCompletion()
request.model = "mixtral:8x22b"
let msg =  OpenAiMessage()
msg.role = "user"
msg.content = "What's the capital of France?"
request.messages = [msg]
request.max_tokens = 50

let result:OpenAiChatResponse = try await client.postAsync(
    "/v1/chat/completions", request:request)
```

## F#

Install the `ServiceStack.Client` NuGet package:

:::copy
`<PackageReference Include="ServiceStack.Client" Version="8.*" />`
:::

Download AI Server's F# DTOs with [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::copy
`x fsharp https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with `JsonApiClient`:

```fsharp
open ServiceStack
open ServiceStack.Text

let client = new JsonApiClient(baseUrl)

let result = client.Post<OpenAiChatCompletionResponse>("/v1/chat/completions", 
     OpenAiChatCompletion(
        Model = "mixtral:8x22b",
        Messages = ResizeArray [
            OpenAiMessage(
                Role = "user",
                Content = "What's the capital of France?"
            )
        ],
        MaxTokens = 50))
```

## VB.NET

Install the `ServiceStack.Client` NuGet package:

:::copy
`<PackageReference Include="ServiceStack.Client" Version="8.*" />`
:::

Download AI Server's VB.NET DTOs with [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::copy
`x vbnet https://openai.servicestack.net`
:::

Call API by sending `OpenAiChatCompletion` Request DTO with `JsonApiClient`:

```vb
Imports ServiceStack
Imports ServiceStack.Text

Dim client = New JsonApiClient(baseUrl)

Dim result = Await client.PostAsync(Of OpenAiChatResponse)(
    "/v1/chat/completions",
    New OpenAiChatCompletion() With {
        .Model = "mixtral:8x22b",
        .Messages = New List(Of OpenAiMessage) From {
            New OpenAiMessage With {
                .Role = "user",
                .Content = "What's the capital of France?"
            }
        },
        .MaxTokens = 50
    })
```


# Simple API Keys Credentials Provider for .NET 8 C# Microservices
Source: https://servicestack.net/posts/apikey-credentials-auth

The usability of the [Simple Auth with API Keys](https://docs.servicestack.net/auth/admin-apikeys) story has
been significantly improved with the new `ApiKeyCredentialsProvider` which enables .NET Microservices to provide
persistent UserSession-like behavior using simple API Keys which can be configured together with the
`AuthSecretAuthProvider` and `ApiKeysFeature` to enable a Credentials Auth implementation which users can
use with their API Keys or Admin AuthSecret.

A typical configuration for .NET Microservices looking to enable Simple Auth access whose APIs are protected
by API Keys and their Admin functionality protected by an Admin Auth Secret can be configured with:

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services =>
        {
            services.AddPlugin(new AuthFeature([
                new ApiKeyCredentialsProvider(),
                new AuthSecretAuthProvider("MyAuthSecret"),
            ]));
            services.AddPlugin(new SessionFeature());
            services.AddPlugin(new ApiKeysFeature());
        })
        .ConfigureAppHost(appHost =>
        {
            using var db = HostContext.AppHost.GetDbConnection();
            appHost.GetPlugin<ApiKeysFeature>().InitSchema(db);
        });
}
```

When registered a Credentials Auth dialog will appear for [ServiceStack Built-in UIs](https://servicestack.net/auto-ui)
allowing users to Sign In with their **API Keys** or Admin **Auth Secret**.

![](/img/posts/apikey-credentials-auth/ai-server-auth-apiexplorer.png)

### Session Auth with API Keys

Behind the scenes this creates a Server [Auth Session](https://docs.servicestack.net/auth/sessions)
but instead of maintaining an Authenticated User Session it saves the API Key in the session then attaches the API Key
to each request. This makes it possible to make API Key validated requests with just a session cookie instead of
requiring resubmission of API Keys for each request.

### AI Server

This is an ideal Auth Configuration for .NET Docker Appliances and Microservices like [AI Server](/posts/ai-server)
that don't need the complexity of ASP .NET Core's Identity Auth machinery and just want to restrict access to their APIs
with API Keys and restrict Admin functionality to Administrator's with an Auth Secret.

The benefit of `ApiKeyCredentialsProvider` is that it maintains a persistent Session so that end users
only need to enter their API Key a single time and they'll be able to navigate to all of AI Server's protected
pages using their API Key maintained in their Server User Session without needing to re-enter it for each UI and
every request.

### User Access with API Keys

AI Server uses **API Keys** to restrict Access to their AI Features to **authorized Users** with Valid API Keys who
are able to use its Built-in UIs for its AI Features with the Users preferred Name and issued API Key:

![](/img/posts/apikey-credentials-auth/ai-server-auth-user.png)

After signing in a single time they'll be able to navigate to any protected page and start using AI Server's
AI features:

![](/img/posts/apikey-credentials-auth/ai-server-auth-user-chat.png)

### User Access to API Explorer

This also lets users use their existing Auth Session across completely different UIs
like [API Explorer](https://docs.servicestack.net/api-explorer)
where they'll have the same access to APIs as they would when calling APIs programatically with their API Keys, e.g:

![](/img/posts/apikey-credentials-auth/ai-server-auth-apiexplorer-api.png)

### Coarse or fine-grained API Key access

By default **any** Valid API Key can access restricted services by `[ValidateApiKey]`

```csharp
[ValidateApiKey]
public class Hello : IGet, IReturn<HelloResponse>
{
    public required string Name { get; set; }
}
```

### API Key Scopes

API Keys can be given elevated privileges where only Keys with user defined scopes:

![](https://docs.servicestack.net/img/pages/auth/simple/admin-ui-apikeys-edit.png)

Are allowed to access APIs restricted with that scope:

```csharp
[ValidateApiKey("todo:read")]
public class QueryTodos : QueryDb<Todo>
{
    public long? Id { get; set; }
    public List<long>? Ids { get; set; }
    public string? TextContains { get; set; }
}
```

### Restricted API Keys to specific APIs 

API Keys can also be locked down to only be allowed to call specific APIs:

![](https://docs.servicestack.net/img/pages/auth/simple/admin-ui-apikeys-restrict-to.png)

## Admin Access

AI Server also maintains an Admin UI and Admin APIs that are only accessible to **Admin** users who 
Authenticate with the App's configured Admin Auth Secret who are able to access AI Server's Admin
UIs to monitor Live AI Requests, create new User API Keys, Manage registered AI Providers, etc. 

![](/img/posts/apikey-credentials-auth/ai-server-auth-admin-jobs.png)

### Admin Restricted APIs

You can restrict APIs to Admin Users by using `[ValidateAuthSecret]`: 

```csharp
[Tag(Tags.Admin)]
[ValidateAuthSecret]
[Api("Add an AI Provider to process AI Requests")]
public class CreateAiProvider : ICreateDb<AiProvider>, IReturn<IdResponse>
{
    //...
}
```

Which are identified in API Explorer with a **padlock** icon whilst APIs restricted by API Key are 
identified with a **key** icon:

![](/img/posts/apikey-credentials-auth/ai-server-auth-apiexplorer-admin.png)


# Podcasts now in Razor SSG
Source: https://servicestack.net/posts/razor-ssg-podcasts

## Razor SSG now supports Podcasts!

[Razor SSG](https://razor-ssg.web-templates.io) is our FREE Project Template for creating fast, statically generated Websites and Blogs with
Markdown & C# Razor Pages. A benefit of using Razor SSG to maintain our
[github.com/ServiceStack/servicestack.net](https://github.com/ServiceStack/servicestack.net) website is that 
any improvements added to **servicestack.net** end up being rolled into the Razor SSG Project Template 
for everyone else to enjoy.

The latest feature recently added is [ServiceStack Podcasts](https://servicestack.net/podcasts), providing an easy alternative to 
learning about new features in our [TL;DR Release Notes](https://docs.servicestack.net/releases/v8_04) during a commute as well as a
fun and more informative experience whilst reading [blog posts](https://servicestack.net/blog). 

The same podcast feature has now been rolled into the Razor SSG template allowing anyone to add the same
feature to their Razor SSG Websites which can be developed and hosted for FREE on GitHub Pages CDN:

### Create a new Razor SSG Project

<project-creator v-slot="x">
    <project-template :name="x.text" repo="NetCoreTemplates/razor-ssg" :tags="['ssg','markdown']">
        <div class="mb-3 text-xl font-medium text-gray-700 dark:text-gray-200">Razor SSG</div>
        <template #icon>
            <svg class="w-12 h-12 text-indigo-600" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentColor" d="M23.844 27.692a16.332 16.332 0 0 1-6.645 1.3q-6.364 0-10.013-3.243a11.3 11.3 0 0 1-3.649-8.9a13.716 13.716 0 0 1 3.785-9.898A12.716 12.716 0 0 1 16.9 3.008a11.676 11.676 0 0 1 8.425 3.006a9.994 9.994 0 0 1 3.142 7.533a10.187 10.187 0 0 1-2.318 7.114a7.532 7.532 0 0 1-5.817 2.547a2.613 2.613 0 0 1-1.845-.642a2.323 2.323 0 0 1-.764-1.6a4.9 4.9 0 0 1-4.148 2.243a4.6 4.6 0 0 1-3.507-1.479a5.706 5.706 0 0 1-1.384-4.063a9.913 9.913 0 0 1 2.2-6.357q2.2-2.763 4.8-2.763a5.063 5.063 0 0 1 4.256 1.716l.311-1.338h2.405l-2.081 9.08a10.716 10.716 0 0 0-.352 2.243q0 .972.744.972a4.819 4.819 0 0 0 3.877-2.047a8.93 8.93 0 0 0 1.621-5.681a7.98 7.98 0 0 0-2.675-6.175a9.887 9.887 0 0 0-6.919-2.432a10.6 10.6 0 0 0-8.158 3.467a12.066 12.066 0 0 0-3.2 8.495a9.561 9.561 0 0 0 3.06 7.573q3.06 2.7 8.586 2.7a13.757 13.757 0 0 0 5.675-1.054ZM19.466 12.25a3.977 3.977 0 0 0-3.6-1.716q-1.824 0-3.263 2.23a8.726 8.726 0 0 0-1.439 4.824q0 3.635 2.905 3.635a3.771 3.771 0 0 0 2.651-1.183a6.309 6.309 0 0 0 1.7-3.2Z"></path></svg>
        </template>
    </project-template>
</project-creator>

### Markdown Powered

The Podcast feature is very similar to the Markdown Blog Posts where each podcast is a simple
`.md` Markdown page seperated by a publish date and its unique slug, e.g:

**[/_podcasts](https://github.com/NetCoreTemplates/razor-ssg/tree/main/MyApp/_podcasts)**

```files
/_pages
/_podcasts
    config.json
    2024-10-02_razor-ssg-podcasts.md
    2024-09-19_scalable-sqlite.md
    2024-09-17_sqlite-request-logs.md
    ...
/_posts
/_videos
/_whatsnew
```

All editable content within different Podcast pages like the Podcast Sidebar is customizable within 
[_podcasts/config.json](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_podcasts/config.json).

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-layout.webp)](https://razor-ssg.web-templates.io/podcasts)

### Podcast Page

Whilst all content about a podcast is contained within its `.md` file and frontmatter which just like
Blog Posts can contain interactive Vue Components and custom [Markdown Containers](https://razor-press.web-templates.io/containers).

The [Backgrounds Jobs Podcast Page](https://razor-ssg.web-templates.io/podcasts/background-jobs) is a
good example of this where its [2024-09-12_background-jobs.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_podcasts/2024-09-12_background-jobs.md?plain=1)
contains both a `<project-creator>` Vue Component as well as `sh` and `youtube` custom markdown
containers to render its page:

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-page.webp)](https://razor-ssg.web-templates.io/podcasts/background-jobs)

### Audio Player

Podcasts are played using the [AudioPlayer.mjs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/wwwroot/pages/podcasts/AudioPlayer.mjs)
Vue Component that's enabled on each podcast page which will appear at the bottom of the page when played:

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-audioplayer.webp)](https://razor-ssg.web-templates.io/podcasts)

The `AudioPlayer` component is also independently usable as a standard Vue Component in
markdown content like [this .md page](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_posts/2024-10-02_razor-ssg-podcasts.md?plain=1#L72):

```html
<audio-player id="scalable-sqlite" title="Scalable SQLite" variant="compact"
    src="https://media.servicestack.com/podcasts/scalable-sqlite.mp3">
</audio-player>
```

:::{.py-8 .mx-auto .w-2/3 .not-prose}
<audio-player id="scalable-sqlite" title="Scalable SQLite" variant="compact"
src="https://media.servicestack.com/podcasts/scalable-sqlite.mp3">
</audio-player>
:::

It can also be embeddable inside Razor `.cshtml` pages using 
[Declarative Vue Components](https://servicestack.net/posts/net8-best-blazor#declarative-vue-components), e.g:

```html
@{
    var episode = Podcasts.GetEpisodes().FirstOrDefault(x => x.Slug == doc.Slug);
    <div data-component="pages/podcasts/AudioPlayer.mjs"
         data-props="{ id:'@episode.Slug', title:'@episode.Title', src:'@episode.Url', variant:'compact' }"
         class="mt-4 md:w-[450px] md:mt-0"></div>
}
```

### Dark Mode

As Razor SSG is built with Tailwind CSS, Dark Mode is also easily supported:

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-dark.webp)](https://razor-ssg.web-templates.io/podcasts/background-jobs)

### Browse by Tags

Just like [blog post archives](https://razor-ssg.web-templates.io/posts/), the frontmatter collection of `tags` is used to generate related podcast pages, 
aiding discoverability by grouping related podcasts by **tag** at the following route: 

    /podcasts/tagged/{tag}

https://razor-ssg.web-templates.io/podcasts/tagged/release

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-tag.webp)](https://razor-ssg.web-templates.io/podcasts/tagged/release)

### Browse by Year

Likewise podcast archives are also browsable by the year their published at the route:

    /podcasts/year/{year}

https://razor-ssg.web-templates.io/podcasts/year/2024

[![](/img/posts/razor-ssg-podcasts/razor-ssg-podcast-year.webp)](https://razor-ssg.web-templates.io/podcasts/year/2024)

### iTunes-compatible Podcast RSS Feed

The information in [config.json](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_podcasts/config.json)
is also used in the generated podcast RSS feed at:

[/podcasts/feed.xml](https://razor-ssg.web-templates.io/podcasts/feed.xml)

Which is a popular format podcast Applications can use to get notified when new Podcast
episodes are available. The RSS Feed is also compatible with [podcasters.apple.com](https://podcasters.apple.com)
and can be used to publish your podcast to [Apple Podcasts](https://podcasts.apple.com).

```xml
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:podcast="https://podcastindex.org/namespace/1.0" xmlns:media="http://search.yahoo.com/mrss/" version="2.0">
    <channel>
        <title>Their Side</title>
        <link>https://razor-ssg.web-templates.io/podcasts</link>
        <description><![CDATA[ Conversations with the most tragically misunderstood people of our time. ]]></description>
        <image>
            <url>https://razor-ssg.web-templates.io/img/posts/cover.png</url>
            <title>Their Side</title>
            <link>/podcasts</link>
        </image>
        <generator>razor-ssg</generator>
        <copyright>Razor SSG</copyright>
        <lastBuildDate>Wed, 02 Oct 2024 03:54:03 GMT</lastBuildDate>
        <managingEditor>email@example.org (Razor SSG)</managingEditor>
        <webMaster>email@example.org (Razor SSG)</webMaster>
        <atom:link href="https://razor-ssg.web-templates.io/podcasts/feed.xml" rel="self" type="application/rss+xml" />
        <itunes:author>Razor SSG</itunes:author>
        <itunes:owner>
            <itunes:name>Razor SSG</itunes:name>
            <itunes:email>email@example.org</itunes:email>
        </itunes:owner>
        <itunes:image href="https://razor-ssg.web-templates.io/img/posts/cover-1920.jpg"/>
...
```


# Scalable Server SQLite Apps
Source: https://servicestack.net/posts/scalable-sqlite

Ever since adding [support for Litestream](https://docs.servicestack.net/ormlite/litestream) in
our project's templates [GitHub Action Deployments](https://servicestack.net/posts/kubernetes_not_required)
we've been using SQLite as the backend for our new .NET C# Apps as it's the 
[most cost-effective option](https://docs.servicestack.net/ormlite/litestream#savings-at-scale)
that frees us from needing to use a cloud managed database which lets us make use of Hetzner's much cheaper
[US Cloud VMs](https://www.hetzner.com/cloud/).

We're also seeing increased usage of SQLite Server Apps with [Bluesky Social](https://github.com/bluesky-social/atproto/pull/1705)
having moved to SQLite and all of 37 Signals new [Once](https://once.com) Web Apps 
[using SQLite](https://world.hey.com/dhh/multi-tenancy-is-what-s-hard-about-scaling-web-services-dd1e0e81), 
Tailscale having migrated their [primary database to SQLite](https://tailscale.com/blog/database-for-2022)
whos ex-Google founders have [been using since 2018](https://www.youtube.com/watch?v=RqubKSF3wig)
and Cloud Providers building distributed databases on top of SQLite like 
[Cloudflare D1](https://blog.cloudflare.com/introducing-d1/) and Fly.io's 
multi-region distributed [LiteFS](https://fly.io/docs/litefs/) solution.

SQLite is a highly-performant DB that can handle a large number of concurrent read operations and
[35% Faster](https://www.sqlite.org/fasterthanfs.html) filesystem performance for write operations with next
to no latency that's often faster than other RDBMS's courtesy of its proximity to the running application which gives it unique advantages over traditional client/server RDBMS's where it's not susceptible to the
[N+1 Queries problem](https://www.sqlite.org/np1queryprob.html) and is also able to execute your
custom C# Logic inside SQL Queries using [Application SQL Functions](https://www.sqlite.org/appfunc.html).

With [litestream.io](https://litestream.io) taking care of real-time replication to managed storage
we just need to workaround SQLite's single concurrent writer to unlock the value, performance and
unique features of SQLite in our Apps which we cover in this release with integrated support for
Database Locks and Sync Commands.

## Single Concurrent Writer

The primary limitation of SQLite is that it only supports a single concurrent writer, which means if you 
have multiple requests writing to the same database at the same time, they will need to coordinate access. 

As long as we can overcome this limitation SQLite can be an excellent choice to power many Web Apps. In the
previous ServiceStack v8.3 release we [worked around this limitation](https://docs.servicestack.net/commands#use-case-sqlite-writes)
by using [MQ Command DTOs](https://docs.servicestack.net/commands#mq-command-dtos) to route all DB
Writes to be executed by a single Background MQ Thread.

This works great for [messaging-based architectures](https://docs.servicestack.net/commands#messaging-workflow)
where you can queue commands to be processed serially, but the overhead of using commands for all 
DB writes can be cumbersome when needing to perform sporadic writes within complex logic.

## Multiple SQLite Databases

Firstly a great way to reduce contention is to use separate SQLite databases for different
subsystems of your Application that way load is distributed across multiple DBs
and writes across each SQLite database can be executed concurrently.

This is especially important for write heavy operations like
[SQLite Request Logging](/posts/sqlite-request-logs) or if your App stores every interaction 
of your App for A/B testing, storing them in separate `analytics.db` databases will remove 
any contention from your primary database.

The other techniques below demonstrates concurrent safe techniques for accessing an SQLite DB: 

### Always use Synchronous APIs for SQLite

Generally it's recommended to use non-blocking Async APIs for any I/O Operations however as
SQLite doesn't make Network I/O requests and its native implementation is blocking, its Async DB
APIs are just pseudo-async wrappers around SQLite's blocking APIs which just adds unnecessary
overhead. For this reason we recommend **always** using synchronous APIs for SQLite, especially 
as it's not possible to await inside a lock:

```csharp
lock (Locks.AppDb)
{
    //Can't await inside a lock
    //await Db.UpdateAsync(row); 
    Db.Update(row);
}
```

It's also safe to assume SQLite will always block since all
[Asynchronous I/O efforts](https://www.sqlite.org/asyncvfs.html) were abandoned in favor
of [WAL mode](https://www.sqlite.org/wal.html) which mitigates the cost of blocking **fsync()**.

## Database Locks

The new `Locks` class maintains an object lock for each registered database connection that can be 
used to synchronize **write access** for different SQLite databases, e.g:

```js
var row = db.SingleById<Table>(request.Id);
row.PopulateWithNonDefaultValues(request);
lock (Locks.AppDb)
{
    Db.Update(row);
}
```

`Locks.AppDb` can be used synchronize db writes for the App's primary database, e.g. `App_Data/app.db`.

Whilst `Locks.GetDbLock(namedConnection)` can be used to get the DB Write Lock for any other 
[registered SQLite Database](https://docs.servicestack.net/ormlite/multi-database-app) by using
the same named connection the SQLite Database Connection was registered against, e.g:

```csharp
var dbFactory = new OrmLiteConnectionFactory(connStr, SqliteDialect.Provider);
dbFactory.RegisterConnection(Databases.Search, 
    $"DataSource=App_Data/search.db;Cache=Shared", SqliteDialect.Provider);
dbFactory.RegisterConnection(Databases.Analytics, 
    $"DataSource=App_Data/analytics.db;Cache=Shared", SqliteDialect.Provider);

//...
using var dbSearch = dbFactory.Open(Database.Search);
lock (Locks.GetDbLock(Database.Search))
{
    dbSearch.Insert(row);
}

using var dbAnalytics = dbFactory.Open(Database.Analytics);
lock (Locks.GetDbLock(Database.Analytics))
{
    dbAnalytics.Insert(row);
}
```

## Queuing DB Writes with SyncCommand

`Locks` are a great option for synchronizing DB Writes that need to be executed within
complex logic blocks, however locks can cause contention in highly concurrent Apps.
One way to remove contention is to serially execute DB Writes instead which we can
do by executing DB Writes within `SyncCommand*` commands and using a named `[Worker(Workers.AppDb)]`
attribute for Writes to the primary database, e.g: 

```csharp
[Worker(Workers.AppDb)]
public class DeleteCreativeCommand(IDbConnection db) 
    : SyncCommand<DeleteCreative>
{
    protected override void Run(DeleteCreative request)
    {
        var artifactIds = request.ArtifactIds;
        db.Delete<AlbumArtifact>(x => artifactIds.Contains(x.ArtifactId));
        db.Delete<ArtifactReport>(x => artifactIds.Contains(x.ArtifactId));
        db.Delete<ArtifactLike>(x => artifactIds.Contains(x.ArtifactId));
        db.Delete<Artifact>(x => x.CreativeId == request.Id);
        db.Delete<CreativeArtist>(x => x.CreativeId == request.Id);
        db.Delete<CreativeModifier>(x => x.CreativeId == request.Id);
        db.Delete<Creative>(x => x.Id == request.Id);
    }
}
```

Other databases should use its named connection for its named worker, e.g: 

```csharp
[Worker(Databases.Search)]
public class DeleteSearchCommand(IDbConnectionFactory dbFactory) 
    : SyncCommand<DeleteSearch>
{
    protected override void Run(DeleteSearch request)
    {
        using var db = dbFactory.Open(Databases.Search);
        db.DeleteById<ArtifactFts>(request.Id);
        //...
    }
}
```

Where it will be executed within its Database Lock. 

## Executing Commands

Now everytime the commands are executed they will be added to a ConcurrentQueue
where they'll be serially executed by the worker's Background Task: 

```csharp
public class MyServices(IBackgroundJobs jobs) : Service
{
    public void Any(DeleteCreative request)
    {
        // Queues a durable job to execute the command with the named worker
        var jobRef = jobs.EnqueueCommand<DeleteCreativeCommand>(request);
        // Returns immediately with a reference to the Background Job
    }

    public async Task Any(DeleteSearch request)
    {
        // Executes a transient (i.e. non-durable) job with the named worker
        var result = await jobs.RunCommandAsync<DeleteSearchCommand>(request);
        // Returns after the command is executed with its result (if any)
    }
}
```

When using any `SyncCommand*` base class, its execution still uses database locks
but any contention is alleviated as they're executed serially by a single worker thread.

### AutoQuery Crud Database Write Locks

To avoid SQLite concurrency write exceptions all DB Writes should be executed within
its database lock or a named worker. Including the auto-generated [AutoQuery Crud](https://docs.servicestack.net/autoquery/crud)
APIs which will implicitly use Database Locks if the **primary database is SQLite**.

AutoQuery CRUD can also be explicitly configured to always be executed within Database Locks with:

```csharp
services.AddPlugin(new AutoQueryFeature {
    UseDatabaseWriteLocks = true
});
```

## SQLite Web Apps

That's about it, by using any of the above techniques to guard against concurrent writes
you can take advantage of the [simplicity, value and performance benefits](https://docs.servicestack.net/ormlite/litestream#the-right-time-for-server-side-sqlite)
of SQLite in your Apps and utilize a solution like [litestream.io](https://litestream.io)
for real-time replication of your SQLite databases to highly reliable managed storage.

SQLite's [Checklist For Choosing The Right Database Engine](https://www.sqlite.org/whentouse.html#checklist_for_choosing_the_right_database_engine)
covers the few situations when a traditional Client/Server RDBMS will be more appropriate.

The primary use-case would be when your App needs to be distributed across multiple App Servers 
as using SQLite essentially forces you into scaling up, which gets more appealing every year 
with hardware getting cheaper and faster and cheap hosting providers like [hetzner.com](https://www.hetzner.com)
where you can get bare metal 48 Core/96 vCore EPYC Servers with fast NVMe SSDs for **€236** per month - 
a fraction of the cost of comparable performance with any cloud managed solution

[![](/img/posts/scalable-sqlite/hetzner-epyc-48.webp)](https://www.hetzner.com/dedicated-rootserver/)

Which is a fraction of what it would cost for comparable performance using cloud managed databases:

[![](/img/posts/scalable-sqlite/azure-sql-database.webp)](https://azure.microsoft.com/en-us/pricing/details/azure-sql-database/single/)

In the rare cases where you need to scale beyond a single server you'll initially be able to 
scale out your different App databases onto different servers.

Beyond that, if your App permits you may be able to adopt a multi-tenant architecture like 
[Bluesky Social](https://bsky.social/about) with each tenant having their own SQLite database 
to effectively enable infinite scaling.

For further info on using high performance SQLite in production web apps check out
[@aarondfrancis](https://x.com/aarondfrancis) comprehensive website and course at 
[highperformancesqlite.com](https://highperformancesqlite.com) - 
which contains a lot of great content accessible for free.

## Example SQLite Apps

Our confidence in SQLite being the best choice for many web applications has led us to adopt
it to power our latest web applications which are all 
[deployed to a shared Hetzner VM](/posts/kubernetes_not_required) 
whose [inexpensive hosting costs](/posts/jamstacks_hosting) allows us to host 
and make them **available for free!**

All projects are open-source and employ the different techniques detailed above that should serve
as a great resource of how they're used in real-world Web Applications:

### Blazor Diffusion

Generate images for free using custom [Civit AI](https://civitai.com) and [FLUX-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell)
models:

[![](/img/posts/scalable-sqlite/blazordiffusion.webp)](https://blazordiffusion.com)

- Website: [blazordiffusion.com](https://blazordiffusion.com)
- GitHub: [github.com/NetCoreApps/BlazorDiffusionVue](https://github.com/NetCoreApps/BlazorDiffusionVue/)

### pvq.app

An OSS alternative to StackOverflow which uses the best proprietary and OSS Large Language Models 
to answer your technical questions. [pvq.app](https://pvq.app) is populated with over **1M+ answers** for the highest
rated StackOverflow questions - checkout [pvq.app/leaderboard](https://pvq.app/leaderboard) 
to find the best performing LLM models (results are surprising!)

[![](/img/posts/scalable-sqlite/pvq.webp)](https://pvq.app)

- Website: [pvq.app](https://pvq.app)
- GitHub: [github.com/ServiceStack/pvq.app](https://github.com/ServiceStack/pvq.app)

### AI Server

The independent Microservice used to provide all AI Features used by the above applications. 
It's already been used to execute millions of LLM and Comfy UI Requests to generate Open AI Chat Answers
and Generated Images used to populate the
[blazordiffusion.com](https://blazordiffusion.com) and [pvq.app](https://pvq.app) websites. 

It was the project used to develop and test [Background Jobs](/posts/background-jobs) in action 
where it serves as a private gateway to process all LLM, AI and image transformations requests 
that any of our Apps need where it dynamically delegates requests across multiple Ollama, 
Open AI Chat, LLM Gateway, Comfy UI, Whisper and ffmpeg providers. 

[![](/img/posts/scalable-sqlite/ai-server.webp)](https://openai.servicestack.net)
[![](/img/posts/scalable-sqlite/ai-server-chat.webp)](https://openai.servicestack.net)

- Website: [openai.servicestack.net](https://openai.servicestack.net)
- GitHub: [github.com/ServiceStack/ai-server](https://github.com/ServiceStack/ai-server)

In addition to maintaining a history of AI Requests, it also provides file storage
for its CDN-hostable AI generated assets and on-the-fly, cacheable image transformations.

### Private AI Gateway

We're developing AI Server as a **Free OSS Product** that runs as a single Docker Container
Microservice that Admins can use its built-in UIs to add multiple Ollama instances, 
Open AI Gateways to execute LLM requests and Client Docker agents installed with Comfy UI, 
ffmpeg and Whisper to handle all other non-LLM Requests. 

#### Multiple Ollama, Open AI Gateway and Comfy UI Agents

The AI Server Docker container itself wont require any infrastructure dependencies or 
specific hardware requirements, however any Ollama endpoints or Docker Comfy UI Agents added 
will need to run on GPU-equipped servers.

#### Native end-to-end Typed Integrations to most popular languages

ServiceStack's [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference)
feature is used to provide native typed integrations to C#, TypeScript, JavaScript, Python, PHP, Swift, Java, 
Kotlin, Dart, F# and VB.NET projects which organizations can drop into their heterogeneous
environments to manage their private AI Services used across their different Apps.

#### Protected Access with API Keys

AI Server utilizes [Simple Auth with API Keys](https://docs.servicestack.net/auth/admin-apikeys)
letting Admins create and distribute API Keys to only allow authorized clients to access their 
AI Server's APIs, which can be optionally further restricted to only
[allow access to specific APIs](https://docs.servicestack.net/auth/apikeys#creating-user-api-keys).


### AI Server V1

[AI Server V1](/posts/ai-server) is now released!

The initial V1 release comes packed with features, including:

#### Large Language Models
- Open AI Chat
  - Support for Ollama endpoints 
  - Support for Open Router, Open AI, Mistral AI, Google and Groq API Gateways

#### Comfy UI Agent / Replicate / DALL-E 3
- Text to Image

#### Comfy UI Agent
- Image to Image
  - Image Upscaling
  - Image with Mask
- Image to Text
- Text to Audio
- Text to Speech
- Speech to Text

#### ffmpeg 
- image/video/audio format conversions
- image/video scaling
- image/video cropping
- image/video watermarking
- video trimming

#### Managed File Storage
- Blob Storage - isolated and restricted by API Key

### AI Server Feedback

Feel free to reach us at [ai-server/discussions](https://github.com/ServiceStack/ai-server/discussions) 
with any AI Server questions.


# SQLite C# Request Logs
Source: https://servicestack.net/posts/sqlite-request-logs

Up until this release all of ServiceStack's database features like [AutoQuery](https://servicestack.net/autoquery)
have been database agnostic courtesy of OrmLite's [support for popular RDBMS's](https://docs.servicestack.net/ormlite/installation)
so that they integrate into an App's existing configured database.

[Background Jobs](/posts/background-jobs) is our first foray into a SQLite-only backend, as it's the only 
RDBMS that enables us to provide encapsulated black-box functionality without requiring any infrastructure 
dependencies. It's low latency, high-performance and ability to create lightweight databases on the fly make 
it ideal for self-managing isolated appliance backends like Background Jobs and Request Logging which don't 
benefit from integrating with your existing RDBMS.

The new [ServiceStack.Jobs](https://nuget.org/packages/ServiceStack.Jobs) NuGet package allows us
to deliver plug and play SQLite backed features into .NET 8 C# Apps that are configured with any RDBMS
or without one. The next feature added is a SQLite backed provider for [Request Logs](https://docs.servicestack.net/request-logger) 
with the new `SqliteRequestLogger` which can be added to existing .NET 8 Apps with the
[mix tool](https://docs.servicestack.net/mix-tool):

:::sh
x mix sqlitelogs
:::

Which adds a reference to **ServiceStack.Jobs** and the [Modular Startup](https://docs.servicestack.net/modular-startup) config below:

```csharp
using ServiceStack.Jobs;
using ServiceStack.Web;

[assembly: HostingStartup(typeof(MyApp.ConfigureRequestLogs))]

namespace MyApp;

public class ConfigureRequestLogs : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            
            services.AddPlugin(new RequestLogsFeature {
                RequestLogger = new SqliteRequestLogger(),
                EnableResponseTracking = true,
                EnableRequestBodyTracking = true,
                EnableErrorTracking = true
            });
            services.AddHostedService<RequestLogsHostedService>();
        });
}

public class RequestLogsHostedService(ILogger<RequestLogsHostedService> log, IRequestLogger requestLogger) : BackgroundService
{
    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        var dbRequestLogger = (SqliteRequestLogger)requestLogger;
        using var timer = new PeriodicTimer(TimeSpan.FromSeconds(3));
        while (!stoppingToken.IsCancellationRequested && await timer.WaitForNextTickAsync(stoppingToken))
        {
            dbRequestLogger.Tick(log);
        }
    }
}
```

This will use a Hosted Background Service to flush Request Logs to the requests SQLite database 
every **3** seconds (configurable in the PeriodicTimer).

If your App is already using `RequestLogsFeature` configured (e.g. with Profiling) you'll want to
remove it:

```csharp
public class ConfigureProfiling : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            // services.AddPlugin(new RequestLogsFeature());
            services.AddPlugin(new ProfilingFeature
            {
                IncludeStackTrace = true,
            });
        });
}
```

## Rolling Monthly SQLite Databases

The benefit of using SQLite is that databases can be created on-the-fly where Requests will be persisted
into isolated **requests** Monthly databases which can be easily archived into managed file storage instead 
of a singular growing database, visible in the [Database Admin UI](https://docs.servicestack.net/admin-ui-database):

![](/img/posts/sqlite-request-logs/sqlite-databases.webp)

SQLite logs will also make it easier to generate monthly aggregate reports to provide key insights
into the usage of your App.

## AutoQuery Grid Admin Logging UI

As SQLite Requests Logs also makes it efficiently possible to sort and filter through logs, the
Logging UI will switch to using a fully queryable `AutoQueryGrid` when using `SqliteRequestLogger`:

![](/img/posts/sqlite-request-logs/sqlite-request-logs.webp)


# Simple C# Background Jobs & Recurring Tasks for .NET 8
Source: https://servicestack.net/posts/background-jobs

We're excited to announce **Background Jobs** our effortless solution for queueing and managing 
background jobs and scheduled tasks in any .NET 8 C# App, implemented in true ServiceStack fashion
where it seamlessly integrates into existing Apps and call existing APIs and sports a 
[built-in](/auto-ui) Management UI to provide real-time monitoring, inspection and management of background jobs.

:::youtube 2Cza_a_rrjA
Durable C# Background Jobs and Scheduled Tasks for .NET
:::

### Durable and Infrastructure-Free

Prior to Background Jobs we've been using [Background MQ](https://docs.servicestack.net/background-mq) for executing
our background tasks which lets you queue any Request DTO to execute its API in a background worker.
It's been our preferred choice as it didn't require any infrastructure dependencies since its concurrent
queues are maintained in memory, this also meant they were non-durable that didn't survive across App restarts. 
Whilst [ServiceStack MQ](https://docs.servicestack.net/messaging) enables an additional endpoint for your APIs our main use-case for using 
it was for executing background tasks which would be better suited by purpose-specific software 
designed for the task.

#### SQLite Persistence

It uses SQLite as the backing store for its durability since it's low latency, 
[fast disk persistence](https://www.sqlite.org/fasterthanfs.html) and embeddable file-based 
database makes it ideally suited for the task which allows creation of naturally partition-able 
and archivable monthly databases on-the-fly without any maintenance overhead or infrastructure 
dependencies making it easy to add to any .NET App without impacting or adding increased load to 
their existing configured databases.

### Queue APIs or Commands

For even greater reuse you're able to queue your existing ServiceStack APIs
as a Background Job in addition to [Commands](https://docs.servicestack.net/commands) added in the 
[last v8.3 release](https://docs.servicestack.net/releases/v8_03) for encapsulating units of logic
into internal invokable, inspectable and auto-retryable building blocks.

### Real Time Admin UI

The Background Jobs Admin UI provides a real time view into the status of all background jobs including 
their progress, completion times, Executed, Failed and Cancelled Jobs, etc. which is useful for monitoring 
and debugging purposes. 

![](/img/posts/background-jobs/jobs-dashboard.webp)

View Real-time progress of queued Jobs

![](/img/posts/background-jobs/jobs-queue.webp)

View real-time progress logs of executing Jobs

![](/img/posts/background-jobs/jobs-logs.webp)

View Job Summary and Monthly Databases of Completed and Failed Jobs

![](/img/posts/background-jobs/jobs-completed.webp)

View full state and execution history of each Job

![](/img/posts/background-jobs/jobs-failed.webp)

Cancel Running jobs and Requeue failed jobs

### Feature Overview

Even in its v1 release it packs all the features we wanted in a Background Jobs solution:

 - No infrastructure dependencies
   - Monthly archivable rolling Databases with full Job Execution History
 - Execute existing APIs or versatile Commands
   - Commands auto registered in IOC
 - Scheduled Reoccurring Tasks
   - Track Last Job Run
 - Serially execute jobs with the same named Worker
 - Queue Jobs dependent on successful completion of parent Job
 - Queue Jobs to be executed after a specified Date
 - Execute Jobs within the context of an Authenticated User
 - Auto retry failed jobs on a default or per-job limit
 - Timeout Jobs on a default or per-job limit
 - Cancellable Jobs
 - Requeue Failed Jobs
 - Execute custom callbacks on successful execution of Job
 - Maintain Status, Logs and Progress of Executing Jobs
 - Execute transitive (i.e. non-durable) jobs using named workers
 - Attach optional `Tag`, `BatchId`, `CreatedBy`, `ReplyTo` and `Args` with Jobs

Please [let us know](https://servicestack.net/ideas) of any other missing features you'd love to see implemented.

## Install

As it's more versatile and better suited, we've replaced the usage of Background MQ with
**ServiceStack.Jobs** in all **.NET 8 Identity Auth Templates** for sending Identity Auth Confirmation 
Emails when SMTP is enabled. So the easiest way to get started with ServiceStack.Jobs is to 
[create a new Identity Auth Project](https://servicestack.net/start), e.g:

:::sh
x new blazor-vue MyApp
:::

### Exiting .NET 8 Templates

Existing .NET 8 Projects can configure their app to use **ServiceStack.Jobs** by mixing in:

:::sh
x mix jobs
:::

Which adds the `Configure.BackgroundJobs.cs` [Modular Startup](https://docs.servicestack.net/modular-startup)
configuration and a **ServiceStack.Jobs** NuGet package reference to your project.

## Usage

Any API, Controller or Minimal API can execute jobs with the `IBackgroundJobs` dependency, e.g.
here's how you can run a background job to send a new email when an API is called in 
any new Identity Auth template:

```csharp
class MyService(IBackgroundJobs jobs) : Service 
{
    public object Any(MyOrder request)
    {
        var jobRef = jobs.EnqueueCommand<SendEmailCommand>(new SendEmail {
            To = "my@email.com",
            Subject = $"Received New Order {request.Id}",
            BodyText = $"""
                       Order Details:
                       {request.OrderDetails.DumptTable()}
                       """,
        });
        //...
    }
}
```

Which records and immediately executes a worker to execute the `SendEmailCommand` with the specified
`SendEmail` Request argument. It also returns a reference to a Job which can be used later to query
and track the execution of a job.

Alternatively a `SendEmail` API could be executed with just the Request DTO: 

```csharp
var jobRef = jobs.EnqueueApi(new SendEmail {
    To = "my@email.com",
    Subject = $"Received New Order {request.Id}",
    BodyText = $"""
               Order Details:
               {request.OrderDetails.DumptTable()}
               """,
});
```

Although Sending Emails is typically not an API you want to make externally available and would 
want to either [Restrict access](https://docs.servicestack.net/auth/restricting-services) or [limit usage to specified users](https://docs.servicestack.net/auth/identity-auth#declarative-validation-attributes).

In both cases the `SendEmail` Request is persisted into the Jobs SQLite database for durability 
that gets updated as it progresses through the queue.

For execution the API or command is resolved from the IOC before being invoked with the Request.
APIs are executed via the [MQ Request Pipeline](https://docs.servicestack.net/order-of-operations)
and commands executed using the [Commands Feature](https://docs.servicestack.net/commands) where
they'll also be visible in the [Commands Admin UI](https://docs.servicestack.net/commands#command-admin-ui).

### Background Job Options

The behavior for each `Enqueue*` method for executing background jobs can be customized with 
the following options: 

 - `Worker` - Serially process job using a named worker thread 
 - `Callback` - Invoke another command with the result of a successful job 
 - `DependsOn` - Execute jobs after successful completion of a dependent job
   - If parent job fails all dependent jobs are cancelled
 - `UserId` - Execute within an Authenticated User Context
 - `RunAfter` - Queue jobs that are only run after a specified date
 - `RetryLimit` - Override default retry limit for how many attempts should be made to execute a job
 - `TimeoutSecs` - Override default timeout for how long a job should run before being cancelled
 - `RefId` - Allow clients to specify a unique Id (e.g Guid) to track job
 - `Tag` - Group related jobs under a user specified tag
 - `CreatedBy` - Optional field for capturing the owner of a job
 - `BatchId` - Group multiple jobs with the same Id
 - `ReplyTo` - Optional field for capturing where to send notification for completion of a Job
 - `Args` - Optional String Dictionary of Arguments that can be attached to a Job

## Schedule Recurring Tasks

In addition to queueing jobs to run in the background, it also supports scheduling recurring tasks
to execute APIs or Commands at fixed intervals.

:::youtube DtB8KaXXMCM
Schedule your Reoccurring Tasks with Background Jobs!
:::

APIs and Commands can be scheduled to run at either a `TimeSpan` or
[CRON Expression](https://github.com/HangfireIO/Cronos?tab=readme-ov-file#cron-format) interval, e.g:

### CRON Expression Examples

```csharp
// Every Minute Expression
jobs.RecurringCommand<CheckUrlsCommand>(Schedule.Cron("* * * * *"));

// Every Minute Constant
jobs.RecurringCommand<CheckUrlsCommand>(Schedule.EveryMinute, new CheckUrls {
    Urls = urls
});
```

### CRON Format

You can use any **unix-cron format** expression supported by the [HangfireIO/Cronos](https://github.com/HangfireIO/Cronos) library:

```txt
|------------------------------- Minute (0-59)
|     |------------------------- Hour (0-23)
|     |     |------------------- Day of the month (1-31)
|     |     |     |------------- Month (1-12; or JAN to DEC)
|     |     |     |     |------- Day of the week (0-6; or SUN to SAT)
|     |     |     |     |
|     |     |     |     |
*     *     *     *     *
```

The allowed formats for each field include:

| Field            | Format of valid values                     |
|------------------|--------------------------------------------|
| Minute           | 0-59                                       |
| Hour             | 0-23                                       |
| Day of the month | 1-31                                       |
| Month            | 1-12 (or JAN to DEC)                       |
| Day of the week  | 0-6 (or SUN to SAT; or 7 for Sunday)       |

#### Matching all values

To match all values for a field, use the asterisk: `*`, e.g here are two examples in which the minute field is left unrestricted:

- `* 0 1 1 1` - the job runs every minute of the midnight hour on January 1st and Mondays.
- `* * * * *` - the job runs every minute (of every hour, of every day of the month, of every month, every day of the week, because each of these fields is unrestricted too).

#### Matching a range

To match a range of values, specify your start and stop values, separated by a hyphen (-). Do not include spaces in the range. Ranges are inclusive. The first value must be less than the second.

The following equivalent examples run at midnight on Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays (for all months):

- `0 0 * * 1-5`
- `0 0 * * MON-FRI`

#### Matching a list

Lists can contain any valid value for the field, including ranges. Specify your values, separated by a comma (,). Do not include spaces in the list, e.g:

- `0 0,12 * * *` - the job runs at midnight and noon.
- `0-5,30-35 * * * *` - the job runs in each of the first five minutes of every half hour (at the top of the hour and at half past the hour).

### TimeSpan Interval Examples

```csharp
jobs.RecurringCommand<CheckUrlsCommand>(
    Schedule.Interval(TimeSpan.FromMinutes(1)));

// With Example
jobs.RecurringApi(Schedule.Interval(TimeSpan.FromMinutes(1)), new CheckUrls {
    Urls = urls
});
```

That can be registered with an optional **Task Name** and **Background Options**, e.g:

```csharp
jobs.RecurringCommand<CheckUrlsCommand>("Check URLs", Schedule.EveryMinute, 
   new() {
       RunCommand = true // don't persist job
   });
```

:::info
If no name is provided, the Command's Name or APIs Request DTO will be used
:::

### Idempotent Registration

Scheduled Tasks are idempotent where the same registration with the same name will
either create or update the scheduled task registration without losing track of the
last time the Recurring Task, as such it's recommended to always define your App's
Scheduled Tasks on Startup:

```csharp
public class ConfigureBackgroundJobs : IHostingStartup
{
   public void Configure(IWebHostBuilder builder) => builder
     .ConfigureServices((context,services) => {
         services.AddPlugin(new CommandsFeature());
         services.AddPlugin(new BackgroundsJobFeature());
         services.AddHostedService<JobsHostedService>();
     }).ConfigureAppHost(afterAppHostInit: appHost => {
         var services = appHost.GetApplicationServices();

         var jobs = services.GetRequiredService<IBackgroundJobs>();
         
         // App's Scheduled Tasks Registrations:
         jobs.RecurringCommand<MyCommand>(Schedule.Hourly);
     });
}
```

### Background Jobs Admin UI

The last job the Recurring Task ran is also viewable in the Jobs Admin UI:

![](/img/posts/background-jobs/jobs-scheduled-tasks-last-job.webp)

### Executing non-durable jobs

`IBackgroundJobs` also supports `RunCommand*` methods for executing background jobs transiently 
(i.e. non-durable), which is useful for commands that want to be serially executed by a named worker 
but don't need to be persisted.

#### Execute in Background and return immediately

You could use this to queue system emails to be sent by the same **smtp** worker and are happy to 
not have its state and execution history tracked in the Jobs database.

```csharp
var job = jobs.RunCommand<SendEmailCommand>(new SendEmail { ... }, 
    new() {
        Worker = "smtp"
    });
```

In this case `RunCommand` returns the actual `BackgroundJob` instance that will be updated by 
the worker. 

#### Execute in Background and wait for completion

You can also use `RunCommandAsync` if you prefer to wait until the job has been executed. Instead
of a Job it returns the **Result** of the command if it returned one. 

```csharp
var result = await jobs.RunCommandAsync<SendEmailCommand>(new SendEmail {...}, 
    new() {
        Worker = "smtp"
    });
```

### Serially Execute Jobs with named Workers

By default jobs are executed immediately in a new Task, we can also change the behavior to
instead execute jobs one-by-one in a serial queue by specifying them to use the same named 
worker as seen in the example above.

Alternatively you can annotate the command with the `[Worker]` attribute if you **always** want 
all jobs executing the command to use the same worker:

```csharp
[Worker("smtp")]
public class SendEmailCommand(IBackgroundJobs jobs) : SyncCommand<SendEmail>
{
    //...
}
```

### Use Callbacks to process the results of Commands

Callbacks can be used to extend the lifetime of a job to include processing a callback to process its results.
This is useful where you would like to reuse the the same command but handle the results differently,
e.g. the same command can email results or invoke a webhook by using a callback:

```csharp
jobs.EnqueueCommand<CheckUrlsCommand>(new CheckUrls { Urls = allUrls },
    new() {
        Callback = nameof(EmailUrlResultsCommand),
    });

jobs.EnqueueCommand<CheckUrlsCommand>(new CheckUrls { Urls = criticalUrls },
    new() {
        Callback = nameof(WebhookUrlResultsCommand),
        ReplyTo = callbackUrl
    });
```

Callbacks that fail are auto-retried the same number of times as their jobs, which if they all fail then 
the entire job is also marked as failed. 

### Run Job dependent on successful completion of parent

Jobs can be queued to only run after the successful completion of another job, this is useful 
for when you need to kick off multiple jobs after a long running task has finished like
generating monthly reports after monthly data has been aggregated, e.g: 

```csharp
var jobRef = jobs.EnqueueCommand<AggregateMonthlyDataCommand>(new Aggregate {
    Month = DateTime.UtcNow
});

jobs.EnqueueCommand<GenerateSalesReportCommand>(new () {
   DependsOn = jobRef.Id,
});

jobs.EnqueueCommand<GenerateExpenseReportCommand>(new () {
   DependsOn = jobRef.Id,
});
```

Inside your command you can get a reference to your current job with `Request.GetBackgroundJob()`
which will have its `ParentId` populated with the parent job Id and `job.ParentJob` containing
a reference to the completed Parent Job where you can access its Request, Results and other job 
information:

```csharp
public class GenerateSalesReportCommand(ILogger<MyCommandNoArgs> log) 
    : SyncCommand
{
    protected override void Run()
    {
        var job = Request.GetBackgroundJob();
        var parentJob = job.ParentJob;
    }
}
```

### Atomic Batching Behavior

We can also use `DependsOn` to implement atomic batching behavior where from inside our
executing command we can queue new jobs that are dependent on the successful execution
of the current job, e.g:

```csharp
public class CheckUrlsCommand(IHttpClientFactory factory, IBackgroundJobs jobs)
    : AsyncCommand<CheckUrls>
{
    protected override async Task RunAsync(CheckUrls req, CancellationToken ct)
    {
        var job = Request.GetBackgroundJob();

        var batchId = Guid.NewGuid().ToString("N");
        using var client = factory.CreateClient();
        foreach (var url in req.Urls)
        {
            var msg = new HttpRequestMessage(HttpMethod.Get, url);
            var response = await client.SendAsync(msg, ct);
            response.EnsureSuccessStatusCode();
      
            jobs.EnqueueCommand<SendEmailCommand>(new SendEmail {
                To = "my@email.com",
                Subject = $"{new Uri(url).Host} status",
                BodyText = $"{url} is up",
            }, new() {
                DependsOn = job.Id,
                BatchId = batchId,
            });
        }
    }
}
```

Where any dependent jobs are only executed if the job was successfully completed. 
If instead an exception was thrown during execution, the job will be failed and
all its dependent jobs cancelled and removed from the queue.

### Executing jobs with an Authenticated User Context

If you have existing logic dependent on a Authenticated `ClaimsPrincipal` or ServiceStack
`IAuthSession` you can have your APIs and Commands also be executed with that user context
by specifying the `UserId` the job should be executed as:

```csharp
var openAiRequest = new CreateOpenAiChat {
   Request = new() {
       Model = "gpt-4",
       Messages = [
           new() {
               Content = request.Question
           }
       ]
   },
}; 

// Example executing API Job with User Context
jobs.EnqueueApi(openAiRequest, 
    new() {
      UserId = Request.GetClaimsPrincipal().GetUserId(),
      CreatedBy = Request.GetClaimsPrincipal().GetUserName(),
   });

// Example executing Command Job with User Context
jobs.EnqueueCommand<CreateOpenAiChatCommand>(openAiRequest, 
    new() {
      UserId = Request.GetClaimsPrincipal().GetUserId(),
      CreatedBy = Request.GetClaimsPrincipal().GetUserName(),
   });
```

Inside your API or Command you access the populated User `ClaimsPrincipal` or
ServiceStack `IAuthSession` using the same APIs that you'd use inside your ServiceStack APIs, e.g:

```csharp
public class CreateOpenAiChatCommand(IBackgroundJobs jobs) 
    : AsyncCommand<CreateOpenAiChat>
{
    protected override async Task RunAsync(
        CreateOpenAiChat request, CancellationToken token)
    {
        var user = Request.GetClaimsPrincipal();
        var session = Request.GetSession();
        //...
    }
}
```

### Queue Job to run after a specified date

Using `RunAfter` lets you queue jobs that are only executed after a specified `DateTime`, 
useful for executing resource intensive tasks at low traffic times, e.g: 

```csharp
var jobRef = jobs.EnqueueCommand<AggregateMonthlyDataCommand>(new Aggregate {
       Month = DateTime.UtcNow
   }, 
   new() {
       RunAfter = DateTime.UtcNow.Date.AddDays(1)
   });
```

### Attach Metadata to Jobs

All above Background Job Options have an effect on when and how Jobs are executed.
There are also a number of properties that can be attached to a Job that can be useful
in background job processing despite not having any effect on how jobs are executed.

These properties can be accessed by commands or APIs executing the Job and are visible
and can be filtered in the Jobs Admin UI to help find and analyze executed jobs.

```csharp
var jobRef = jobs.EnqueueCommand<CreateOpenAiChatCommand>(openAiRequest, 
   new() {
      // Group related jobs under a common tag
      Tag = "ai",
      // A User-specified or system generated unique Id to track the job
      RefId = request.RefId,
      // Capture who created the job
      CreatedBy = Request.GetClaimsPrincipal().GetUserName(),
      // Link jobs together that are sent together in a batch
      BatchId = batchId,
      // Capture where to notify the completion of the job to
      ReplyTo = "https:example.org/callback",
      // Additional properties about the job that aren't in the Request  
      Args = new() {
          ["Additional"] = "Metadata"
      }
   });
```

### Querying a Job

A job can be queried by either it's auto-incrementing `Id` Primary Key or by a unique `RefId`
that can be user-specified.

```csharp
var jobResult = jobs.GetJob(jobRef.Id);

var jobResult = jobs.GetJobByRefId(jobRef.RefId);
```

At a minimum a `JobResult` will contain the Summary Information about a Job as well as the 
full information about a job depending on where it's located:

```csharp
class JobResult
{
    // Summary Metadata about a Job in the JobSummary Table 
    JobSummary Summary
    // Job that's still in the BackgroundJob Queue
    BackgroundJob? Queued
    // Full Job information in Monthly DB CompletedJob Table
    CompletedJob? Completed
    // Full Job information in Monthly DB FailedJob Table
    FailedJob? Failed
    // Helper to access full Job Information
    BackgroundJobBase? Job => Queued ?? Completed ?? Failed 
}
```

### Job Execution Limits

Default Retry and Timeout Limits can be configured on the Backgrounds Job plugin:

```csharp
services.AddPlugin(new BackgroundsJobFeature
{
   DefaultRetryLimit = 2,
   DefaultTimeout = TimeSpan.FromMinutes(10),
});
```

These limits are also overridable on a per-job basis, e.g: 

```csharp
var jobRef = jobs.EnqueueCommand<AggregateMonthlyDataCommand>(new Aggregate {
       Month = DateTime.UtcNow
   }, 
   new() {
      RetryLimit = 3,
      Timeout = TimeSpan.FromMinutes(30),
   });
```

### Logging, Cancellation an Status Updates

We'll use the command for checking multiple URLs to demonstrate some recommended patterns
and how to enlist different job processing features.

```csharp
public class CheckUrlsCommand(
    ILogger<CheckUrlsCommand> logger,
    IBackgroundJobs jobs,
    IHttpClientFactory clientFactory) : AsyncCommand<CheckUrls>
{
    protected override async Task RunAsync(CheckUrls req, CancellationToken ct)
    {
        // 1. Create Logger that Logs and maintains logging in Jobs DB
        var log = Request.CreateJobLogger(jobs,logger);

        // 2. Get Current Executing Job
        var job = Request.GetBackgroundJob();

        var result = new CheckUrlsResult {
            Statuses = new()
        };
        using var client = clientFactory.CreateClient();
        for (var i = 0; i < req.Urls.Count; i++)
        {
            // 3. Stop processing Job if it's been cancelled 
            ct.ThrowIfCancellationRequested();

            var url = req.Urls[i];
            try
            {
                var msg = new HttpRequestMessage(HttpMethod.Get,url);
                var response = await client.SendAsync(msg, ct);

                result.Statuses[url] = response.IsSuccessStatusCode;
                log.LogInformation("{Url} is {Status}",
                    url, response.IsSuccessStatusCode ? "up" : "down");

                // 4. Optional: Maintain explicit progress and status updates
                log.UpdateStatus(i/(double)req.Urls.Count,$"Checked {i} URLs");
            }
            catch (Exception e)
            {
                log.LogError(e, "Error checking {Url}", url);
                result.Statuses[url] = false;
            }
        }

        // 5. Send Results to WebHook Callback if specified
        if (job.ReplyTo != null)
        {
            jobs.EnqueueCommand<NotifyCheckUrlsCommand>(result,
                new() {
                    ParentId = job.Id,
                    ReplyTo = job.ReplyTo,
                });
        }
    }
}
```

We'll cover some of the notable parts useful when executing Jobs:

#### 1. Job Logger

We can use a Job logger to enable database logging that can be monitored in real-time in the 
Admin Jobs UI. Creating it with both `BackgroundJobs` and `ILogger` will return a combined 
logger that both Logs to standard output and to the Jobs database:

```csharp
var log = Request.CreateJobLogger(jobs,logger);
```

Or just use `Request.CreateJobLogger(jobs)` to only save logs to the database.

#### 2. Resolve Executing Job

If needed the currently executing job can be accessed with:

```csharp
var job = Request.GetBackgroundJob();
```

Where you'll be able to access all the metadata the jobs were created with including `ReplyTo`
and `Args`.

#### 3. Check if Job has been cancelled

To be able to cancel a long running job you'll need to periodically check if a Cancellation
has been requested and throw a `TaskCanceledException` if it has to short-circuit the command
which can be done with:

```csharp
ct.ThrowIfCancellationRequested();
```

You'll typically want to call this at the start of any loops to prevent it from doing any more work.

#### 4. Optionally record progress and status updates

By default Background Jobs looks at the last API or Command run and worker used to estimate 
the duration and progress for how long a running job will take.

If preferred your command can explicitly set a more precise progress and optional status update
that should be used instead, e.g:

```csharp
log.UpdateStatus(progress:i/(double)req.Urls.Count, $"Checked {i} URLs");
```

Although generally the estimated duration and live logs provide a good indication for the progress
of a job.

#### 5. Notify completion of Job

Calling a Web Hook is a good way to notify externally initiated job requests of the completion
of a job. You could invoke the callback within the command itself but there are a few benefits
to initiating another job to handle the callback:

 - Frees up the named worker immediately to process the next task
 - Callbacks are durable, auto-retried and their success recorded like any job
 - If a callback fails the entire command doesn't need to be re-run again

We can queue a callback with the result by passing through the `ReplyTo` and link it to the
existing job with:

```csharp
if (job.ReplyTo != null)
{
   jobs.EnqueueCommand<NotifyCheckUrlsCommand>(result,
       new() {
           ParentId = job.Id,
           ReplyTo = job.ReplyTo,
       });
}
```

Which we can implement by calling the `SendJsonCallbackAsync` extension method with the
Callback URL and the Result DTO it should be called with:

```csharp
public class NotifyCheckUrlsCommand(IHttpClientFactory clientFactory) 
    : AsyncCommand<CheckUrlsResult>
{
    protected override async Task RunAsync(
        CheckUrlsResult request, CancellationToken token)
    {
        await clientFactory.SendJsonCallbackAsync(
            Request.GetBackgroundJob().ReplyTo, request, token);
    }
}
```

#### Callback URLs

`ReplyTo` can be any URL which by default will have the result POST'ed back to the URL with a JSON
Content-Type. Typically URLs will contain a reference Id so external clients can correlate a callback
with the internal process that initiated the job. If the callback API is publicly available you'll
want to use an internal Id that can't be guessed so the callback can't be spoofed, like a Guid, e.g:

`$"https://api.example.com?refId={RefId}"`

If needed the callback URL can be customized on how the HTTP Request callback is sent.

If the URL contains a space, the text before the space is treated as the HTTP method:

`"PUT https://api.example.com"`

If the auth part contains a colon `:` it's treated as Basic Auth:

`"username:password@https://api.example.com"`

If name starts with `http.` sends a HTTP Header

`"http.X-API-Key:myApiKey@https://api.example.com"`

Otherwise it's sent as a Bearer Token:

`"myToken123@https://api.example.com"`

Bearer Token or HTTP Headers starting with `$` is substituted with Environment Variable if exists:

`"$API_TOKEN@https://api.example.com"`

When needed headers, passwords and tokens can be URL encoded if they contain any delimiter characters.

## Implementing Commands

At a minimum a command need only implement the simple [IAsyncCommand interface](https://docs.servicestack.net/commands#commands-feature): 

```csharp
public interface IAsyncCommand<in T>
{
    Task ExecuteAsync(T request);
}
```

Which is the singular interface that can execute any command.

However commands executed via Background Jobs have additional context your commands may need to 
access during execution, including the `BackgroundJob` itself, the `CancellationToken` and
an Authenticated User Context.

To reduce the effort in creating commands with a `IRequest` context we've added a number ergonomic 
base classes to better capture the different call-styles a unit of logic can have including 
**Sync** or **Async** execution, whether they require **Input Arguments** or have **Result Outputs**.

Choosing the appropriate Abstract base class benefits from IDE tooling in generating the method
signature that needs to be implemented whilst Async commands with Cancellation Tokens in its method
signature highlights any missing async methods that are called without the token. 

### Sync Commands

 - `SyncCommand` - Requires No Arguments
 - `SyncCommand<TRequest>` - Requires TRequest Argument
 - `SyncCommandWithResult<TResult>` - Requires No Args and returns Result
 - `SyncCommandWithResult<TReq,TResult>` - Requires Arg and returns Result

```csharp
public record MyArgs(int Id);
public record MyResult(string Message);

public class MyCommandNoArgs(ILogger<MyCommandNoArgs> log) : SyncCommand
{
    protected override void Run()
    {
        log.LogInformation("Called with No Args");
    }
}

public class MyCommandArgs(ILogger<MyCommandNoArgs> log) : SyncCommand<MyArgs>
{
    protected override void Run(MyArgs request)
    {
        log.LogInformation("Called with {Id}", request.Id);
    }
}

public class MyCommandWithResult(ILogger<MyCommandNoArgs> log) 
    : SyncCommandWithResult<MyResult>
{
    protected override MyResult Run()
    {
        log.LogInformation("Called with No Args and returns Result");
        return new MyResult("Hello World");
    }
}

public class MyCommandWithArgsAndResult(ILogger<MyCommandNoArgs> log) 
    : SyncCommandWithResult<MyArgs,MyResult>
{
    protected override MyResult Run(MyArgs request)
    {
        log.LogInformation("Called with {Id} and returns Result", request.Id);
        return new MyResult("Hello World");
    }
}
```

### Async Commands

- `AsyncCommand` - Requires No Arguments
- `AsyncCommand<TRequest>` - Requires TRequest Argument
- `AsyncCommandWithResult<TResult>` - Requires No Args and returns Result
- `AsyncCommandWithResult<TReq,TResult>` - Requires Arg and returns Result

```csharp
public class MyAsyncCommandNoArgs(ILogger<MyCommandNoArgs> log) : AsyncCommand
{
    protected override async Task RunAsync(CancellationToken token)
    {
        log.LogInformation("Async called with No Args");
    }
}

public class MyAsyncCommandArgs(ILogger<MyCommandNoArgs> log) 
    : AsyncCommand<MyArgs>
{
    protected override async Task RunAsync(MyArgs request, CancellationToken t)
    {
        log.LogInformation("Async called with {Id}", request.Id);
    }
}

public class MyAsyncCommandWithResult(ILogger<MyCommandNoArgs> log) 
    : AsyncCommandWithResult<MyResult>
{
    protected override async Task<MyResult> RunAsync(CancellationToken token)
    {
        log.LogInformation("Async called with No Args and returns Result");
        return new MyResult("Hello World");
    }
}

public class MyAsyncCommandWithArgsAndResult(ILogger<MyCommandNoArgs> log) 
    : AsyncCommandWithResult<MyArgs,MyResult>
{
    protected override async Task<MyResult> RunAsync(
        MyArgs request, CancellationToken token)
    {
        log.LogInformation("Called with {Id} and returns Result", request.Id);
        return new MyResult("Hello World");
    }
}
```


# Utilize C# Commands to build more robust and observable systems
Source: https://servicestack.net/posts/commands-feature

Much of ServiceStack has been focused on providing a productive [API First Development](https://docs.servicestack.net/api-first-development) 
experience and adding value-added features around your System's external APIs, including:

 - [Native end-to-end typed API integrations](https://docs.servicestack.net/add-servicestack-reference) to **11 popular languages**
 - Built-in [API Explorer](https://docs.servicestack.net/api-explorer) to discover, browse and invoke APIs
 - Instant CRUD UIs with [Auto Query](https://docs.servicestack.net/autoquery/) and [Locode](https://docs.servicestack.net/locode/)
 - Custom CRUD UIs with [Blazor components](https://blazor-gallery.jamstacks.net) and [Vue Components](https://docs.servicestack.net/vue/)
 
As well as [Declarative Validation](https://docs.servicestack.net/declarative-validation), multiple [Auth Integrations](https://docs.servicestack.net/auth/)
and other extensive [Declarative Features](https://docs.servicestack.net/locode/declarative) to enhance your external facing APIs.

### Internal API Implementation

Little attention has been given to internal implementations of APIs since it can use anything that fulfils its 
service contract by returning the APIs populated Response DTO. 

How code-bases are structured is largely a matter of developer preference, however we believe we've also been able 
to add value in this area by introducing an appealing option with our new managed **Commands** Feature.

:::youtube SXPdBHbncPc
Use C# Commands to build robust and observable systems with Admin UI
:::

## Code Architecture 

Ultimately nothing beats the simplicity of "No Architecture" by maintaining all logic within a Service Implementation 
which just needs to call a few App dependencies to implement its functionality and return a populated Response DTO:

```csharp
public object Any(MyRequest request) => new MyResponse { ... };
```

This is still the best option for small implementations where the Service is the only consumer of the logic that should
be run on the HTTP Worker Request Thread.

#### When to restructure

The times when you may want to consider moving logic out of your Service into separate classes include:

- **Code Reuse**: Make it easier to reuse your Service logic in other Services
- **Complexity**: Break down complex logic into smaller more manageable pieces
- **Testability**: Make it easier to test your Logic in isolation
- **Observability**: Make it easier to log and monitor
- **Robustness**: Make it easier to handle, retry and recover from errors
- **Flexibility**: Make it easier to run in parallel or in a different managed thread

We'll look at how the new **Commands Feature** can help with these concerns.

### Code Reuse

Following principles of YAGNI in doing the simplest thing that could possibly work, whenever we want to reuse logic
across Services we'd first start by moving it to an extension method on the dependency that it uses, e.g. 

```csharp
public static async Task<List<Contact>> GetActiveSubscribersAsync(
    this IDbConnection db, MailingList mailingList)
{        
    return await db.SelectAsync(db.From<Contact>(db.TableAlias("c"))
        .Where(x => x.DeletedDate == null && x.UnsubscribedDate == null && 
            x.VerifiedDate != null && (mailingList & x.MailingLists) == mailingList)
        .WhereNotExists(db.From<InvalidEmail>()
            .Where<Contact,InvalidEmail>((c,e) => 
                e.EmailLower == Sql.TableAlias(c.EmailLower, "c"))
            .Select(x => x.Id))
    );
}
```

Which does a great job at encapsulating logic and making it reusable and readable:

```csharp
foreach (var sub in await Db.GetActiveSubscribersAsync(MailingList.Newsletter)) {
    //...
}
```

Where it can be reused without referencing any external classes whilst also being easily discoverable via intellisense.    

This works great for 1 or 2 dependencies, but becomes more cumbersome as the number of dependencies grows, e.g: 

```csharp
public static async Task<List<Contact>> GetActiveSubscribersAsync(
    this IDbConnection db, ILogger log, ICacheClient cache, MailingList mailingList)
```

In which the complexity of the extension method dependencies leaks and impacts all calling classes that need to include
them and also starts to impact its readability, e.g:

```csharp
public class MyService(ILogger<MyService> log, ICacheClient cache, IDbConnection db) 
    : Service
{
    public object Any(MyRequest request)
    {
        var subs = await Db.GetActiveSubscribersAsync(log, cache, request.MailList);
    }
}
```

### Refactoring Logic into separate classes

The solution to this is to refactor the logic into a separate class and leverage the IOC to inject the dependencies it needs,
fortunately with Primary Constructors this now requires minimal boilerplate code, e.g:

```csharp
class MyLogic(ILogger<MyService> log, ICacheClient cache, IDbConnection db)
{
    //...
}
```

But it still requires manual registration adding additional complexity to 
your Host project `Program.cs` or [Modular Configurations](https://docs.servicestack.net/modular-startup) which needs to 
manage registration for all these new logic classes, e.g: 

```csharp
builder.Services.AddTransient<MyLogic>();
```

## Commands Feature

Which touches on the first benefit of the **Commands Feature** which like ServiceStack Services auto registers
all classes implementing the intentionally simple and impl-free `IAsyncCommand` interface, e.g:

```csharp
public interface IAsyncCommand<in T>
{
    Task ExecuteAsync(T request);
}
```

Allowing for maximum flexibility in how to implement your logic classes, which are essentially encapsulated
units of logic with a single method to execute it, e.g:

```csharp
public class AddTodoCommand(ILogger<AddTodoCommand> log, IDbConnection db) 
    : IAsyncCommand<CreateTodo>
{
    public async Task ExecuteAsync(CreateTodo request)
    {
        var newTodo = request.ConvertTo<Todo>();
        newTodo.Id = await db.InsertAsync(newTodo, selectIdentity:true);
        log.LogDebug("Created Todo {Id}: {Text}", newTodo.Id, newTodo.Text);
    }
}
```

Where we immediately get the benefits of code reuse, encapsulation, and readability without needing to manually 
register and pollute your App's configuration with them.

By default Commands are registered as transient dependencies, but you can also register them with a different lifetime
scope using the `[Lifetime]` attribute, e.g:

```csharp
[Lifetime(Lifetime.Scoped)]
public class AddTodoCommand(ILogger<AddTodoCommand> log, IDbConnection db)
    : IAsyncCommand<CreateTodo> {}
```

Or by manually registering them, if you need a custom registration:

```csharp
services.AddTransient<AddTodoCommand>(c => CreateAddTodoCommand(c));
```

### Commands with Results

For maximum flexibility, we want to encourage temporal decoupling by separating initiating a command from its execution, 
so instead of adding a different method to execute commands with results, we're instead recommending the convention of
storing the result of a command in a `Result` property, e.g:

```csharp
public interface IAsyncCommand<in TRequest, out TResult> 
    : IAsyncCommand<TRequest>, IHasResult<TResult> { }

public interface IHasResult<out T>
{
    T Result { get; }
}
```

So we could implement a command with a result like:

```csharp
public class AddTodoCommand(ILogger<AddTodoCommand> log, IDbConnection db) 
    : IAsyncCommand<CreateTodo, Todo>
{
    public Todo Result { get; private set; }
    
    public async Task ExecuteAsync(CreateTodo request)
    {
        Result = request.ConvertTo<Todo>();
        Result.Id = await db.InsertAsync(newTodo, selectIdentity:true);
        log.LogDebug("Created Todo {Id}: {Text}", Result.Id, Result.Text);
    }
}
```

### Messaging

Although for better resilience and scalability we recommend utilizing a messaging pattern to notify the outputs of a 
command by publishing messages to invoke dependent logic instead of returning a result, e.g:

```csharp
public class AddTodoCommand(IDbConnection db, IMessageProducer mq) 
    : IAsyncCommand<CreateTodo>
{
    public async Task ExecuteAsync(CreateTodo request)
    {
        var newTodo = request.ConvertTo<Todo>();
        newTodo.Id = await db.InsertAsync(newTodo, selectIdentity:true);
        mq.Publish(new SendNotification { TodoCreated = newTodo });
    }    
}
```

Which decouples the sender and receiver of the message, allowing it to finish without needing to wait and concern itself 
on how subsequent logic is processed, e.g. how to handle errors, whether to execute it in a different managed thread, in parallel, etc.

Messaging encourages adopting a more reliable asynchronous one-way workflow instead of implementing logic serially where 
the sender is timely coupled to the successful execution of all subsequent logic before being able to complete, e.g:

```csharp
await cmd.ExecuteAsync(createTodo);
var newTodo = cmd.Result;
await SendNewTodoNotificationAsync(newTodo);
```

It allows for more reliable and observable workflows that removes the temporal coupling between components where each 
execution step can be executed on different threads, independently monitored and retried if needed.

```txt
[A] -> [B] -> [C]
```

### Commands as Application Building Blocks

As they're not dependent on any framework and can support multiple execution patterns, we believe Commands make great 
building blocks for insulating units of logic as they're simple and testable and allow for managed execution which can 
easily add logging, monitoring, and resilience around your logic.

### Background MQ

It should be noted adopting a messaging pattern doesn't require additional infrastructure complexity of an external MQ Server
as you can use the [Background MQ](https://docs.servicestack.net/background-mq) to execute messages in configurable managed background threads.

### Executing Commands

Commands are effectively a pattern to structure your logic that doesn't depend on any implementation assembly or 
framework, so they can just be executed directly, e.g:

```csharp
using var db = dbFactory.Open();
var cmd = new AddTodoCommand(new NullLogger<AddTodoCommand>(), db);
await cmd.ExecuteAsync(new CreateTodo { Text = "New Todo" });
```

### Command Executor

They also allow for a managed execution which the **CommandsFeature** provides with its `ICommandExecutor` which
can be executed like:

```csharp
public class MyService(ICommandExecutor executor) : Service
{
    public object Any(MyRequest request)
    {
        var cmd = executor.Command<AddTodoCommand>(); 
        await cmd.ExecuteAsync(new AddTodoCommand { Text = "New Todo" });
    }
}
```

This still results in the same behavior where exceptions are bubbled but also adds observability and resilience and
other niceties like executing any Fluent or Declarative Validation on Command Requests.

### Retry Failed Commands

We can make commands more resilient by adding the `[Retry]` attribute to opt into auto retrying failed commands:

```csharp
[Retry]
public class AddTodoCommand() : IAsyncCommand<CreateTodo> {}
```

Which will automatically retry the command as per the default Retry Policy:

```csharp
services.AddPlugin(new CommandsFeature
{
    DefaultRetryPolicy = new(
        Times: 3,
        Behavior: RetryBehavior.FullJitterBackoff,
        DelayMs: 100,
        MaxDelayMs: 60_000,
        DelayFirst: false
    )
});
```

That can be overridden on a per-command basis with the `[Retry]` attribute, e.g:

```csharp
[Retry(Times=4, MaxDelayMs=300_000, Behavior=RetryBehavior.LinearBackoff)]
public class AddTodoCommand() : IAsyncCommand<CreateTodo> {}
```

The different Retry Behaviors available include:

```csharp
public enum RetryBehavior
{
    // Use the default retry behavior
    Default,
    
    // Always retry the operation after the same delay
    Standard,
    
    // Should be retried with a linear backoff delay strategy
    LinearBackoff,

    // Should be retried with an exponential backoff strategy
    ExponentialBackoff,

    // Should be retried with a full jittered exponential backoff strategy
    FullJitterBackoff,
}
```

## Command Admin UI 

Which can be inspected in the new **Command Admin UI** where you can view summary stats of all executed Commands and **APIs** 
in the **Summary** tab, e.g:

[![](/img/posts/commands-feature/AddTodoCommand-summary.png)](/img/posts/commands-feature/AddTodoCommand-summary.png)

### Latest Command Executions

It also maintains a rolling log of the latest executed commands in the **Latest** tab:

[![](/img/posts/commands-feature/AddTodoCommand-latest.png)](/img/posts/commands-feature/AddTodoCommand-latest.png)

### Failed Command Executions

Whilst the **Errors** tab shows a list of all failed **Command** and **API** executions:

[![](/img/posts/commands-feature/AddTodoCommand-errors.png)](/img/posts/commands-feature/AddTodoCommand-errors.png)

### Execute Internal Commands

A benefit of using Commands as the building block for your internal logic is that they enjoy many of the same benefits
of ServiceStack's message-based Services where they can be invoked using just the Command **Name** and a **Request** Body
which allows them to be discovered and executed from the **Explore** Tab:

[![](/img/posts/commands-feature/AddTodoCommand-execute.png)](/img/posts/commands-feature/AddTodoCommand-execute.png)

In this way they can be treated like **Internal APIs** for being able to invoke internal functionality that's only accessible 
by **Admin** Users.

### Group Commands by Tag

Just like ServiceStack Services they can be grouped by **Tag** which can be used to group related commands: 

```csharp
[Tag("Todos")]
public class AddTodoCommand() : IAsyncCommand<CreateTodo> {}
```

## MQ Integration

Although `CommandsFeature` is a standalone feature we're registering it in the new Identity Auth Templates `Configure.Mq.cs`
which already uses the Background MQ to execute messages in managed background threads where it's used to send Identity Auth emails:

```csharp
public class ConfigureMq : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            services.AddSingleton<IMessageService>(c => new BackgroundMqService());
            services.AddPlugin(new CommandsFeature());
        })
        .ConfigureAppHost(afterAppHostInit: appHost => {
            var mqService = appHost.Resolve<IMessageService>();

            //Register ServiceStack APIs you want to be able to invoke via MQ
            mqService.RegisterHandler<SendEmail>(appHost.ExecuteMessage);
            mqService.Start();
        });
}
```

Despite being 2 independent features, they work well together as the Background MQ can be used to execute Commands in
managed background threads of which a single thread is used to execute each Request Type by default (configurable per request).

You'd typically want to use queues to improve scalability by reducing locking and concurrency contention of heavy resources
by having requests queued and executed in a managed background thread where it's able to execute requests as fast as it can without contention.
Queues are also a great solution for working around single thread limitations of resources like writes to SQLite databases.

## Use Case - SQLite Writes

As we've started to use server-side SQLite databases for our new Apps given its [many benefits](https://docs.servicestack.net/ormlite/litestream)
we needed a solution to workaround its limitation of not being able to handle multiple writes concurrently.

One of the benefits of using SQLite is creating and managing multiple databases is relatively cheap, so we can mitigate
this limitation somewhat by maintaining different subsystems in separate databases, e.g:

[![](/img/posts/commands-feature/pvq-databases.png)](/img/posts/commands-feature/pvq-databases.png)

But each database can only be written to by a single thread at a time, which we can now easily facilitate with
**Background MQ** and **MQ Command DTOs**.

### MQ Command DTOs

We can use the new `[Command]` attribute to be able to execute multiple commands on a single Request DTO Properties, e.g:

```csharp
[Tag(Tag.Tasks)]
[Restrict(RequestAttributes.MessageQueue), ExcludeMetadata]
public class DbWrites : IGet, IReturn<EmptyResponse>
{
    [Command<CreatePostVoteCommand>]
    public Vote? CreatePostVote { get; set; }
    
    [Command<CreateCommentVoteCommand>]
    public Vote? CreateCommentVote { get; set; }
    
    [Command<CreatePostCommand>]
    public Post? CreatePost { get; set; }
    
    [Command<UpdatePostCommand>]
    public Post? UpdatePost { get; set; }
    
    [Command<DeletePostsCommand>]
    public DeletePosts? DeletePosts { get; set; }
    
    [Command<DeleteAnswersCommand>]
    public DeleteAnswers? DeleteAnswers { get; set; }
    
    [Command<CreateAnswerCommand>]
    public Post? CreateAnswer { get; set; }
    
    [Command<PostSubscriptionsCommand>]
    public PostSubscriptions? PostSubscriptions { get; set; }
    
    [Command<TagSubscriptionsCommand>]
    public TagSubscriptions? TagSubscriptions { get; set; }    
    //...
}
```

Then to execute the commands we can use the `Request.ExecuteCommandsAsync` extension method for its 
Background MQ API implementation:

```csharp
public class BackgroundMqServices : Service
{
    public Task Any(DbWrites request) => Request.ExecuteCommandsAsync(request);
}
```

Which goes through all Request DTO properties to execute all populated properties with their associated
command, using it as the request for the command.

So after registering the `DbWrites` Command DTO with the MQ Service:

```csharp
mqService.RegisterHandler<DbWrites>(appHost.ExecuteMessage);
```

We can now publish a single `DbWrites` message to execute multiple commands in a single managed background thread, e.g:

```csharp
public class NotificationServices(MessageProducer mq) : Service
{
    public object Any(Watch request)
    {
        var userName = Request.GetClaimsPrincipal().GetUserName();

        mq.Publish(new DbWrites
        {
            PostSubscriptions = request.PostId == null ? null : new()
            {
                UserName = userName,
                Subscriptions = [request.PostId.Value],
            },
            TagSubscriptions = request.Tag == null ? null : new()
            {
                UserName = userName,
                Subscriptions = [request.Tag],
            },
        });
        
        mq.Publish(new AnalyticsTasks {
            WatchRequest = request,
        });
    }
}
```

We also benefit from its natural parallelism where write requests to different Databases are executed in parallel.


# Simple Auth Story for .NET 8 C# Microservices
Source: https://servicestack.net/posts/simple-auth-microservices

With ServiceStack now [fully integrated with ASP.NET Core Identity Auth](https://docs.servicestack.net/auth/identity-auth)
our latest [.NET 8 Tailwind Templates](/start) now include a full-featured Auth Configuration complete with User Registration, 
Login, Password Recovery, Two Factory Auth, and more.

Whilst this is great for C# Web Applications which need it, it neglects the class of Apps which don't need User Auth and
the additional complexity it brings with Identity and Password Management, EF Migrations, Token Expirations, etc. 

For these stand-alone Apps, Microservices and Docker Appliances that would still like to restrict Access to their APIs
but don't need the complexity of ASP .NET Core's Authentication machinery, a simpler Auth Story is ideal.

With the [introduction of API Keys](/posts/apikeys) we're able to provide a simpler Auth Story for stand-alone 
.NET 8 Microservices that's easy for **Admin** Users to manage and control which trusted clients and B2B Integrations
can access their functionality.

:::youtube 0ceU91ZBhTQ
Simple Auth Story with API Keys ideal for .NET 8 Microservices
:::

The easiest way to get started is by creating a new Empty project with API Keys enabled with your preferred database
to store the API Keys in. SQLite is a good choice for stand-alone Apps as it doesn't require any infrastructure dependencies.

<div class="not-prose mx-auto">
  <h3 id="template" class="mb-4 text-4xl tracking-tight font-extrabold text-gray-900">
      Create a new Empty project with API Keys
  </h3>
  <auth-templates></auth-templates>
</div>

### Existing Projects

Existing projects not configured with Authentication can enable this simple Auth configuration by running:

:::sh
x mix apikeys-auth
:::

Which will add the [ServiceStack.Server](https://nuget.org/packages/ServiceStack.Server) dependency and the 
[Modular Startup](https://docs.servicestack.net/modular-startup) configuration below:

```csharp
public class ConfigureApiKeys : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
    .ConfigureServices(services =>
    {
        services.AddPlugin(new AuthFeature(new AuthSecretAuthProvider("p@55wOrd")));
        services.AddPlugin(new ApiKeysFeature
        {
            // Optional: Available Scopes Admin Users can assign to any API Key
            // Features = [
            //     "Paid",
            //     "Tracking",
            // ],
            // Optional: Available Features Admin Users can assign to any API Key
            // Scopes = [
            //     "todo:read",
            //     "todo:write",
            // ],
        });
    })
    .ConfigureAppHost(appHost =>
    {
        using var db = appHost.Resolve<IDbConnectionFactory>().Open();
        var feature = appHost.GetPlugin<ApiKeysFeature>();
        feature.InitSchema(db);
    });
}
```

Which configures the **AuthSecretAuthProvider** with the **Admin** password and the **ApiKeysFeature** to enable
support for [API Keys](https://docs.servicestack.net/auth/apikeys).

### Admin UI

The **Admin** password will give you access to the [Admin UI](https://docs.servicestack.net/admin-ui) at:

:::{.text-4xl .text-center .text-indigo-800}
/admin-ui
:::

![](/img/posts/simple-auth-microservices/admin-ui-signin.png)

![](/img/posts/simple-auth-microservices/admin-ui-dashboard.png)

### API Keys Admin UI

Clicking on **API Keys** menu item will take you to the API Keys Admin UI where you'll be able to create new API Keys 
that you can distribute to different API consumers you want to be able to access your APIs:

![](/img/posts/simple-auth-microservices/admin-ui-apikeys.png)

The **ApiKeysFeature** plugin will let you control different parts of the UI, including what **Features** you want to
assign to API Keys and what **Scopes** you want individual API Keys to be able to have access to.

```csharp
services.AddPlugin(new ApiKeysFeature
{
    Features = [
        "Paid",
        "Tracking",
    ],
    Scopes = [
        "todo:read",
        "todo:write",
    ],
    // ExpiresIn =[
    //     new("", "Never"),
    //     new("30", "30 days"),
    //     new("365", "365 days"),
    // ],    
    // Hide = ["RestrictTo","Notes"],
});
```

Any configuration on the plugin will be reflected in the UI:

![](/img/posts/simple-auth-microservices/admin-ui-apikeys-new.png)

The API Keys Admin UI also lets you view and manage all API Keys in your App, including the ability to revoke API Keys, 
extend their Expiration date as well as manage any Scopes and Features assigned to API Keys.

![](/img/posts/simple-auth-microservices/admin-ui-apikeys-edit.png)

### Protect APIs with API Keys

You'll now be able to protect APIs by annotating Request DTOs with the `[ValidateApiKey]` attribute:

```csharp
[ValidateApiKey]
public class Hello : IGet, IReturn<HelloResponse>
{
    public required string Name { get; set; }
}
```

Which only allows requests with a **valid API Key** to access the Service.

### Scopes

We can further restrict API access by assigning them a scope which will only allow access to Valid API Keys configured 
with that scope, e.g:

```csharp
[ValidateApiKey("todo:read")]
public class QueryTodos : QueryDb<Todo>
{
    public long? Id { get; set; }
    public List<long>? Ids { get; set; }
    public string? TextContains { get; set; }
}

[ValidateApiKey("todo:write")]
public class CreateTodo : ICreateDb<Todo>, IReturn<Todo>
{
    [ValidateNotEmpty]
    public required string Text { get; set; }
    public bool IsFinished { get; set; }
}

[ValidateApiKey("todo:write")]
public class UpdateTodo : IUpdateDb<Todo>, IReturn<Todo>
{
    public long Id { get; set; }
    [ValidateNotEmpty]
    public required string Text { get; set; }
    public bool IsFinished { get; set; }
}

[ValidateApiKey("todo:write")]
public class DeleteTodos : IDeleteDb<Todo>, IReturnVoid
{
    public long? Id { get; set; }
    public List<long>? Ids { get; set; }
}
```

### Restrict To APIs

Scopes allow for coarse-grained access control allowing a single scope to access a logical group of APIs. For more 
fine-grained control you can use **Restrict To APIs** to specify just the APIs an API Key can access:

![](/img/posts/simple-auth-microservices/admin-ui-apikeys-restrict-to.png)

Unlike scopes which can access APIs with the **same scope** or **without a scope**, Valid API Keys configured with
**Restrict To APIs** can only access those specific APIs.

### Features

Features are user-defined strings accessible within your Service implementation to provide different behavior
based on Features assigned to the API Key, e.g:

```csharp
public object Any(QueryTodos request)
{
    if (Request.GetApiKey().HasFeature("Paid"))
    {
        //...
    }
}
```

### API Explorer

Support for API Keys is also integrated into the [API Explorer](https://docs.servicestack.net/api-explorer) allowing
users to use their API Keys to access API Key protected Services which are highlighted with a **Key** Icon:

![](/img/posts/simple-auth-microservices/apiexplorer-requires-apikey.png)

Users can enter their API Key by clicking on the **Key** Icon in the top right, or the link in the Warning alert
when trying to access an API Key protected Service:

![](/img/posts/simple-auth-microservices/apiexplorer-apikey-dialog.png)

### Client Usage

All HTTP and existing [Service Clients](https://docs.servicestack.net/clients-overview) can be configured to use API Keys
for machine-to-machine communication, which like most API Key implementations can be passed in a [HTTP Authorization Bearer Token](https://datatracker.ietf.org/doc/html/rfc6750#section-2.1)
that can be configured in Service Clients with:

#### C#

```csharp
var client = new JsonApiClient(BaseUrl) {
    BearerToken = apiKey
};
```

#### TypeScript

```ts
const client = new JsonServiceClient(BaseUrl)
client.bearerToken = apiKey
```

### API Key HTTP Header

Alternatively, API Keys can also be passed in the `X-Api-Key` HTTP Header which allows clients to be configured
with an alternative Bearer Token allowing the same client to call both **Authenticated** and **API Key** protected APIs, e.g:

#### C#

```csharp
var client = new JsonApiClient(BaseUrl) {
    BearerToken = jwt,
    Headers = {
        [HttpHeaders.XApiKey] = apiKey
    }
};
```

#### TypeScript

```ts
const client = new JsonServiceClient(BaseUrl)
client.bearerToken = apiKey
client.headers.set('X-Api-Key', apiKey)
```

## Conclusion

We hope this shows how stand-alone .NET 8 Microservices and self-contained Docker Apps can use the 
simple **Admin** and **API Keys** configuration to easily secure their APIs with API Keys, complete with **Management UI** 
and **typed Service Client** integrations.


# Using API Keys to secure .NET 8 C# APIs
Source: https://servicestack.net/posts/apikeys

As we continue to embrace and natively integrate with ASP.NET Core's .NET 8 platform, we've reimplemented the last major
feature missing from ServiceStack Auth - support for API Keys that's now available from **ServiceStack v8.3**.

### What are API Keys?

API Keys are a simple and effective way to authenticate and authorize access to your C# APIs, which are typically used 
for machine-to-machine communication, where a client application needs to access an API without user intervention. 
API Keys are often used to control access to specific resources or features in your API, providing a simple way 
to manage access control.

### Redesigning API Keys

Building on our experience with API Keys in previous versions of ServiceStack, we've taken the opportunity to redesign
how API Keys work to provide a more flexible and powerful way to manage access control for your APIs.

The existing [API Key Auth Provider](https://docs.servicestack.net/auth/api-key-authprovider) was implemented as 
another Auth Provider that provided another way to authenticate a single user. The consequences of this was:

 - Initial API Request was slow as it required going through the Authentication workflow to authenticate the user and setup authentication for that request
 - No support for fine-grained access control as API Keys had same access as the authenticated user
 - API Keys had to be associated with a User which was unnecessary for machine-to-machine communication

Given the primary use-case for API Keys is for machine-to-machine communication where the client is not a User,
nor do they want systems they give out their API Keys to, to have access to their User Account, we've changed
how API Keys work in .NET 8.

## .NET 8 API Keys Feature

:::youtube U4vqOIHOs_Q
New .NET 8 API Keys Feature with Built-In UIs!
:::

The first design decision to overcome the above issues was to separate API Keys from Users and Authentication itself,
where the new `ApiKeysFeature` is now just a plugin instead of an Auth Provider, which can be added to existing Identity
Auth Apps with:

:::sh
x mix apikeys
:::

Which will add the API Keys [Modular Startup](https://docs.servicestack.net/modular-startup) to your Host project, a minimal example of which looks like:

```csharp
public class ConfigureApiKeys : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services => {
            services.AddPlugin(new ApiKeysFeature());
        })
        .ConfigureAppHost(appHost => {
            using var db = appHost.Resolve<IDbConnectionFactory>().Open();
            var feature = appHost.GetPlugin<ApiKeysFeature>();
            feature.InitSchema(db);
        });
}
```

Where it registers the `ApiKeysFeature` plugin and creates the `ApiKey` table in the App's configured database if it
doesn't already exist.

### Creating Seed API Keys

The plugin can also be used to programmatically generate API Keys for specified Users:

```csharp
if (feature.ApiKeyCount(db) == 0)
{
    var createApiKeysFor = new [] { "admin@email.com", "manager@email.com" };
    var users = IdentityUsers.GetByUserNames(db, createApiKeysFor);
    foreach (var user in users)
    {
        // Create a super API Key for the admin user
        List<string> scopes = user.UserName == "admin@email.com"
            ? [RoleNames.Admin] 
            : [];
        var apiKey = feature.Insert(db, new() { 
            Name="Seed Key", UserId=user.Id, UserName=user.UserName, Scopes=scopes });
        
        var generatedApiKey = apiKey.Key;
    }
}
```

### Basic Usage

With the plugin registered, you can now use the `ValidateApiKey` attribute to limit APIs to only be accessible with a 
valid API Key, e.g:

```csharp
[ValidateApiKey]
public class MyRequest {}
```

### Use API Keys with our without Users and Authentication

API Keys can optionally be associated with a User, but they don't have to be, nor do they run in the context of a User
or are able to invoke any Authenticated APIs on their own. Users who create them can also limit their scope to only
call APIs they have access to, which can be done with user-defined scopes:

### Scopes

Scopes are user-defined strings that can be used to limit APIs from only being accessible with API Keys that have the 
required scope. For example, we could create generate API Keys that have **read only**, **write only** or **read/write** 
access to APIs by assigning them different scopes, e.g: 

```csharp
public static class Scopes
{
    public const string TodoRead = "todo:read";
    public const string TodoWrite = "todo:write";
}

[ValidateApiKey(Scopes.TodoRead)]
public class QueryTodos : QueryDb<Todo> {}

[ValidateApiKey(Scopes.TodoWrite)]
public class CreateTodo : ICreateDb<Todo>, IReturn<Todo> {}

[ValidateApiKey(Scopes.TodoWrite)]
public class UpdateTodo : IUpdateDb<Todo>, IReturn<Todo> {}

[ValidateApiKey(Scopes.TodoWrite)]
public class DeleteTodos : IDeleteDb<Todo>, IReturnVoid {}
```

Where only API Keys with the `todo:read` scope can access the `QueryTodos` API, and only API Keys with the `todo:write`
scope can access the `CreateTodo`, `UpdateTodo` and `DeleteTodos` APIs.

APIs that aren't assigned a scope can be accessed by any valid API Key.

The only built-in Scope is `Admin` which like the `Admin` role enables full access to all `[ValidateApiKeys]` APIs.

### Fine-grained Access Control

Alternatively API Keys can be restricted to only be able to access specific APIs.

### Features

In addition to scopes, API Keys can also be tagged with user-defined **Features** which APIs can inspect to enable 
different behavior, e.g. a **Paid** feature could be used to increase rate limits or return premium content whilst a 
**Tracking** feature could be used to keep a record of API requests, etc.

These can be accessed in your Services with:

```csharp
public object Any(QueryTodos request)
{
    if (Request.GetApiKey().HasFeature(Features.Paid))
    {
        // return premium content
    }
}
```

## Integrated UIs

Like many of ServiceStack's other premium features, API Keys are fully integrated into [ServiceStack's built-in UIs](https://servicestack.net/auto-ui)
including [API Explorer](https://docs.servicestack.net/api-explorer) and the [Admin UI](https://docs.servicestack.net/admin-ui).

### API Explorer

Your Users and API Consumers can use API Explorer to invoke protected APIs with their API Key. API Key protected APIs
will display a **key** icon next to the API instead of the **padlock** which is used to distinguish APIs that require
Authentication.

Users can configure API Explorer with their API Key by either clicking the **key** icon on the top right or by clicking
the **API Key** link on the alert message that appears when trying to access an API requiring an API Key:

![](/img/posts/apikeys/apiexplorer-apikeys.png)

Both of these will open the **API Key** dialog where they can paste their API Key:

![](/img/posts/apikeys/apiexplorer-apikeys-dialog.png)

:::info NOTE
API Keys are not stored in localStorage and only available in the current session
:::

### Admin UI

Whilst **Admin** users can view and manage API Keys in the API Key [Admin UI](https://docs.servicestack.net/admin-ui) at:

:::{.text-4xl .text-center .text-indigo-800}
/admin-ui/apikeys
:::

![](/img/posts/apikeys/admin-ui-apikeys.png)

This will let you view and manage all API Keys in your App, including the ability to revoke API Keys, extend their 
Expiration date as well as manage any Scopes and Features assigned to API Keys.

### Customizing API Key UIs

The `ApiKeysFeature` plugin can be configured to specify which **Scopes** and **Features** can be assigned to API Keys
as well as the different Expiration Options you want available in the API Key management UIs, e.g:

```csharp
services.AddPlugin(new ApiKeysFeature {
    // Optional: Available Scopes Admin Users can assign to any API Key
    Features = [
        Features.Paid,
        Features.Tracking,
    ],
    // Optional: Available Features Admin Users can assign to any API Key
    Scopes = [
        Scopes.TodoRead,
        Scopes.TodoWrite,
    ],
    // Optional: Limit available Expiry options that can be assigned to API Keys
    // ExpiresIn = [
    //     new("", "Never"),
    //     new("7", "7 days"),
    //     new("30", "30 days"),
    //     new("365", "365 days"),
    // ],
});
```

### Admin User API Keys

When the `ApiKeysFeature` plugin is registered, the [User Admin UI](https://docs.servicestack.net/admin-ui-identity-users) 
will be enhanced to include the ability to create and manage API Keys for the user at the bottom of the **Edit User** form:   

![](/img/posts/apikeys/admin-ui-user-apikeys.png)

#### Creating User API Keys

When creating API Keys, you can assign them a **Name**, its **Expiration** date and any **Scopes**, **Features** and **Notes**.

![](/img/posts/apikeys/admin-ui-user-apikeys-create.png)

### Restrict to APIs

`Scopes` provide a simple way to logically group a collection of related APIs behind UX-friendly names without Users 
needing to know the behavior of each individual API. 

In addition, Users who want fine-grained control can also restrict API Keys to only be able to access specific APIs that 
their systems make use of by selecting them from the **Restrict to APIs** option:

![](/img/posts/apikeys/apikeys-restrict-to.png)

#### One Time only access of generated API Key

All UIs limit access to the generated API Key token so that it's only accessible at the time of creation:

![](/img/posts/apikeys/admin-ui-user-apikeys-create-dialog.png)

#### Editing User API Keys

Everything about the API Key can be edited after it's created except for the generated API Key token itself, in addition
to be able to cancel and revoke the API Key:

![](/img/posts/apikeys/admin-ui-user-apikeys-edit.png)

Invalid API Keys that have expired or have been disabled will appear disabled in the UI:

![](/img/posts/apikeys/admin-ui-user-apikeys-disabled.png)

## User Management API Keys

In addition to the built-in Admin UIs to manage API Keys, all Identity Auth Tailwind templates have also been updated
to include support for managing API Keys in their User Account pages:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://blazor.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://blazor-vue.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor-vue.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-vue.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://blazor-wasm.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor-wasm.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-wasm.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://razor.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/razor.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">razor.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://mvc.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/mvc.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">mvc.web-templates.io</div>
    </a>
</div>

The templates aren't configured to use API Keys by default, but new projects can be configured to use API Keys by 
selecting the **API Keys** feature on the [Start Page](/start):

[![](/img/posts/apikeys/start-apikeys.png)](/start)

Or by mixing the `apikeys` project in your host project:

:::sh
x mix apikeys
:::

Which add the `Configure.ApiKeys.cs` modular startup to your Host project, which registers the `ApiKeysFeature` plugin
where you'd use the `UserScopes` and `UserFeatures` collections instead to control which scopes and features Users
can assign to their own API Keys, e.g:

```csharp
services.AddPlugin(new ApiKeysFeature {
    // Optional: Available Scopes Admin Users can assign to any API Key
    Features = [
        Features.Paid,
        Features.Tracking,
    ],
    // Optional: Available Features Admin Users can assign to any API Key
    Scopes = [
        Scopes.TodoRead,
        Scopes.TodoWrite,
    ],
    
    // Optional: Limit available Scopes Users can assign to their own API Keys
    UserScopes = [
        Scopes.TodoRead,
    ],
    // Optional: Limit available Features Users can assign to their own API Keys
    UserFeatures = [
        Features.Tracking,
    ],
});
```

### Identity Auth API Keys

When enabled users will be able to create and manage their own API Keys from their Identity UI pages
which will use any configured `UserScopes` and `UserFeatures`:

![](/img/posts/apikeys/identity-auth-apikeys.png)

### Client Usage

Like most API Key implementations, API Keys can be passed in a [HTTP Authorization Bearer Token](https://datatracker.ietf.org/doc/html/rfc6750#section-2.1)
that can be configured in ServiceStack Service Clients with: 

#### C#

```csharp
var client = new JsonApiClient(BaseUrl) {
    BearerToken = apiKey
};
```

#### TypeScript

```ts
const client = new JsonServiceClient(BaseUrl)
client.bearerToken = apiKey
```

### API Key HTTP Header

Alternatively, API Keys can also be passed in the `X-Api-Key` HTTP Header which allows clients to be configured
with an alternative Bearer Token allowing the same client to call both **Authenticated** and **API Key** protected APIs, e.g:

#### C#

```csharp
var client = new JsonApiClient(BaseUrl) {
    BearerToken = jwt,
    Headers = {
        [HttpHeaders.XApiKey] = apiKey
    }
};
```

#### TypeScript

```ts
const client = new JsonServiceClient(BaseUrl)
client.bearerToken = apiKey
client.headers.set('X-Api-Key', apiKey)
```

Or use a different HTTP Header by configuring `ApiKeysFeature.HttpHeader`, e.g:

```csharp
services.AddPlugin(new ApiKeysFeature {
    HttpHeader = "X-Alt-Key"
});
```


# Support for RHEL 9's hardened cryptography policy
Source: https://servicestack.net/posts/rhel9-cryptography

A consequence of RedHat Enterprise Linux 9's hardened 
[system-wide cryptographic policies](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/security_hardening/using-the-system-wide-cryptographic-policies_security-hardening) 
is that it's incompatible with ServiceStack's current licensing mechanism which uses RSA encryption and SHA1 hashing algorithm
to protect and validate license keys.

Unfortunately this makes it no longer possible to use License Keys to run unrestricted ServiceStack Apps on default 
installs of RHEL 9. The difficulty being we can't both support RHEL 9's hardened cryptography policy and 
maintain compatibility with being able to use newer License Keys on all previous versions of ServiceStack - vital
for enabling frictionless rotation of License Keys. 

As a system-wide policy we're unable to work around this restriction in the library to allow usage of RSA+SHA1 to just
validate License Keys which is the only place it's used. As it only affected a small number of users initially we recommend 
that they just switch to use
[RHEL's Legacy Cryptography Policy](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/security_hardening/using-the-system-wide-cryptographic-policies_security-hardening)
which allows for maximum compatibility with existing software.

### Road to Solution

![](/img/posts/rhel9-cryptography/bg-redhat.webp)

As more customers upgraded to RHEL 9 and started experiencing the same issue, we've decided to invest time to 
try and address this issue starting with adding support for a configurable Hashing algorithm when creating and validating
License Keys. We still have the issue of not being able to generate a new License Key that would be compatible with both
default RHEL 9 and all previous versions of ServiceStack.

The solutions under consideration were:
 - Generate a new License Key that's compatible with RHEL 9's hardened cryptography policy, but inform customers that 
they'll be unable to use the new License Key on their existing versions of ServiceStack and to continue to use their 
existing License Key for existing versions
 - Generate 2 License Keys, and explain to Customers which key to use for previous versions of ServiceStack and which key 
to use for RHEL 9 
 - Provide a way for customers to regenerate their License Key to support RHEL 9's hardened cryptography policy

Since this issue only affected a minority of our Customers we decided to go with the last option to avoid inflicting any
additional complexity on the majority of our Customers who are unaffected by this issue.

### Generate License Key for RHEL 9+

Starting from ServiceStack v8.3+ Customers can regenerate a new License Key with a stronger **SHA512** Hash Algorithm 
that's compatible with RHEL 9's default hardened cryptography policy by visiting:

:::{.text-3xl .text-indigo-600}
https://account.servicestack.net/regenerate-license
:::

### Future

We'll need to wait at least 1-2 years before we can make the stronger Hash Algorithm the default in order to reduce the
impact of not being able to use new License Keys on versions of ServiceStack prior to **v8.2**.

After the switch is made regenerating license keys will no longer be necessary.


# Using ASP.NET Core Output Caching
Source: https://servicestack.net/posts/redis-outputcache

With the release of ServiceStack 8.1, we've embraced tighter integration with ASP.NET Core, including support for registering ServiceStack services with ASP.NET Core's Endpoint Routing system. This opens up exciting opportunities to leverage more of ASP.NET Core's rich feature set in your ServiceStack applications.

One such feature is ASP.NET Core's built-in support for Output Caching (also known as Response Caching). Output Caching allows you to dramatically improve the performance of your APIs by caching the output and serving it directly from the cache for subsequent requests. This can significantly reduce the load on your server and database for frequently accessed, cacheable responses.

## Enabling Output Caching

To utilize Output Caching with your ServiceStack Endpoints, you first need to add the Output Caching middleware to your ASP.NET Core request pipeline in the `Configure` method of your `Program.cs`:

```csharp
// Program.cs
var builder = WebApplication.CreateBuilder(args);
var services = builder.Services;
app.UseOutputCache();
// ...
app.UseServiceStack(new AppHost(), options => options.MapEndpoints());
```

Then in `ConfigureServices` you need to add the Output Caching services:

```csharp
services.AddOutputCache();
```

The order of adding OutputCache to your request pipeline can be very sensitive to change, so this will depend largely on your application and dependencies you are already using. For example, below is an example of using it in a Blazor application.

```csharp
var app = builder.Build();

// Configure the HTTP request pipeline.
if (app.Environment.IsDevelopment())
{
    app.UseMigrationsEndPoint();
}
else
{
    app.UseExceptionHandler("/Error", createScopeForErrors: true);
    app.UseHsts();
}

app.UseHttpsRedirection();

app.UseStaticFiles();
app.UseAntiforgery();
// Add OutputCache after Antiforgery and before Auth related middleware
app.UseOutputCache();

// Required for OutputCache
app.UseAuthentication();
app.UseAuthorization();

app.MapRazorComponents<App>()
    .AddInteractiveServerRenderMode();

// Add additional endpoints required by the Identity /Account Razor components.
app.MapAdditionalIdentityEndpoints();

app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
});
```

## Configuring Caching Behavior

With the middleware in place, you can now configure caching behaviors for your ServiceStack Endpoints by registering against the Route Handlers within the ServiceStack `options`.

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
    options.RouteHandlerBuilders.Add((routeHandlerBuilder, operation, verb, route) =>
    {
        routeHandlerBuilder.CacheOutput(c =>
        {
            // Use Cache Profiles
            c.UseProfile("Default30");

            // Or configure caching per-request
            c.Expire(TimeSpan.FromSeconds(30));
            c.VaryByAll();
        });
    });
});
```

You can also vary the cache by specific properties, e.g:

```csharp
builder.CacheOutput(c => c.VaryBy("userRole","region"));
```

Or use Cache Profiles for reusable caching strategies:

```csharp
builder.Services.AddOutputCache(options =>
{
    options.AddPolicy("Default30", p => p.Expire(TimeSpan.FromSeconds(30)));
});
```

Then apply the named profile to your endpoints:

```csharp
builder.CacheOutput(c => c.UseProfile("Default30"));
```

## Finer-grained Control

For more granular control, you can apply the `[OutputCache]` attribute directly on your Service class, and use the ServiceStack AppHost metadata in your `RouteHandlerBuilder`s `Add` method to detect and cache only the routes that are attributed with `OutputCache`.

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
    options.RouteHandlerBuilders.Add((routeHandlerBuilder, operation, verb, route) =>
    {
        // Initialized appHost and allServiceTypes
        var appHost = HostContext.AppHost;
        var allServiceTypes = appHost.Metadata.ServiceTypes;

        // Find the service matching the RequestType of the operation
        var operationType = operation.RequestType;
        // Match with operation, verb and route
        appHost.Metadata.OperationsMap.TryGetValue(operationType, out var operationMap);
        var serviceType = operationMap?.ServiceType;
        if (serviceType == null)
            return;
        if (serviceType.HasAttributeOf<OutputCacheAttribute>())
        {
            // Handle duration from OutputCacheAttribute
            var outputCacheAttribute = serviceType.FirstAttribute<OutputCacheAttribute>();
            routeHandlerBuilder.CacheOutput(policyBuilder =>
            {
                policyBuilder.Cache().Expire(TimeSpan.FromSeconds(outputCacheAttribute.Duration));
            });
        }
    });
});
```



```csharp
[OutputCache(Duration = 60)]
public class MyServices : Service
{
    public object Any(Hello request)
    {
        return new HelloResponse { Result = $"Hello, {request.Name}!" };
    }
}
```

This enables for fine grained control of the built in `OutputCache` functionality compatible with using the same attribute with your MVC Controllers, and you can extend your use by updating the code above within the ServiceStack options.

## ServiceStack Redis Distributed Cache

The above examples so far have been using a cache store that comes with the OutputCache package. This is just an in memory store, so isn't suitable for a distributed application. Thankfully, you can override the IOutputCacheStore interface in your IoC to change out the implementation that uses a centralized system like a Redis server.

```csharp
public class RedisOutputCacheStore(IRedisClientsManager redisManager) : IOutputCacheStore
{
    public async ValueTask<byte[]?> GetAsync(string key, CancellationToken cancellationToken)
    {
        await using var redis =  await redisManager.GetClientAsync(token: cancellationToken);
        var value = await redis.GetAsync<byte[]>(key, cancellationToken);
        return value;
    }

    public async ValueTask SetAsync(string key, byte[] value, string[]? tags, TimeSpan validFor, CancellationToken cancellationToken)
    {
        await using var redis = await redisManager.GetClientAsync(token: cancellationToken);
        
        // First persist in normal cache hashset
        await redis.SetAsync(key, value, validFor, cancellationToken);

        if (tags == null)
            return;
        foreach (var tag in tags)
        {
            await redis.AddItemToSetAsync($"tag:{tag}", key, cancellationToken);
        }
    }

    public async ValueTask EvictByTagAsync(string tag, CancellationToken cancellationToken)
    {
        await using var redis = await redisManager.GetClientAsync(token: cancellationToken);
        
        var keys = await redis.GetAllItemsFromListAsync($"tag:{tag}", cancellationToken);
        
        foreach (var key in keys)
        {
            await redis.RemoveEntryAsync(key);
            await redis.RemoveItemFromSetAsync($"tag:{tag}", key, cancellationToken);
        }
    }
}
```

The above is a simple implementation of the IOutputCacheStore using the ServiceStack.Redis client to handle a centralized distributed cache. Using the class above, we can create a `Configure.OutputCache.cs` file that registers our IoC dependencies.

```csharp
[assembly: HostingStartup(typeof(BlazorOutputCaching.ConfigureOutputCache))]

namespace BlazorOutputCaching;

public class ConfigureOutputCache : IHostingStartup
{
    public void Configure(IWebHostBuilder builder)
    {
        builder.ConfigureServices(services =>
        {
            services.AddSingleton<IRedisClientsManager>(c =>
                new BasicRedisClientManager("localhost:6379"));
            services.AddSingleton<IOutputCacheStore, RedisOutputCacheStore>();
        });
    }
}
```

We register out Redis client manager for our RedisOutputCacheStore, and then the store itself.

## Summary

ASP.NET Core Output Caching is a powerful tool for improving the performance of your ServiceStack endpoints. With ServiceStack 8.1's tight integration with ASP.NET Core Endpoint Routing, utilizing this feature is now straightforward.

As always, caching is a balancing act. Apply it judiciously to frequently accessed, cacheable data. And be sure to implement appropriate invalidation strategies to keep your application's data fresh.

By leveraging Output Caching effectively, you can dramatically improve the scalability and responsiveness of your ServiceStack powered applications. Try it out in your ServiceStack 8.1+ projects and let us know how it goes!


# Using ASP.NET Core Rate Limiter Middleware
Source: https://servicestack.net/posts/asp-rate-limiter-middleware

Introduction Rate limiting is an important technique for protecting web APIs and applications from excessive traffic and abuse. By throttling the number of requests a client can make in a given time period, rate limiting helps ensure fair usage, maintains performance and availability, and defends against denial-of-service attacks.

ASP.NET Core provides built-in middleware for rate limiting based on client IP address or client ID. And now with the latest release, ServiceStack has added support for ASP.NET Core endpoints, making it possible to leverage the same rate limiting middleware across all ASP.NET Core endpoints, including ServiceStack APIs.

In this post, we'll look at how to enable rate limiting in an ASP.NET Core app using the standard middleware. Then we'll explore some more advanced options to fine-tune the rate limiting behavior. Finally, we'll see how to implement per-user rate limiting for multi-tenant SaaS applications using ASP.NET Core Identity and ServiceStack Mapped Endpoints.

Setting Up Rate Limiting To get started, let's enable the basic rate limiting middleware in an ASP.NET Core application:

1. Install the `Microsoft.AspNetCore.RateLimiting` NuGet package
2. In the `Program.cs`, add `AddRateLimiter` to register the rate limiting services:

```csharp
builder.Services.AddRateLimiter(options =>
{
    options.GlobalLimiter = PartitionedRateLimiter.Create<HttpContext, string>(httpContext =>
        RateLimitPartition.GetFixedWindowLimiter(
            partitionKey: httpContext.User.Identity?.Name ?? httpContext.Request.Headers.Host.ToString(), 
            factory: partition => new FixedWindowRateLimiterOptions
            {
                AutoReplenishment = true,
                PermitLimit = 100,
                QueueLimit = 0,
                Window = TimeSpan.FromMinutes(1)
            }));
    
    options.OnRejected = (context, cancellationToken) =>
    {
        if (context.Lease.TryGetMetadata(MetadataName.RetryAfter, out var retryAfter))
        {
            context.HttpContext.Response.Headers.RetryAfter = retryAfter.TotalSeconds.ToString();
        }

        context.HttpContext.Response.StatusCode = StatusCodes.Status429TooManyRequests;
        context.HttpContext.Response.WriteAsync("Too many requests. Please try again later.");

        return new ValueTask();
    };
});
```

This sets up a fixed window rate limiter with a limit of 100 requests per minute, partitioned by either authenticated username or client host name.

1. Add the `UseRateLimiter` middleware to the pipeline:

```csharp
app.UseRateLimiter();
```

With this basic setup, the API is now protected from excessive requests from individual clients. If a client exceeds the limit of 100 requests/minute, subsequent requests will receive a `HTTP 429 Too Many Requests` response.

Advanced Options The rate limiting middleware provides several options to customize the behavior:

- `PermitLimit` - The maximum number of requests allowed in the time window
- `QueueLimit` - The maximum number of requests that can be queued when the limit is exceeded. Set to 0 to disable queueing.
- `Window` - The time window for the limit, e.g. 1 minute, 1 hour, etc.
- `AutoReplenishment` - Whether the rate limit should reset automatically at the end of each window

For example, to allow short bursts but constrain average rate, we could implement a sliding window algorithm:

```csharp
options.GlobalLimiter = PartitionedRateLimiter.Create<HttpContext, string>(httpContext =>
    RateLimitPartition.GetSlidingWindowLimiter(
        partitionKey: httpContext.User.Identity?.Name ?? httpContext.Request.Headers.Host.ToString(),
        factory: partition => new SlidingWindowRateLimiterOptions
        {
            AutoReplenishment = true,
            PermitLimit = 100,
            QueueLimit = 25,
            Window = TimeSpan.FromMinutes(1),
            SegmentsPerWindow = 4
        }));

options.OnRejected = (context, cancellationToken) =>
{
    if (context.Lease.TryGetMetadata(MetadataName.RetryAfter, out var retryAfter))
    {
        context.HttpContext.Response.Headers.RetryAfter = retryAfter.TotalSeconds.ToString();
    }

    context.HttpContext.Response.StatusCode = StatusCodes.Status429TooManyRequests;
    context.HttpContext.Response.WriteAsync("Too many requests. Please try again later.");

    return new ValueTask();
};
```

This allows up to 100 requests per minute on average, with the ability to burst up to 25 additional requests which are queued.

## Per-User Rate Limiting for SaaS Applications

In a typical SaaS application, each user or tenant may have a different subscription plan that entitles them to a certain level of API usage. We can implement this per-user rate limiting by leveraging ASP.NET Core Identity to authenticate users and retrieve their plan details, and then configuring the rate limiter accordingly.

First, ensure you have ASP.NET Core Identity set up in your application to handle user authentication. Then, add a property to your user class to store the rate limit for each user based on their plan:

```csharp
public class ApplicationUser : IdentityUser
{
    public int RateLimit { get; set; }
}
```

Next, update the rate limiter configuration to partition by user and read the rate limit from the user's plan:

```csharp
builder.Services.AddRateLimiter(options =>
{
    options.AddPolicy("per-user", context =>
    {
        var user = context.User.Identity?.Name;
        if (string.IsNullOrEmpty(user))
        {
            // Fallback to host name for unauthenticated requests
            return RateLimitPartition.GetFixedWindowLimiter(
                partitionKey: context.Request.Headers.Host.ToString(),
                factory: partition => new FixedWindowRateLimiterOptions
                {
                    AutoReplenishment = true,
                    PermitLimit = 100,
                    QueueLimit = 0,
                    Window = TimeSpan.FromMinutes(1)
                });
        }
        // User exists
        // Get the user's rate limit from their plan
        var userId = context.User.FindFirstValue(ClaimTypes.NameIdentifier);
        var userManager = context.RequestServices.GetService<UserManager<ApplicationUser>>();
        var appUser = userManager.FindByIdAsync(userId).Result;
        var rateLimit = appUser?.RateLimit ?? 0;

        // Create a user-specific rate limiter
        return RateLimitPartition.GetFixedWindowLimiter(
            partitionKey: user,
            factory: partition => new FixedWindowRateLimiterOptions
            {
                AutoReplenishment = true,
                PermitLimit = rateLimit,
                QueueLimit = 0,
                Window = TimeSpan.FromMinutes(1)
            });
    });
});
```

This configuration first checks if the request is authenticated. If not, it falls back to the default host-based rate limiting.

For authenticated requests, it retrieves the user ID from the authentication claims and looks up the user in the ASP.NET Core Identity `UserManager`. It then reads the `RateLimit` property from the user object, which should be set based on the user's subscription plan.

Finally, it creates a user-specific rate limiter using the `PartitionedRateLimiter` with the user's ID as the partition key and their personal rate limit as the `PermitLimit`.

With this setup, each user will be rate limited independently based on their plan allowance. If a user exceeds their personal limit, they will receive a `429 Too Many Requests` response, while other users can continue making requests up to their own limits.

Not only that, our rate handling is consistent across ASP.NET Core Endpoints regardless of how they are implemented, be it ServiceStack APIs, MVC Controllers, Minimal APIs etc. If you do need to target ServiceStack APIs with a specific policy name, you can create one with a policy name and use it when calling `UseServiceStack`.

```csharp
services.AddRateLimiter(options =>
{
    // Policy name "per-user" is used by ServiceStack Mapped Endpoints
    options.AddPolicy("per-user", context =>
    {
        var user = context.User.Identity?.Name;
        if (string.IsNullOrEmpty(user))
        {
            // Fallback to host name for unauthenticated requests
            return RateLimitPartition.GetFixedWindowLimiter(
                partitionKey: context.Request.Headers.Host.ToString(),
                factory: partition => new FixedWindowRateLimiterOptions
                {
                    AutoReplenishment = true,
                    PermitLimit = 100,
                    QueueLimit = 0,
                    Window = TimeSpan.FromMinutes(1)
                });
        }
        // User exists
        // Get the user's rate limit from their plan
        var userId = context.User.FindFirstValue(ClaimTypes.NameIdentifier);
        var userManager = context.RequestServices.GetService<UserManager<ApplicationUser>>();
        var appUser = userManager.FindByIdAsync(userId).Result;
        var rateLimit = appUser?.RateLimit ?? 0;

        // Create a user-specific rate limiter
        return RateLimitPartition.GetFixedWindowLimiter(
            partitionKey: user,
            factory: partition => new FixedWindowRateLimiterOptions
            {
                AutoReplenishment = true,
                PermitLimit = rateLimit,
                QueueLimit = 0,
                Window = TimeSpan.FromMinutes(1)
            });
    });
    
    // ...
});

//...

// Make sure to call UseRateLimiter
app.UseRateLimiter();

// Specify which policy is used by ServiceStack Mapped Endpoints
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
    options.RouteHandlerBuilders.Add((routeBuilder, operation, method, route) =>
    {
        routeBuilder.RequireRateLimiting(policyName: "per-user");
    });
});
```

By combining ASP.NET Core rate limiting with ASP.NET Core Identity in this way, you can implement flexible, per-user rate limiting suitable for multi-tenant SaaS applications. The same approach can be extended to handle different rate limits for different API endpoints or user roles as needed. By upgrading ServiceStack to use ASP.NET Core Endpoints, you can now leverage the same rate limiting middleware across all ASP.NET Core Endpoints, including ServiceStack APIs.


# Kotlin Compose Multiplatform with end-to-end typed Kotlin & C# APIs
Source: https://servicestack.net/posts/kotlin-compose-multiplatform

The last few years of neglect of Xamarin has slid it into irrelevance, removing itself from consideration in the already 
shortlist of viable development options for creating native multi-platform iOS, Android and Desktop Apps, which leaves
us just Flutter and React Native as the only viable options. 

Thanks to the vast language ecosystem covered by [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference),
which ever technology you end up choosing to develop native Mobile and Desktop Apps with,
you'll always be able to develop with the productivity and type safety benefits of end-to-end typed APIs in your preferred language,
whether it's [TypeScript](https://docs.servicestack.net/typescript-add-servicestack-reference) or
[JavaScript](https://docs.servicestack.net/javascript-add-servicestack-reference) for React Native,
[Dart](https://docs.servicestack.net/dart-add-servicestack-reference) for Flutter,
[Java](https://docs.servicestack.net/java-add-servicestack-reference) or [Kotlin](https://docs.servicestack.net/kotlin-add-servicestack-reference) for Android,
or [Swift](https://docs.servicestack.net/swift-add-servicestack-reference) for iOS.

Fortunately JetBrains has stepped in to fill the void with Compose Multiplatform offering a 
modern alternative for creating native Mobile, Desktop & Web Apps which can also leverage
[Kotlin ServiceStack Reference](https://docs.servicestack.net/kotlin-add-servicestack-reference) for end-to-end typed APIs.

[Compose Multiplatform](https://www.jetbrains.com/lp/compose-multiplatform/) builds on
[Jetpack Compose](https://developer.android.com/jetpack/compose) - Google's modern toolkit for building 
native Android UIs bringing it to more platforms, including Windows, macOS and Linux Desktops, 
Web UIs with [Kotlin Wasm](https://kotlinlang.org/docs/wasm-overview.html)
and on iOS with [Kotlin/Native](https://kotlinlang.org/docs/native-overview.html).

We'll look at the latest [Compose Multiplatform v1.6 Release](https://blog.jetbrains.com/kotlin/2024/02/compose-multiplatform-1-6-0-release/)
and use it to build a cross-platform Desktop App integrated with a .NET API backend utilizing 
[Kotlin ServiceStack Reference](https://docs.servicestack.net/kotlin-add-servicestack-reference) to generate Kotlin DTOs
that can be used with the generic ServiceStack Java `JsonServiceClient` to enable its end-to-end typed API
integration.

### JVM Platform Required

Whilst Compose Multiplatform supports both JVM and non-JVM platforms, targeting a non JVM platform is very limited
as you won't be able to reference and use any Java packages like ServiceStack's Java Client library in `net.servicestack:client`
which is required for this example utilizing [Kotlin ServiceStack Reference](https://docs.servicestack.net/kotlin-add-servicestack-reference)
typed Kotlin DTOs.

## Compose Multiplatform iOS & Android Apps

:::youtube r6T3B7o1GYE
JetBrains Compose Multiplatform iOS & Android Apps
:::

The quickest way to a working Compose Multiplatform App integrated with a .NET API backend is to create a new project 
from the Compose Desktop template:

<div class="not-prose relative bg-white dark:bg-black py-4">
    <div class="mx-auto max-w-md px-4 text-center sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8">
        <p class="mt-2 text-3xl font-extrabold tracking-tight text-gray-900 dark:text-gray-50 sm:text-4xl">Create a new Compose Desktop App</p>
        <p class="mx-auto mt-5 max-w-prose text-xl text-gray-500"> 
            Create a new Kotlin Multiplatform App with your preferred project name:
        </p>
    </div>
    <compose-template repo="NetCoreTemplates/kmp-desktop" hide="demo"></compose-template>
</div>

Or install from the command-line with the [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::sh
x new kmp-desktop MyApp
:::

### Install JetBrains IDE

As a JetBrains technology, you're spoilt for choice for which IDE to use.

#### Android Studio

If you're primarily developing for Android, Android Studio is likely the the best option, which you can setup by following their 
[Getting Started with Android Studio](https://www.jetbrains.com/help/kotlin-multiplatform-dev/compose-multiplatform-setup.html) guide. 

#### JetBrains Fleet

Otherwise if you're primarily developing a Desktop App it's recommended to use [Fleet](https://www.jetbrains.com/fleet/) - JetBrains 
alternative to VS Code as a lightweight IDE for Kotlin Multiplatform Development. 
It's the preferred IDE when developing against a .NET API as you can develop both Kotlin front-end UI and backend .NET APIs from a single IDE.

To get setup with Fleet, follow the [Getting Started with JetBrains Fleet](https://www.jetbrains.com/help/kotlin-multiplatform-dev/fleet.html).

### Open Project with Fleet

Once you've installed Fleet, you can open your Desktop App project by opening the Folder in the Fleet IDE, or like VS Code
you can launch it to open your Project's folder from the command-line with:

:::sh
fleet MyApp
:::

### Setup Fleet

When first opening fleet you'll start with an empty canvas. I'd recommend adding the **Files** tool window on the left panel
to manage the Kotlin UI and the **Solution** tool window on the bottom left panel to manage the .NET API backend.

### Run .NET API and Kotlin Desktop App

Once setup, you can run both the Desktop App and the .NET API backend with from the Run Dialog with the `Ctrl+R` keyboard shortcut, 
or by clicking on the play button icon in the top menu bar:

![](/img/posts/kotlin-compose-multiplatform/fleet-run.webp)

You'll want to run the .NET API backend first by selecting your Project Name which should launch your browser at `https://localhost:5001`, 
then launch the Desktop App by selecting the **composeApp [Desktop]** configuration which should launch a working Desktop App
that calls your .NET API on each keystroke to search for matching files in your project:

![](/img/posts/kotlin-compose-multiplatform/search-files-app.webp)

The majority of the UI is maintained in
[/commonMain/kotlin/App.kt](https://github.com/NetCoreTemplates/kmp-desktop/blob/main/kmp/composeApp/src/commonMain/kotlin/App.kt)
created using Jetpack Compose's declarative Kotlin UI.

### Update Kotlin DTOs

The typed Kotlin DTOs for your .NET APIs is generated in [dtos.kt](https://github.com/NetCoreTemplates/kmp-desktop/blob/main/kmp/composeApp/src/commonMain/kotlin/dtos.kt).
Which can be regenerated by running **Update DTOs** in the Run Dialog.

Alternatively they can also be regenerated by running the `dtos` npm script from the command-line in your .NET Host project:

:::sh
npm run dtos
:::

#### Android Studio

If you're using Android Studio, you can also install the [ServiceStack Plugin](https://plugins.jetbrains.com/plugin/7749-servicestack) from
the JetBrains Marketplace:

![](/img/posts/kotlin-compose-multiplatform/android-studio-plugins.webp)

Which provides a **Add ServiceStack Reference** UI on the Context Menu, by right-clicking the folder where you want the DTOs generated:

![](/img/posts/kotlin-compose-multiplatform/add-servicestack-reference-dialog.webp)

Then to update just right-click the `dtos.kt` and click **Update ServiceStack Reference** on the context menu: 

![](/img/posts/kotlin-compose-multiplatform/update-servicestack-reference-dialog.webp)

### Command Line

For any other Text Editors or IDEs a Kotlin ServiceStack Reference can also be added from the command-line using the 
[x dotnet tool](https://docs.servicestack.net/dotnet-tool) by specifying the BaseUrl where the ServiceStack APIs are hosted, e.g:

:::sh
x kotlin https://localhost:5001
:::

To update and regenerate all Kotlin DTOs within a folder, run:  

:::sh
x kotlin
:::

## Create a new Kotlin Multiplatform App from Scratch

For a customized Compose Multiplatform App, you can create a new App with [Kotlin Multiplatform Wizard](https://kmp.jetbrains.com)
with just the options you need:

[![](/img/posts/kotlin-compose-multiplatform/kmp-wizard.webp)](https://kmp.jetbrains.com)

Which you can download in an empty Web Project:

:::sh
x new web MyApp
:::

Then open the folder with both the Kotlin Multiplatform and .NET Web App in fleet:

:::sh
fleet MyApp
:::


# New React SPA Template
Source: https://servicestack.net/posts/net8-react-spa-template

## ServiceStack React SPA Template

Just as we've enhanced the built-in ASP.NET Core React SPA template with the new [ServiceStack Vue SPA template](/posts/net8-vue-spa-template)
we've also enhanced the built-in ASP.NET Core React SPA template with the new TypeScript [Vite React SPA template](https://react-spa.web-templates.io)
with many new value-added and high-productivity features.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
        <svg class="w-28 h-28" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32"><g fill="none"><path fill="url(#vscodeIconsFileTypeVite0)" d="m29.884 6.146l-13.142 23.5a.714.714 0 0 1-1.244.005L2.096 6.148a.714.714 0 0 1 .746-1.057l13.156 2.352a.714.714 0 0 0 .253 0l12.881-2.348a.714.714 0 0 1 .752 1.05z"/><path fill="url(#vscodeIconsFileTypeVite1)" d="M22.264 2.007L12.54 3.912a.357.357 0 0 0-.288.33l-.598 10.104a.357.357 0 0 0 .437.369l2.707-.625a.357.357 0 0 1 .43.42l-.804 3.939a.357.357 0 0 0 .454.413l1.672-.508a.357.357 0 0 1 .454.414l-1.279 6.187c-.08.387.435.598.65.267l.143-.222l7.925-15.815a.357.357 0 0 0-.387-.51l-2.787.537a.357.357 0 0 1-.41-.45l1.818-6.306a.357.357 0 0 0-.412-.45"/><defs><linearGradient id="vscodeIconsFileTypeVite0" x1="6" x2="235" y1="33" y2="344" gradientTransform="translate(1.34 1.894)scale(.07142)" gradientUnits="userSpaceOnUse"><stop stop-color="#41d1ff"/><stop offset="1" stop-color="#bd34fe"/></linearGradient><linearGradient id="vscodeIconsFileTypeVite1" x1="194.651" x2="236.076" y1="8.818" y2="292.989" gradientTransform="translate(1.34 1.894)scale(.07142)" gradientUnits="userSpaceOnUse"><stop stop-color="#ffea83"/><stop offset=".083" stop-color="#ffdd35"/><stop offset="1" stop-color="#ffa800"/></linearGradient></defs></g></svg>
        <svg class="w-28 h-28" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32"><circle cx="16" cy="15.974" r="2.5" fill="#007acc"/><path fill="#007acc" d="M16 21.706a28.385 28.385 0 0 1-8.88-1.2a11.3 11.3 0 0 1-3.657-1.958A3.543 3.543 0 0 1 2 15.974c0-1.653 1.816-3.273 4.858-4.333A28.755 28.755 0 0 1 16 10.293a28.674 28.674 0 0 1 9.022 1.324a11.376 11.376 0 0 1 3.538 1.866A3.391 3.391 0 0 1 30 15.974c0 1.718-2.03 3.459-5.3 4.541a28.8 28.8 0 0 1-8.7 1.191m0-10.217a27.948 27.948 0 0 0-8.749 1.282c-2.8.977-4.055 2.313-4.055 3.2c0 .928 1.349 2.387 4.311 3.4A27.21 27.21 0 0 0 16 20.51a27.6 27.6 0 0 0 8.325-1.13C27.4 18.361 28.8 16.9 28.8 15.974a2.327 2.327 0 0 0-1.01-1.573a10.194 10.194 0 0 0-3.161-1.654A27.462 27.462 0 0 0 16 11.489"/><path fill="#007acc" d="M10.32 28.443a2.639 2.639 0 0 1-1.336-.328c-1.432-.826-1.928-3.208-1.327-6.373a28.755 28.755 0 0 1 3.4-8.593a28.676 28.676 0 0 1 5.653-7.154a11.376 11.376 0 0 1 3.384-2.133a3.391 3.391 0 0 1 2.878 0c1.489.858 1.982 3.486 1.287 6.859a28.806 28.806 0 0 1-3.316 8.133a28.385 28.385 0 0 1-5.476 7.093a11.3 11.3 0 0 1-3.523 2.189a4.926 4.926 0 0 1-1.624.307m1.773-14.7a27.948 27.948 0 0 0-3.26 8.219c-.553 2.915-.022 4.668.75 5.114c.8.463 2.742.024 5.1-2.036a27.209 27.209 0 0 0 5.227-6.79a27.6 27.6 0 0 0 3.181-7.776c.654-3.175.089-5.119-.713-5.581a2.327 2.327 0 0 0-1.868.089A10.194 10.194 0 0 0 17.5 6.9a27.464 27.464 0 0 0-5.4 6.849Z"/><path fill="#007acc" d="M21.677 28.456c-1.355 0-3.076-.82-4.868-2.361a28.756 28.756 0 0 1-5.747-7.237a28.676 28.676 0 0 1-3.374-8.471a11.376 11.376 0 0 1-.158-4A3.391 3.391 0 0 1 8.964 3.9c1.487-.861 4.01.024 6.585 2.31a28.8 28.8 0 0 1 5.39 6.934a28.384 28.384 0 0 1 3.41 8.287a11.3 11.3 0 0 1 .137 4.146a3.543 3.543 0 0 1-1.494 2.555a2.59 2.59 0 0 1-1.315.324m-9.58-10.2a27.949 27.949 0 0 0 5.492 6.929c2.249 1.935 4.033 2.351 4.8 1.9c.8-.465 1.39-2.363.782-5.434A27.212 27.212 0 0 0 19.9 13.74a27.6 27.6 0 0 0-5.145-6.64c-2.424-2.152-4.39-2.633-5.191-2.169a2.327 2.327 0 0 0-.855 1.662a10.194 10.194 0 0 0 .153 3.565a27.465 27.465 0 0 0 3.236 8.1Z"/></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="docker-containers" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    Vite React SPA Template
</h3></div>
<p class="mx-auto mt-5 max-w-3xl text-xl text-gray-500">
    Explore the high productivity features in the new ServiceStack React SPA template
</p>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="WXLF0piz6G0" style="background-image: url('https://img.youtube.com/vi/WXLF0piz6G0/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

:::{.text-center}
## Live Demo
:::

:::{.shadow .pb-1}
[![](https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/react-spa.png)](https://react-spa.web-templates.io)
:::

## ASP.NET Core React SPA Template 

The [React and ASP.NET Core](https://learn.microsoft.com/en-us/visualstudio/javascript/tutorial-asp-net-core-with-react) 
template provides a seamless starting solution which runs both the .NET API backend and Vite React frontend during development.

It's a modern template enabling an excellent developer workflow for .NET React Apps, configured with Vite's fast 
HMR (Hot Module Reload), TypeScript support with TSX enabling development of concise and expressive type-safe components.   

### Minimal API integration

Whilst a great starting point, it's still only a basic template configured with a bare-bones React Vite App that's modified
to show an example of calling a Minimal API.

### Built-in API Integration

Although the approach used isn't very scalable, with a proxy rule needed for every user-defined API route:

```ts
export default defineConfig({
    //...
    server: {
        proxy: {
            '^/weatherforecast': {
                target,
                secure: false
            }
        },
    }
})
```

And the need for hand maintained Types to describe the shape of the API responses with [Stringly Typed](https://wiki.c2.com/?StringlyTyped)
fetch API calls referencing **string** routes:

```ts
interface Forecast {
    date: string;
    temperatureC: number;
    temperatureF: number;
    summary: string;
}

function App() {
    const [forecasts, setForecasts] = useState<Forecast[]>();

    useEffect(() => {
        populateWeatherData();
    }, []);
    //...
}

async function populateWeatherData() {
    const response = await fetch('weatherforecast');
    const data = await response.json();
    setForecasts(data);
}
```

Which is used to render the API response in a hand rolled table:

```tsx
function App() {
    //...
    const contents = forecasts === undefined
        ? <p><em>Loading... Please refresh once the ASP.NET backend has started. See 
            <a href="https://aka.ms/jspsintegrationreact">jsps</a> for more details.
            </em></p>
        : <table className="table table-striped" aria-labelledby="tabelLabel">
            <thead>
            <tr>
                <th>Date</th>
                <th>Temp. (C)</th>
                <th>Temp. (F)</th>
                <th>Summary</th>
            </tr>
            </thead>
            <tbody>
                {forecasts.map(forecast =>
                    <tr key={forecast.date}>
                        <td>{forecast.date}</td>
                        <td>{forecast.temperatureC}</td>
                        <td>{forecast.temperatureF}</td>
                        <td>{forecast.summary}</td>
                    </tr>
                )}
            </tbody>
        </table>;
}
```

### ServiceStack API Integration

Fortunately ServiceStack can significantly improve this development experience with the
[/api pre-defined route](https://docs.servicestack.net/endpoint-routing#api-pre-defined-route) where only a single
proxy rule is needed to proxy all APIs:

```ts
export default defineConfig({
    //...
    server: {
        proxy: {
            '^/api': {
                target,
                secure: false
            }
        },
    }
})
```

### End-to-end Typed APIs

Instead of hand-rolled types and Stringly Typed API calls, it utilizes server
[generated TypeScript DTOs](https://docs.servicestack.net/typescript-add-servicestack-reference)
with a generic JsonServiceClient to enable end-to-end Typed APIs:

```ts
import { useState, useEffect } from "react"
import { useClient } from "@/gateway"
import { GetWeatherForecast } from "@/dtos"

const client = useClient()
const [forecasts, setForecasts] = useState<Forecast[]>([])

useEffect(() => {
    (async () => {
        const api = await client.api(new GetWeatherForecast())
        if (api.succeeded) {
            setForecasts(api.response!)
        }
    })()
}, [])
```

This benefits in less code to maintain, immediate static typing analysis to ensure correct usage of APIs and valuable
feedback when APIs are changed, that's easily updated with a single command:

:::sh
npm run dtos
:::

### React Component Ecosystem

Given it's popularity, React has arguably the richest ecosystem of freely available libraries and components, a good
example are the popular [shadcn/ui](https://ui.shadcn.com) Tailwind components. Unlike most libraries they're source copied 
piecemeal into your project where they're locally modifiable, i.e. instead of an immutable package reference. 

As they're just blueprints, they're not dependent on a single library and will utilize the best library to implement
each component if needed. E.g. the [Data Table](https://ui.shadcn.com/docs/components/data-table) component documents how to implement
your own Data Table utilizing the headless [TanStack Table](https://tanstack.com/table/latest) - a version of which
we've built into [DataTable.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/components/DataTable.tsx)
which is used in the template to implement both complex CRUD UIs and 
[weather.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/weather.tsx) simple table results:

```tsx
import { columnDefs, DataTable, getCoreRowModel } from "@/components/DataTable.tsx"

const columns = columnDefs(['date', 'temperatureC', 'temperatureF', 'summary'],
  ({ temperatureC, temperatureF}) => {
    temperatureC.header = "Temp. (C)"
    temperatureF.header = "Temp. (F)"
    temperatureC.cell = temperatureF.cell = ({ getValue }) => <>{getValue()}&deg;</>
  })

return (<LayoutPage title="Weather">
  <DataTable columns={columns} data={forecasts} getCoreRowModel={getCoreRowModel()} />
</LayoutPage>)
```

To render the [/weather](https://react-spa.web-templates.io/weather) customized Data Table:

:::{.mx-auto .max-w-lg .shadow .rounded}
[![](/img/posts/net8-react-spa-template/data-table.png)](https://react-spa.web-templates.io/weather)
:::

The template also includes customizable [Form.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/components/Form.tsx)
Input components which can be used to create beautiful validation-bound forms which effortlessly integrates with ServiceStack's
[Error Handling](https://docs.servicestack.net/error-handling) and 
[Declarative Validation](https://docs.servicestack.net/declarative-validation) attributes.

## ServiceStack React SPA Features

Other high-productivity features available in the ServiceStack React SPA template include:

### Integrated Identity Auth

Pre-configured with ASP.NET Core Identity Auth, including Sign In and Custom Registration APIs and
UI Pages which can be customized as needed, examples of Role-based security as well as a turn key solution for
Integrating Identity Auth Registration workflow with your [SMTP Provider](https://docs.servicestack.net/auth/identity-auth#smtp-iemailsender)
with all emails sent from a managed non-blocking [Background MQ](https://docs.servicestack.net/background-mq)
for optimal responsiveness and execution.

### tailwindcss

[Tailwind](https://tailwindcss.com) has quickly become the best modern CSS framework for creating scalable,
[mobile-first](https://tailwindcss.com/#mobile-first) responsive websites built
upon a beautiful expert-crafted constraint-based [Design System](https://tailwindcss.com/#constraint-based)
that enables effortless reuse of a growing suite of [Free Community](https://tailwindcomponents.com) and
professionally-designed [Tailwind UI Component](https://tailwindui.com) Libraries, invaluable for quickly creating beautiful websites.

[![](/img/pages/blazor/tailwindui.png)](https://tailwindcss.com)

<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mx-auto w-40 h-40 text-gray-800" viewBox="0 0 24 24">
    <path fill="currentColor" d="M9.37 5.51A7.35 7.35 0 0 0 9.1 7.5c0 4.08 3.32 7.4 7.4 7.4c.68 0 1.35-.09 1.99-.27A7.014 7.014 0 0 1 12 19c-3.86 0-7-3.14-7-7c0-2.93 1.81-5.45 4.37-6.49zM12 3a9 9 0 1 0 9 9c0-.46-.04-.92-.1-1.36a5.389 5.389 0 0 1-4.4 2.26a5.403 5.403 0 0 1-3.14-9.8c-.44-.06-.9-.1-1.36-.1z"></path>
</svg>

In addition to revolutionizing how we style mobile-first responsive Apps, Tailwind's
[Dark Mode](https://tailwindcss.com/#dark-mode) does the same for enabling Dark Mode
a feature supported throughout the template and its Tailwind UI Components.

[![](/img/posts/net8-react-spa-template/dark-mode.png)](https://tailwindcss.com/#dark-mode)

### Built for Productivity

So that you're immediately productive out-of-the-box, the template includes a rich set of high-productivity features, including:

|                                                                     |                                                              |
|---------------------------------------------------------------------|--------------------------------------------------------------|
| [tailwind/typography](https://tailwindcss-typography.vercel.app)    | Beautiful css typography for markdown articles & blog posts  |
| [tailwind/forms](https://github.com/tailwindlabs/tailwindcss-forms) | Beautiful css form & input styles that's easily overridable  |
| [Markdown](https://mdxjs.com/docs/getting-started/)                 | Native [mdx](https://mdxjs.com) Markdown integration         |
| [React Router](https://reactrouter.com)                             | Full featured routing library for React                      |
| [plugin/press](https://github.com/ServiceStack/vite-plugin-press)   | Static markdown for creating blogs, videos and other content |
| [plugin/pages](https://github.com/hannoeru/vite-plugin-pages)       | Conventional file system based routing for Vite              |
| [plugin/svg](https://github.com/pd4d10/vite-plugin-svgr)            | Load SVG files as React components                           |
| [Iconify](https://iconify.design)                                   | Unified registry to access 100k+ high quality SVG icons      |

### Bookings CRUD Pages

The [Bookings CRUD example](https://react-spa.web-templates.io/bookings-crud) shows how you can utilize a customized 
Data Table and templates Form components to create a beautifully styled CRUD UI with minimal effort.

## Vite Press Plugin

[![](https://images.unsplash.com/photo-1524668951403-d44b28200ce0?crop=entropy&fit=crop&h=384&w=768)](https://vue-spa.web-templates.io/posts/vite-press-plugin)

Most Apps typically have a mix of dynamic functionality and static content which in our experience is best maintained
in Markdown, which is why excited about the new [Vite Press Plugin](https://vue-spa.web-templates.io/posts/vite-press-plugin)
which brings the same Markdown features in our
[razor-ssg](https://razor-ssg.web-templates.io), [razor-press](https://razor-press.web-templates.io) and our
[blazor-vue](https://blazor-vue.web-templates.io) templates, and re-implements them in Vite where they can be used
to add the same rich content features to Vite Vue and Vite React Apps.

A goal for vite-press-plugin is to implement a suite of universal markdown-powered features that can be reused across all
our Vue, React and .NET Razor and Blazor project templates, allowing you to freely copy and incorporate same set of
markdown feature folders to power markdown content features across a range of websites built with different technologies.

All of Razor SSG's features are available in Vite Press Plugin, including:

- [Blog](https://vue-spa.web-templates.io/blog) - Full Featured, beautiful Tailwind Blog with multiple discoverable views
- [What's New](https://vue-spa.web-templates.io/whatsnew) - Build Product and Feature Release pages
- [Videos](https://vue-spa.web-templates.io/videos) - Maintain Video Libraries and Playlists
- [Metadata APIs](https://vue-spa.web-templates.io/posts/vite-press-plugin#metadata-apis-feature) - Generate queryable static .json metadata APIs for all content
- [Includes](https://vue-spa.web-templates.io/posts/vite-press-plugin#includes-feature) - Create and reuse Markdown fragments

It also supports an enhanced version of markdown for embedding richer UI markup in markdown content where most of
[VitePress Containers](https://vitepress.dev/guide/markdown#custom-containers) are supported, including:

- [Custom Markdown Containers](https://vue-spa.web-templates.io/posts/vite-press-plugin#markdown-containers)
    - **Alerts**
        - `info`
        - `tip`
        - `warning`
        - `danger`
    - `copy`
    - `sh`
    - `youtube`
- [Markdown Fenced Code Blocks](https://vue-spa.web-templates.io/posts/vite-press-plugin#markdown-fenced-code-blocks) - Convert fenced code blocks into Richer UIs

### React Components In Markdown

At the cost of reduced portability, you’re also able to embed richer Interactive Vue components directly in markdown:

- [React Components in Markdown](https://react-spa.web-templates.io/posts/markdown-components-in-react)


# New Vue SPA Template
Source: https://servicestack.net/posts/net8-vue-spa-template

With ServiceStack [now fully integrated with .NET 8](/posts/servicestack-endpoint-routing), our focus has shifted 
from providing platform-agnostic solutions that supports all ServiceStack's .NET Framework and .NET hosts to building 
on the new capabilities of .NET 8 by enhancing ASP .NET Core's built-in features and templates with 
ServiceStack's high-productivity features.

## ServiceStack Vue SPA Template

The latest [Vue SPA](https://vue-spa.web-templates.io) template is a good example of this, building on and enhancing
the built-in ASP.NET Core Vue SPA template with many high-productivity features.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
        <svg class="-mt-3 w-32 h-32" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32"><g fill="none"><path fill="url(#vscodeIconsFileTypeVite0)" d="m29.884 6.146l-13.142 23.5a.714.714 0 0 1-1.244.005L2.096 6.148a.714.714 0 0 1 .746-1.057l13.156 2.352a.714.714 0 0 0 .253 0l12.881-2.348a.714.714 0 0 1 .752 1.05z"/><path fill="url(#vscodeIconsFileTypeVite1)" d="M22.264 2.007L12.54 3.912a.357.357 0 0 0-.288.33l-.598 10.104a.357.357 0 0 0 .437.369l2.707-.625a.357.357 0 0 1 .43.42l-.804 3.939a.357.357 0 0 0 .454.413l1.672-.508a.357.357 0 0 1 .454.414l-1.279 6.187c-.08.387.435.598.65.267l.143-.222l7.925-15.815a.357.357 0 0 0-.387-.51l-2.787.537a.357.357 0 0 1-.41-.45l1.818-6.306a.357.357 0 0 0-.412-.45"/><defs><linearGradient id="vscodeIconsFileTypeVite0" x1="6" x2="235" y1="33" y2="344" gradientTransform="translate(1.34 1.894)scale(.07142)" gradientUnits="userSpaceOnUse"><stop stop-color="#41d1ff"/><stop offset="1" stop-color="#bd34fe"/></linearGradient><linearGradient id="vscodeIconsFileTypeVite1" x1="194.651" x2="236.076" y1="8.818" y2="292.989" gradientTransform="translate(1.34 1.894)scale(.07142)" gradientUnits="userSpaceOnUse"><stop stop-color="#ffea83"/><stop offset=".083" stop-color="#ffdd35"/><stop offset="1" stop-color="#ffa800"/></linearGradient></defs></g></svg>
        <svg class="w-28 h-28" xmlns="http://www.w3.org/2000/svg" width="1.16em" height="1em" viewBox="0 0 256 221"><path fill="#41b883" d="M204.8 0H256L128 220.8L0 0h97.92L128 51.2L157.44 0z"/><path fill="#41b883" d="m0 0l128 220.8L256 0h-51.2L128 132.48L50.56 0z"/><path fill="#35495e" d="M50.56 0L128 133.12L204.8 0h-47.36L128 51.2L97.92 0z"/></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="docker-containers" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    Vite Vue SPA Template
</h3></div>
<p class="mx-auto mt-5 max-w-3xl text-xl text-gray-500">
    Explore the high productivity features in the new ServiceStack Vue SPA template
</p>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="JlUjWlVslRg" style="background-image: url('https://img.youtube.com/vi/JlUjWlVslRg/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

:::{.text-center}
## Live Demo
:::

:::{.shadow .pb-1}
[![](https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/vue-spa.png)](https://vue-spa.web-templates.io)
:::

## ASP.NET Core Vue SPA Template 

The [Vue and ASP.NET Core](https://learn.microsoft.com/en-us/visualstudio/javascript/tutorial-asp-net-core-with-vue) 
template provides a seamless starting solution which runs both the .NET API backend and Vite Vue frontend during development.

It's a modern template capturing the best Vue has to offer, configured with Vite's fast HMR (Hot Module Reload) and TypeScript support - 
it allows App's to be developed with Vue's typed [Single File Components](https://vuejs.org/guide/scaling-up/sfc.html) enabling
both a productive development experience and an optimal high-performance production build at runtime.

### Minimal API integration

Whilst a great starting point, it's still only a basic template configured with a bare-bones Vue Vite App that's modified
to show an example of calling a Minimal API.

### Built-in API Integration

Although the approach used isn't very scalable, with a proxy rule needed for every user-defined API route:

```ts
export default defineConfig({
    //...
    server: {
        proxy: {
            '^/weatherforecast': {
                target,
                secure: false
            }
        },
    }
})
```

And the need for hand maintained Types to describe the shape of the API responses with [Stringly Typed](https://wiki.c2.com/?StringlyTyped)
fetch API calls referencing **string** routes:

```ts
import { defineComponent } from 'vue';

type Forecasts = {
    date: string,
    temperatureC: string,
    temperatureF: string,
    summary: string
}[];

interface Data {
    loading: boolean,
    post: null | Forecasts
}

export default defineComponent({
    data(): Data {
        return {
            loading: false,
            post: null
        };
    },
    created() {
        // fetch the data when the view is created and the data is
        // already being observed
        this.fetchData();
    },
    watch: {
        // call again the method if the route changes
        '$route': 'fetchData'
    },
    methods: {
        fetchData(): void {
            this.post = null;
            this.loading = true;

            fetch('weatherforecast')
                .then(r => r.json())
                .then(json => {
                    this.post = json as Forecasts;
                    this.loading = false;
                    return;
                });
        }
    },
});
```

Which is used to render the API response in a hand rolled table: 

```html
<div v-if="post" class="content">
    <table>
        <thead>
            <tr>
                <th>Date</th>
                <th>Temp. (C)</th>
                <th>Temp. (F)</th>
                <th>Summary</th>
            </tr>
        </thead>
        <tbody>
            <tr v-for="forecast in post" :key="forecast.date">
                <td>{{ forecast.date }}</td>
                <td>{{ forecast.temperatureC }}</td>
                <td>{{ forecast.temperatureF }}</td>
                <td>{{ forecast.summary }}</td>
            </tr>
        </tbody>
    </table>
</div>
```

### ServiceStack API Integration

Fortunately ServiceStack can significantly improve this development experience with the 
[/api pre-defined route](https://docs.servicestack.net/endpoint-routing#api-pre-defined-route) where only a single
proxy rule is needed to proxy all APIs:

```ts
export default defineConfig({
    //...
    server: {
        proxy: {
            '^/api': {
                target,
                secure: false
            }
        },
    }
})
```

### End-to-end Typed APIs

Instead of hand-rolled types and Stringly Typed API calls, it utilizes server 
[generated TypeScript DTOs](https://docs.servicestack.net/typescript-add-servicestack-reference)
with a generic JsonServiceClient to enable end-to-end Typed APIs:

```ts
import { ref, onMounted } from 'vue'
import { ApiResult } from "@servicestack/client"
import { useClient } from "@servicestack/vue"
import { GetWeatherForecast } from "@/dtos"

const client = useClient()
const api = ref(new ApiResult())

onMounted(async () => {
    api.value = await client.api(new GetWeatherForecast())
})
```

This benefits in less code to maintain, immediate static typing analysis to ensure correct usage of APIs and valuable 
feedback when APIs are changed, that's easily updated with a single command:

:::sh
npm run dtos
:::

### High Productivity Vue Components

With access to the [ServiceStack Vue Components](https://docs.servicestack.net/vue/) library there's also less code to 
maintain in the UI, where you can render a beautiful tailwind styled DataGrid with just:

```html
<DataGrid :items="api.response" />
```

## ServiceStack Vue SPA Features

Other high-productivity features available in the ServiceStack Vue SPA template include:

### Integrated Identity Auth

Pre-configured with ASP.NET Core Identity Auth, including Sign In and Custom Registration APIs and
UI Pages which can be customized as needed, examples of Role-based security as well as a turn key solution for
Integrating Identity Auth Registration workflow with your [SMTP Provider](https://docs.servicestack.net/auth/identity-auth#smtp-iemailsender)
with all emails sent from a managed non-blocking [Background MQ](https://docs.servicestack.net/background-mq)
for optimal responsiveness and execution.

### tailwindcss

[Tailwind](https://tailwindcss.com) has quickly become the best modern CSS framework for creating scalable, 
[mobile-first](https://tailwindcss.com/#mobile-first) responsive websites built
upon a beautiful expert-crafted constraint-based [Design System](https://tailwindcss.com/#constraint-based) 
that enables effortless reuse of a growing suite of [Free Community](https://tailwindcomponents.com) and 
professionally-designed [Tailwind UI Component](https://tailwindui.com) Libraries, invaluable for quickly creating beautiful websites.

[![](/img/pages/blazor/tailwindui.png)](https://tailwindcss.com)

<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mx-auto w-40 h-40 text-gray-800" viewBox="0 0 24 24">
    <path fill="currentColor" d="M9.37 5.51A7.35 7.35 0 0 0 9.1 7.5c0 4.08 3.32 7.4 7.4 7.4c.68 0 1.35-.09 1.99-.27A7.014 7.014 0 0 1 12 19c-3.86 0-7-3.14-7-7c0-2.93 1.81-5.45 4.37-6.49zM12 3a9 9 0 1 0 9 9c0-.46-.04-.92-.1-1.36a5.389 5.389 0 0 1-4.4 2.26a5.403 5.403 0 0 1-3.14-9.8c-.44-.06-.9-.1-1.36-.1z"></path>
</svg>

### Dark Mode

In addition to revolutionizing how we style mobile-first responsive Apps, Tailwind's
[Dark Mode](https://tailwindcss.com/#dark-mode) does the same for enabling Dark Mode
a feature supported throughout all of ServiceStack's [Vue Component Library](https://docs.servicestack.net/vue/).

![](/img/whatsnew/v6.5/dark-and-light-mode.png)

### Built for Productivity

So that you're immediately productive out-of-the-box, the template includes a rich set of high-productivity features, including:

|                                                                            |                                                              |
|----------------------------------------------------------------------------|--------------------------------------------------------------|
| [tailwind/typography](https://tailwindcss-typography.vercel.app)           | Beautiful css typography for markdown articles & blog posts  |
| [tailwind/forms](https://github.com/tailwindlabs/tailwindcss-forms)        | Beautiful css form & input styles that's easily overridable  |
| [Markdown](https://github.com/markdown-it/markdown-it)                     | Native Markdown integration                                  |
| [plugin/press](https://github.com/ServiceStack/vite-plugin-press)          | Static markdown for creating blogs, videos and other content |
| [plugin/vue-router](https://github.com/posva/unplugin-vue-router)          | Conventional file system based routing for Vue 3 on Vite     |
| [plugin/layouts](https://github.com/JohnCampionJr/vite-plugin-vue-layouts) | Support for multiple page layouts                            |
| [plugin/components](https://github.com/antfu/unplugin-vue-components)      | Auto importing & registering of components on-demand         |
| [plugin/svg](https://github.com/jpkleemans/vite-svg-loader)                | Load SVG files as Vue components                             |
| [Iconify](https://iconify.design)                                          | Unified registry to access 100k+ high quality SVG icons      |

### Bookings CRUD Pages

Bookings CRUD example shows how you can rapidly develop beautiful responsive, customized CRUD UIs with minimal effort using 
[AutoQuery APIs](https://docs.servicestack.net/autoquery/), [AutoForms](https://docs.servicestack.net/autoform) &
[AutoQueryGrid](https://blazor-gallery.servicestack.net/gallery/autoquerygrid) Vue Components.

### Admin Pages

Whilst Bookings CRUD is a good example of creating custom UI for end users, you may also want to quickly develop a set
of back-office CRUD Admin UIs to manage your App's Database tables, which is easily achievable AutoQueryGrid's default
behavior:

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="wlRA4_owEsc" style="background-image: url('https://img.youtube.com/vi/wlRA4_owEsc/maxresdefault.jpg')"></lite-youtube>
</div>

The development UX of Admin Pages is further improved in Vue Vite which is able to use SFC Pages and conventional file system
routing to quickly add Admin Pages to manage an App's back-end tables, e.g:

#### [/admin/coupons.vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/admin/coupons.vue)

```html
<AutoQueryGrid type="Coupon" />
```

#### [/admin/bookings.vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/admin/bookings.vue)

```html
<AutoQueryGrid type="Booking"
selected-columns="id,name,roomType,roomNumber,bookingStartDate,cost,couponId,discount"
  :header-titles="{ roomNumber:'Room No', bookingStartDate:'Start Date' }"
   :visible-from="{ roomNumber:'lg', cost:'md', couponId:'xl', discount:'never' }" />
```

## Vite Press Plugin

[![](https://images.unsplash.com/photo-1524668951403-d44b28200ce0?crop=entropy&fit=crop&h=384&w=768)](https://vue-spa.web-templates.io/posts/vite-press-plugin)

Most Apps typically have a mix of dynamic functionality and static content which in our experience is best maintained
in Markdown, which is why excited about the new [Vite Press Plugin](https://vue-spa.web-templates.io/posts/vite-press-plugin)
which brings the same Markdown features in our 
[razor-ssg](https://razor-ssg.web-templates.io), [razor-press](https://razor-press.web-templates.io) and our
[blazor-vue](https://blazor-vue.web-templates.io) templates, and re-implements them in Vite where they can be used
to add the same rich content features to Vite Vue and Vite React Apps.

A goal for vite-press-plugin is to implement a suite of universal markdown-powered features that can be reused across all 
our Vue, React and .NET Razor and Blazor project templates, allowing you to freely copy and incorporate same set of 
markdown feature folders to power markdown content features across a range of websites built with different technologies.

All of Razor SSG's features are available in Vite Press Plugin, including:

 - [Blog](https://vue-spa.web-templates.io/blog) - Full Featured, beautiful Tailwind Blog with multiple discoverable views
 - [What's New](https://vue-spa.web-templates.io/whatsnew) - Build Product and Feature Release pages
 - [Videos](https://vue-spa.web-templates.io/videos) - Maintain Video Libraries and Playlists
 - [Metadata APIs](https://vue-spa.web-templates.io/posts/vite-press-plugin#metadata-apis-feature) - Generate queryable static .json metadata APIs for all content 
 - [Includes](https://vue-spa.web-templates.io/posts/vite-press-plugin#includes-feature) - Create and reuse Markdown fragments

It also supports an enhanced version of markdown for embedding richer UI markup in markdown content where most of 
[VitePress Containers](https://vitepress.dev/guide/markdown#custom-containers) are supported, including:  

 - [Custom Markdown Containers](https://vue-spa.web-templates.io/posts/vite-press-plugin#markdown-containers)
   - **Alerts**
     - `info`
     - `tip`
     - `warning`
     - `danger`
   - `copy` 
   - `sh`
   - `youtube`
 - [Markdown Fenced Code Blocks](https://vue-spa.web-templates.io/posts/vite-press-plugin#markdown-fenced-code-blocks) - Convert fenced code blocks into Richer UIs

### Vue Components In Markdown

At the cost of reduced portability, you’re also able to embed richer Interactive Vue components directly in markdown:

- [Vue Components in Markdown](https://vue-spa.web-templates.io/posts/markdown-components-in-vue)


# Vite Press Plugin
Source: https://servicestack.net/posts/vite-press-plugin

The Vite Press Plugin is an alternative to [VitePress](https://vitepress.dev) for adding Markdown features 
to existing Vite Vue or React projects. It's a non-intrusive plugin for Vue and React Vite apps that want to 
add markdown powered content features without needing to adopt an opinionated framework for their entire App.

## Universal Markdown Features

A goal for **vite-press-plugin** is to implement a suite of universal markdown-powered features that can be reused across 
Vue, React and .NET Razor and Blazor projects, allowing you to incorporate same set of markdown feature 
folders to power markdown content features across a range of websites built with different technologies.

### Vite Apps with vite-press-plugin

The **vite-press-plugin** currently powers the markdown features in the static Vite Vue and React templates which are
ideal for creating static websites, blogs, documentation and marketing websites that can be hosted FREE on [GitHub Pages CDN](https://pages.github.com):

#### Static Vite Templates with vite-press-plugin

- [press-vue](https://press-vue.servicestack.net) - Vite Vue App
- [press-react](https://press-react.servicestack.net) - Vite React App

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://press-vue.servicestack.net">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/press-vue.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">press-vue.servicestack.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://press-react.servicestack.net">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/press-react.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">press-react.servicestack.net</div>
    </a>
</div>

The **vite-press-plugin** makes the Markdown features available to the Vite App, whilst the markdown rendering itself is optimally
implemented in:

- Vue Templates - with [markdown-it](https://github.com/markdown-it/markdown-it) in [Vue SFC](https://vuejs.org/guide/scaling-up/sfc.html) Components
- React Templates - with [remark](https://github.com/remarkjs/remark) and [MDX](https://mdxjs.com) in [React](https://react.dev) Components

#### .NET 8 API backend with Vite Vue & React SPA frontend

When more capabilities are required and you want a .NET API backend to your Vite Vue or React SPA frontend, 
you can use one of our integrated .NET 8 SPA templates:

 - [vue-spa](https://vue-spa.web-templates.io) - .NET 8 API with Vite Vue SPA frontend
 - [react-spa](https://react-spa.web-templates.io) - .NET 8 API with Vite React SPA frontend

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://vue-spa.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/vue-spa.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">vue-spa.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://react-spa.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/react-spa.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">react-spa.web-templates.io</div>
    </a>
</div>

### .NET Templates with C# and Markdig

Whilst the same Markdown feature folders are [implemented in C#](https://razor-ssg.web-templates.io/posts/razor-ssg)
and rendered with [Markdig](https://github.com/xoofx/markdig) and either Razor Pages or Blazor Components:

#### .NET 8 Razor SSG and Blazor SSR Templates

 - [razor-ssg](https://razor-ssg.web-templates.io) - .NET Razor SSG Blog and Marketing Website with **Markdig**
 - [razor-press](https://razor-press.web-templates.io) - .NET Razor SSG Documentation Website with **Markdig**
 - [blazor-vue](https://blazor-vue.web-templates.io) - .NET 8 Blazor Server Rendered Website with **Markdig**

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://razor-ssg.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/razor-ssg.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">razor-ssg.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://blazor-vue.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor-vue.png" />
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-vue.web-templates.io</div>
    </a>
</div>

### Markdown Feature Folders

The content for each Markdown feature is maintained within its own feature folder with a `_` prefix:

```files
/_includes
/_posts
/_videos
/_whatsnew
```

#### Markdown Document Structure

Additional metadata for each markdown page is maintained in the frontmatter of each markdown page, e.g.
the front matter for this blog post contains:

```md
---
title:   Vite Press Plugin
summary: Introducing the Vite Press Plugin
author:  Lucy Bates
tags:    [docs,markdown]
image:   https://picsum.photos/2000/1000
---
```

The frontmatter is used in combination with file attributes to populate the document metadata.
The schema used to support the current markdown features include:

```ts
type Doc = {
    title: string     // title of Markdown page (frontmatter)
    slug: string      // slug to page (populated)
    path: string      // path to page (populated)
    fileName: string  // filename of markdown file (populated)
    content: string   // markdown content (populated)
    date: string      // date of page (frontmatter)
    tags: string[]    // related tags (frontmatter)
    order?: number    // explicit page ordering (frontmatter)
    group?: string    // which group page belongs to (populated)
    draft?: boolean   // make visible in production (frontmatter)
    wordCount: number      // (populated)
    lineCount: number      // (populated)
    minutesToRead: number  // (populated)
}

type Post = Doc & {
    summary: string // short summary of blog post (frontmatter)
    author: string  // author of blog post (frontmatter)
    image: string   // hero image of blog post (frontmatter)
}

type Video = Doc & {
    url: string // URL of YouTube Video
}

type WhatsNew = Doc & {
    url: string    // URL of YouTube Video
    image: string  // Image to display for feature
}
```

Markdown files can contain additional frontmatter which is also merged with the document metadata.

### Accessing Markdown Metadata

In Vue App's the Metadata is available as an injected dependency that's navigable with the typed `VirtualPress` schema, e.g:

```ts
import type { VirtualPress } from "vite-plugin-press"

const press:VirtualPress = inject('press')!
```

In React App's it's available via an injected context:

```ts
import { PressContext } from "@/contexts"

const press = useContext(PressContext)
```

Which is defined as:

```ts
import { createContext } from 'react'
import type { VirtualPress } from 'vite-plugin-press'

export const PressContext = createContext<VirtualPress>({} as VirtualPress)
```

This `VirtualPress` metadata is used to power all markdown features.

### Blog

The blog maintains its markdown posts in a flat  [/_posts](https://github.com/NetCoreTemplates/vue-spa/tree/main/MyApp.Client/src/_posts) 
folder which each Markdown post containing its publish date and URL slug it should be published under, e.g:

```files
/_posts
  2023-01-21_start.md
  2024-02-11_jwt-identity-auth.md
  2024-03-01_vite-press-plugin.md
```

Supporting all Blog features requires several different pages to render each of its view:

| Description | Example | Vue | React | 
| - | - | - | - |
| Main Blog layout | [/blog](/blog) | [blog.vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/blog.vue) | [blog.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/blog.tsx) |
| Navigable Archive of Posts | [/posts](/posts) | [index.vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/posts/index.vue) | [index.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/posts/index.tsx) |
| Individual Blog Post (like this!) | [/posts/vite-press-plugin](/posts/vite-press-plugin) | [\[slug\].vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/posts/%5Bslug%5D.vue) | [\[slug\].tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/posts/%5Bslug%5D.tsx) |
| Display Posts by Author | [/posts/author/lucy-bates](/posts/author/lucy-bates) | [\[name\].vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/posts/author/%5Bname%5D.vue) | [\[name\].tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/posts/author/%5Bname%5D.tsx) |
| Display Posts by Tag | [/posts/tagged/markdown](/posts/tagged/markdown) | [\[tag\].vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/posts/tagged/%5Btag%5D.vue) | [\[tag\].tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/posts/tagged/%5Btag%5D.tsx) |
| Display Posts by Year | [/posts/year/2024](/posts/year/2024) | [\[year\].vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/posts/year/%5Byear%5D.vue) | [\[year\].tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/posts/year/%5Byear%5D.tsx) |

#### Configuration

Additional information about the Website Blog is maintained in `_posts/config.json`

```json
{
  "localBaseUrl": "http://localhost:5173",
  "publicBaseUrl": "https://press-vue.servicestack.net",
  "siteTwitter": "@Vue",
  "blogTitle": "From the blog",
  "blogDescription": "Writing on software design and aerospace industry.",
  "blogEmail": "email@example.org (Vue)",
  "blogImageUrl": "https://servicestack.net/img/logo.png"
}
```

#### Authors

Whilst information about Post Authors are maintained in `_posts/authors.json`

```json
[
  {
    "name": "Lucy Bates",
    "email": "lucy@email.org",
    "bio": "Writing on software design and aerospace industry.",
    "profileUrl": "/img/profiles/user1.svg",
    "twitterUrl": "https://twitter.com/lucy",
    "threadsUrl": "https://threads.net/@lucy",
    "gitHubUrl": "https://github.com/lucy"
  },
]
```

To associate an Author the **name** property is used to match a posts frontmatter **author**.

### General Features

Most unique markdown features are captured in their Markdown's frontmatter metadata, but in general these features
are broadly available for all features:

 - **Live Reload** - Latest Markdown content is displayed during **Development** 
 - **Drafts** - Prevent posts being worked on from being published with `draft: true`
 - **Future Dates** - Posts with a future date wont be published until that date

### What's New Feature

The [/whatsnew](/whatsnew) page is an example of creating a custom Markdown feature to implement a portfolio or a product releases page
where a new folder is created per release, containing both release date and release or project name, with all features in that release 
maintained markdown content sorted in alphabetical order:

```files
/_whatsnew
  /2023-03-08_Animaginary
    feature1.md
  /2023-03-18_OpenShuttle
    feature1.md
  /2023-03-28_Planetaria
    feature1.md
```

What's New follows the same structure as Pages feature which is rendered in:

 - [whatsnew.vue](https://github.com/NetCoreTemplates/vue-spa/blob/main/MyApp.Client/src/pages/whatsnew.vue)
 - [whatsnew.tsx](https://github.com/NetCoreTemplates/react-spa/blob/main/MyApp.Client/src/pages/whatsnew.tsx)
 
### Videos Feature

The videos feature maintained in the `_videos` folder allows grouping of related videos into different folder groups, e.g:

```files
/_videos
  /vue
    admin.md
    autoquerygrid.md
    components.md
  /react
    locode.md
    bookings.md
    nextjs.md
```

These can then be rendered as UI fragments using the `<VideoGroup>` component, e.g:

```tsx
<VideoGroup
  title="Vue Components"
  summary="Learn about productive features in Vue Component Library"
  group="vue"
  learnMore="https://docs.servicestack.net/vue/" />
```

### Includes Feature

The includes feature allows maintaining reusable markdown fragments in the `_includes` folder, e.g:

```files
/_includes
  /features
    videos.md
    whatsnew.md
  privacy.md
```

Which can be included in other Markdown files with:

```md
:::include privacy.md:::

:::include features/include.md:::
```

Alternatively they can be included in other Vue, React or Markdown pages with the `<Include>` component, e.g:

```tsx
<Include src="privacy.md" />

<Include src="features/include.md" />
```

### Metadata APIs Feature

To support external clients from querying static markdown metadata you can export it to pre-rendered static `*.json` 
data structures by configuring `metadataPath` to the location you the `*.json` files published to, e.g:

```ts
export default defineConfig({
    plugins: [
        Press({
            metadataPath: 'public/api',
        }),
    ]
})
```

This will publish all the content of each content type in the year they were published in, along with an `all.json` containing
all content published in that year as well aso for all time, e.g:

```files
/meta
  /2022
    all.json
    posts.json
    videos.json
  /2023
    all.json
    posts.json
  /2024
    all.json
    posts.json
    videos.json
    whatsnew.json
  all.json
  index.json
```

With this you can fetch the metadata of all the new **Blog Posts** added in **2023** from:

[/api/2024/blog.json](/api/2024/blog.json)

Or all the website content added in **2024** from:

[/api/2024/all.json](/api/2024/all.json)

Or **ALL** the website metadata content from:

[/api/all.json](/api/all.json)

This feature makes it possible to support use-cases like CreatorKit's
[Generating Newsletters](https://servicestack.net/creatorkit/portal-mailruns#generating-newsletters) feature which generates 
a Monthly Newsletter Email with all new content added within a specified period.

## Markdown Containers

Most of [VitePress Containers](https://vitepress.dev/guide/markdown#custom-containers) are also implemented, enabling
rich markup to enhance markdown content and documentation universally across all Markdown App implementations:

#### Input

    :::info
    This is an info box.
    :::

    :::tip
    This is a tip.
    :::

    :::warning
    This is a warning.
    :::

    :::danger
    This is a dangerous warning.
    :::

#### Output

:::info
This is an info box.
:::

:::tip
This is a tip.
:::

:::warning
This is a warning.
:::

:::danger
This is a dangerous warning.
:::

### Custom Title

You can specify a custom title by appending the text right after the container type:

#### Input

    :::danger STOP
    Danger zone, do not proceed
    :::

#### Output

:::danger STOP
Danger zone, do not proceed
:::


### copy

The **copy** container is ideal for displaying text snippets in a component that allows for easy copying:

#### Input

    :::copy
    Copy Me!
    :::

#### Output

:::copy
Copy Me!
:::

HTML or XML fragments can also be copied by escaping them first:

#### Input

```md
:::copy
`<PackageReference Include="ServiceStack" Version="8.*" />`
:::
```

#### Output

:::copy
`<PackageReference Include="ServiceStack" Version="8.*" />`
:::

### sh

Similarly the **sh** container is ideal for displaying and copying shell commands:

#### Input

    :::sh
    npm run dev
    :::

#### Output

:::sh
npm run dev
:::

### YouTube

For embedding YouTube Videos, optimally rendered using the `<LiteYouTube>` component, e.g:

#### Input

    :::youtube YIa0w6whe2U
    Vue Components Library
    :::

#### Output

:::youtube YIa0w6whe2U
Vue Components Library
:::

## Markdown Fenced Code Blocks

For more flexibility you can utilize custom fenced components like the `files` fenced code block which can 
be used to capture ascii representation of a structured documentation like a folder & file structure, e.g:

    ```files
    /_videos
      /vue
        admin.md
        autoquerygrid.md
        components.md
      /react
        locode.md
        bookings.md
        nextjs.md
    ```

That we can render into a more UX-friendly representation by calling the `Files` component with the body
of the code-block to convert the structured ascii layout into a more familiar GUI layout:

```files
/_videos
  /vue
    admin.md
    autoquerygrid.md
    components.md
  /react
    locode.md
    bookings.md
    nextjs.md
```

The benefit of this approach of marking up documentation is that the markdown content still remains in an optimal 
human-readable form even when the markdown renderer lacks the custom fenced components to render the richer UI.

## Components In Markdown

Up till now all above features will let you render the same markdown content in all available Vue, React, Razor or Blazor
templates. At the cost of reduced portability, you're also able to embed rich Interactive Vue or React components directly in 
markdown.

:::include component-links.md:::


# New Blazor Interactive Auto Template with Custom Admin UIs
Source: https://servicestack.net/posts/blazor-8-admin

Since the release of .NET 8, we have been upgrading our [templates](https://github.com/NetCoreTemplates) and example applications to take advantage 
of some of the new features, especially for Blazor.
Our templates now make use of static Server Side Rendering (SSR) for Blazor, which allows for faster initial page loads 
and better SEO, and our `blazor-wasm` template uses `InteractiveAuto` by default to provide a more responsive UI.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="BXjcKkaK-nM" style="background-image: url('https://img.youtube.com/vi/BXjcKkaK-nM/maxresdefault.jpg')"></lite-youtube>
</div>

## What is InteractiveAuto?

Blazor for .NET 8 has [four different rendering modes](https://learn.microsoft.com/en-us/aspnet/core/blazor/components/render-modes?view=aspnetcore-8.0#render-modes) you can take advantage of:

- Static Server (static SSR)
- Interactive Server
- Interactive WebAssembly (WASM)
- Interactive Auto

For non-interactive pages, the static SSR mode is the fastest, as it renders the page on the server and sends the HTML to the client.
However, when your page needs to be interactive, you need to use one of the interactive modes.

Prior to .NET 8, there was a trade-off between the two available render modes (static server rendering wasn't yet available).
The `Interactive Server` mode was faster to load, but the `Interactive WASM` mode was more responsive.

The initial load times for `Interactive WASM` could be quite slow, as the entire application and all its dependencies needed to be downloaded before the page could render most of the content.

<img class="border-gray-800 border-b border-r" src="/img/posts/blazor-8-admin/blazor-wasm-6-slow.gif">

> The initial load time for the `Interactive WASM` mode can be quite slow even for a minimal app

Our templates previously worked around this limitation with a custom Pre-Rendering solution, as the wait times were too long for a good user experience.

With .NET 8, the new `Interactive Auto` mode provides the best of both worlds as pre-rendering is now enabled by default.

<img class="border-gray-800 border-b border-r" src="/img/posts/blazor-8-admin/blazor-wasm-8-fast.gif">

When the page is first loaded, it uses the `Interactive Server` mode, which is faster than `Interactive WASM` as it doesn't need to download WASM resources.
So the user can start interacting with the page straight away, but with a slight delay for each of their interactions due to having to perform round-trips to the server for each interaction.

In the background, the WASM resources are downloaded which can then be used to render the site on the client for subsequent visits.

## Using InteractiveAuto in your Blazor application

In Blazor for .NET 8, render modes can be set on both a per-page and per-component basis.

```html
@page "/counter"
@rendermode InteractiveAuto

<Counter />
```

```html
<Counter @rendermode="RenderMode.InteractiveAuto" />
```

## ServiceStack.Blazor Components

The [ServiceStack.Blazor Components](https://blazor-gallery.jamstacks.net) have been updated for .NET 8 and work with the new `InteractiveAuto` render mode.

This means you can focus more on your application logic and less on the UI, as the components provide a high-productivity UI for common tasks such as CRUD operations.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="JW88KHwt_5I" style="background-image: url('https://img.youtube.com/vi/BXjcKkaK-nM/maxresdefault.jpg')"></lite-youtube>
</div>

### AutoQueryGrid

The [AutoQueryGrid](https://blazor-gallery.servicestack.net/gallery/autoquerygrid) component provides a full-featured data grid that can be used to display and edit data from an AutoQuery service.
This is ideal for creating custom admin pages for your application. 
By integrating your admin screens into your application, you can optimize the user experience for specific workflows and get a huge amount of reuse of your existing AutoQuery services.

```html
<AutoQueryGrid Model="Modifier" Apis="Apis.AutoQuery<QueryModifiers,CreateModifier,UpdateModifier,DeleteModifier>()" />
```

![](/img/posts/blazor-8-admin/autoquerygrid.png)

For BlazorDiffusion, our StableDiffusion example application, we used the AutoQueryGrid to create a custom admin page for managing the modifiers in the application.

![](/img/posts/blazor-8-admin/blazordiffusion-modifiers.png)

This is the simplest and fastest use of the AutoQueryGrid component, but it can also be heavily customized for lots of different use cases.

In BlazorDiffusion we customize the grid to enable easy navigation contextually between separate customized admin screens for each Creative, linking to related table data.

![](/img/posts/blazor-8-admin/blazordiffusion-creatives.png)

```html
<AutoQueryGrid @ref=@grid Model="Creative" Apis="Apis.AutoQuery<QueryCreatives,UpdateCreative,HardDeleteCreative>()"
               ConfigureQuery="ConfigureQuery">
    <EditForm>
        <div class="relative z-10" aria-labelledby="slide-over-title" role="dialog" aria-modal="true">
            <div class="pointer-events-none fixed inset-y-0 right-0 flex max-w-full pl-10 sm:pl-16">
                <CreativeEdit Creative="context" OnClose="grid.OnEditDone" />
            </div>
        </div>
    </EditForm>
    <Columns>
        <Column Title="User" Field="(Creative x) => x.OwnerId" />
        <Column Title="Id" Field="(Creative x) => x.Id" />
        <Column Field="(Creative x) => x.Modifiers">
            <Template>
                @if (context.Modifiers?.Count > 0)
                {
                <TextLink class="flex" href=@($"/admin/modifiers?Ids={string.Join(",", context.Modifiers.Select(x => x.ModifierId))}")>
                    <Icon class="w-6 h-6 mr-1" Image=@typeof(Modifier).GetIcon() />
                    @TextUtils.Pluralize("Modifier", context.Modifiers)
                </TextLink>
                }
            </Template>
        </Column>
        <Column Field="(Creative x) => x.Artists">
            <Template>
                @if (context.Artists?.Count > 0)
                {
                <TextLink class="flex" href=@($"/admin/artists?Ids={string.Join(",", context.Artists.Select(x => x.ArtistId))}")>
                    <Icon class="w-6 h-6 mr-1" Image=@typeof(Artist).GetIcon() />
                    @TextUtils.Pluralize("Artist", context.Artists)
                </TextLink>
                }
            </Template>
        </Column>
        <Column Field="(Creative x) => x.Artifacts">
            <Template>
                @if (context.Artifacts?.Count > 0)
                {
                <TextLink class="flex" href=@($"/admin/artifacts?CreativeId={context.Id}")>
                    <Icon class="w-6 h-6 mr-1" Image=@typeof(Artifact).GetIcon() />
                    @TextUtils.Pluralize("Artifact", context.Artifacts)
                </TextLink>
                }
            </Template>
        </Column>
        <Column Field="(Creative x) => x.Key" />
        <Column Field="(Creative x) => x.CreatedDate" Format="s" />
        <Column Field="(Creative x) => x.UserPrompt" />
    </Columns>
</AutoQueryGrid>
```

In the above example, we use the `ConfigureQuery` parameter to customize the query used by the AutoQueryGrid when displaying values.
This is ideal if you want to filter the data for specific workflows, for example, only showing the data that is relevant to the current user.

We combine this with a `Tabs` component to provide a navigation bar for the user to switch between the different filters on the same AutoQueryGrid.

```html
<Tabs TabOptions="TabOptions" TabChanged="TabChangedAsync" />
```

![](/img/posts/blazor-8-admin/blazordiffusion-tab.png)

![](/img/posts/blazor-8-admin/blazordiffusion-tab1.png)

We also use the `EditForm` parameter to customize the edit form for the AutoQueryGrid, so the workflow for editing a Creative is optimized using your own completely custom UI.

```html
<AutoQueryGrid @ref=@grid Model="Creative" Apis="Apis.AutoQuery<QueryCreatives,UpdateCreative,HardDeleteCreative>()"
               ConfigureQuery="ConfigureQuery">
    <EditForm>
        <div class="relative z-10" aria-labelledby="slide-over-title" role="dialog" aria-modal="true">
            <div class="pointer-events-none fixed inset-y-0 right-0 flex max-w-full pl-10 sm:pl-16">
                <CreativeEdit Creative="context" OnClose="grid.OnEditDone" />
            </div>
        </div>
    </EditForm>
```

## Upgrading to .NET 8

BlazorDiffusion was an example application we originally developed for .NET 6.
We upgraded the production release of this application to use our `blazor-vue` template, which can be perfect for public-facing web applications and teams that don't mind including a JavaScript framework in their application.

However, to show the flexibility of Blazor for .NET 8, we also upgraded the whole application from our updated `blazor-wasm` template to take advantage of the new `InteractiveAuto` mode.

### Component Compatibility

Since the ServiceStack.Blazor library has been updated for .NET 8, we just needed to bring over the shared components from the original application and update the references to the new library.

When upgrading your application pages and components, you will need to avoid any JavaScript interop that runs during the `InitializeAsync` lifecycle method, as this is not supported in the `InteractiveAuto` mode.

### Running on both Server vs Client

When using the `InteractiveAuto` mode, first visits will be running on the server, so your pages and components need to be available to both projects, as well as have any required dependencies registered in both projects `Program.cs` files.

By placing your shared pages and components in a shared project like the `.Client` project in the `blazor-wasm` template, you can easily share them between the two projects.

Look for any of your pages or components that use the `@injects` directive, as these will need to be registered in both projects.

:::info
Avoid sharing sensitive information via dependency injection, as this will be available to the client at runtime which will be able to be decompiled and inspected.
:::

## Source code and live demo

The source code for the upgraded `BlazorDiffusionAuto` application is 
[available on GitHub](https://github.com/NetCoreApps/BlazorDiffusionAuto) and you can view a live demo of the application at 
[auto.blazordiffusion.com](https://auto.blazordiffusion.com).

## Conclusion

The new `InteractiveAuto` mode in Blazor for .NET 8 provides the best of both worlds for Blazor applications.
A built in pre-rendering solution means that you can have a fast initial load time, but still have a responsive UI for subsequent visits.

And since the ServiceStack.Blazor components have been updated for .NET 8, you can take advantage of the high-productivity UI components to quickly create customizable and professional-looking admin pages in a Blazor application.

## Feedback

If you have any questions or feedback, please feel free to reach out to us on [our forums](https://forums.servicestack.net) or [GitHub Discussions](https://servicestack.net/ask).


# ASP.NET Core JWT Identity Auth
Source: https://servicestack.net/posts/jwt-identity-auth

JWTs enable stateless authentication of clients without servers needing to maintain any Auth state in server infrastructure
or perform any I/O to validate a token. As such,
[JWTs are a popular choice for Microservices](https://docs.servicestack.net/auth/jwt-authprovider#stateless-auth-microservices)
as they only need to configured with confidential keys to validate access.

### ASP.NET Core JWT Authentication

ServiceStack's JWT Identity Auth reimplements many of the existing [ServiceStack JWT AuthProvider](https://docs.servicestack.net/auth/jwt-authprovider)
features but instead of its own implementation, integrates with and utilizes ASP.NET Core's built-in JWT Authentication that's
configurable in .NET Apps with the `.AddJwtBearer()` extension method, e.g:

#### Program.cs

```csharp
services.AddAuthentication()
    .AddJwtBearer(options => {
        options.TokenValidationParameters = new()
        {
            ValidIssuer = config["JwtBearer:ValidIssuer"],
            ValidAudience = config["JwtBearer:ValidAudience"],
            IssuerSigningKey = new SymmetricSecurityKey(
                Encoding.UTF8.GetBytes(config["JwtBearer:IssuerSigningKey"]!)),
            ValidateIssuerSigningKey = true,
        };
    })
    .AddIdentityCookies(options => options.DisableRedirectsForApis());
```

Then use the `JwtAuth()` method to enable and configure ServiceStack's support for ASP.NET Core JWT Identity Auth: 

#### Configure.Auth.cs

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services => {
            services.AddPlugin(new AuthFeature(IdentityAuth.For<ApplicationUser>(
                options => {
                    options.SessionFactory = () => new CustomUserSession();
                    options.CredentialsAuth();
                    options.JwtAuth(x => {
                        // Enable JWT Auth Features...
                    });
                })));
        });
}
```

### Enable in Swagger UI

Once configured we can enable JWT Auth in Swagger UI by installing **Swashbuckle.AspNetCore**:

:::copy
`<PackageReference Include="Swashbuckle.AspNetCore" Version="6.*" />`
:::

Then enable Open API, Swagger UI, ServiceStack's support for Swagger UI and the JWT Bearer Auth option:

```csharp
public class ConfigureOpenApi : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            if (context.HostingEnvironment.IsDevelopment())
            {
                services.AddEndpointsApiExplorer();
                services.AddSwaggerGen();
                services.AddServiceStackSwagger();
                services.AddJwtAuth();
                //services.AddBasicAuth<Data.ApplicationUser>();
            
                services.AddTransient<IStartupFilter,StartupFilter>();
            }
        });

    public class StartupFilter : IStartupFilter
    {
        public Action<IApplicationBuilder> Configure(Action<IApplicationBuilder> next)
            => app => {
                // Provided by Swashbuckle library
                app.UseSwagger();
                app.UseSwaggerUI();
                next(app);
            };
    }
}
```

This will enable the **Authorize** button in Swagger UI where you can authenticate with a JWT Token:

![](/img/posts/jwt-identity-auth/jwt-swagger-ui.png)

### JWT Auth in Built-in UIs

This also enables the **JWT** Auth Option in ServiceStack's built-in 
[API Explorer](https://docs.servicestack.net/api-explorer), 
[Locode](https://docs.servicestack.net/locode/) and 
[Admin UIs](https://docs.servicestack.net/admin-ui):

<img class="shadow p-1" src="/img/posts/jwt-identity-auth/jwt-api-explorer.png">

### Authenticating with JWT

JWT Identity Auth is a drop-in replacement for ServiceStack's JWT AuthProvider where Authenticating via Credentials
will convert the Authenticated User into a JWT Bearer Token returned in the **HttpOnly**, **Secure** `ss-tok` Cookie
that will be used to Authenticate the client:

```csharp
var client = new JsonApiClient(BaseUrl);
await client.SendAsync(new Authenticate {
    provider = "credentials",
    UserName = Username,
    Password = Password,
});

var bearerToken = client.GetTokenCookie(); // ss-tok Cookie
```

## JWT Refresh Tokens

Refresh Tokens can be used to allow users to request a new JWT Access Token when the current one expires.

To enable support for JWT Refresh Tokens your `IdentityUser` model should implement the `IRequireRefreshToken` interface
which will be used to store the 64 byte Base64 URL-safe `RefreshToken` and its `RefreshTokenExpiry` in its persisted properties:

```csharp
public class ApplicationUser : IdentityUser, IRequireRefreshToken
{
    public string? RefreshToken { get; set; }
    public DateTime? RefreshTokenExpiry { get; set; }
}
```

Now after successful authentication, the `RefreshToken` will also be returned in the `ss-reftok` Cookie:

```csharp
var refreshToken = client.GetRefreshTokenCookie(); // ss-reftok Cookie
```

### Transparent Server Auto Refresh of JWT Tokens

To be able to terminate a users access, Users need to revalidate their eligibility to verify they're still allowed access 
(e.g. deny Locked out users). This JWT revalidation pattern is implemented using Refresh Tokens which are used to request 
revalidation of their access and reissuing a new JWT Access Token which can be used to make authenticated requests until it expires.

As Cookies are used to return Bearer and Refresh Tokens ServiceStack is able to implement the revalidation logic on the 
server where it transparently validates Refresh Tokens, and if a User is eligible will reissue a new JWT Token Cookie that
replaces the expired Access Token Cookie.

Thanks to this behavior HTTP Clients will be able to Authenticate with just the Refresh Token, which will transparently
reissue a new JWT Access Token Cookie and then continue to perform the Authenticated Request:

```csharp
var client = new JsonApiClient(BaseUrl);
client.SetRefreshTokenCookie(RefreshToken);

var response = await client.SendAsync(new Secured { ... });
```

There's also opt-in sliding support for extending a User's RefreshToken after usage which allows Users to treat 
their Refresh Token like an API Key where it will continue extending whilst they're continuously using it to make API requests, 
otherwise expires if they stop. How long to extend the expiry of Refresh Tokens after usage can be configured with:

```csharp
options.JwtAuth(x => {
    // How long to extend the expiry of Refresh Tokens after usage (default None)
    x.ExtendRefreshTokenExpiryAfterUsage = TimeSpan.FromDays(90);
});
```

## Convert Session to Token Service

Another useful Service that's available is being able to Convert your current Authenticated Session into a Token
with the `ConvertSessionToToken` Service which can be enabled with:

```csharp
options.JwtAuth(x => {
    x.IncludeConvertSessionToTokenService = true;
});
```

This can be useful for when you want to Authenticate via an external OAuth Provider that you then want to convert into a stateless
JWT Token by calling the `ConvertSessionToToken` on the client, e.g:

#### .NET Clients

```csharp
await client.SendAsync(new ConvertSessionToToken());
```

#### TypeScript/JavaScript

```ts
fetch('/session-to-token', { method:'POST', credentials:'include' })
```

The default behavior of `ConvertSessionToToken` is to remove the Current Session from the Auth Server which will prevent 
access to protected Services using our previously Authenticated Session. If you still want to preserve your existing Session 
you can indicate this with:

```csharp
await client.SendAsync(new ConvertSessionToToken { 
    PreserveSession = true 
});
```

### JWT Options

Other configuration options available for Identity JWT Auth include:

```csharp
options.JwtAuth(x => {
    // How long should JWT Tokens be valid for. (default 14 days)
    x.ExpireTokensIn = TimeSpan.FromDays(14);
    
    // How long should JWT Refresh Tokens be valid for. (default 90 days)
    x.ExpireRefreshTokensIn = TimeSpan.FromDays(90);
    
    x.OnTokenCreated = (req, user, claims) => {
        // Customize which claims are included in the JWT Token
    };
    
    // Whether to invalidate Refresh Tokens on Logout (default true)
    x.InvalidateRefreshTokenOnLogout = true;
    
    // How long to extend the expiry of Refresh Tokens after usage (default None)
    x.ExtendRefreshTokenExpiryAfterUsage = null;
});
```


# Built-In Identity Auth Admin UI
Source: https://servicestack.net/posts/identity-auth-admin-ui

With ServiceStack now [deeply integrated into ASP.NET Core Apps](/posts/servicestack-endpoint-routing) we're back to
refocusing on adding value-added features that can benefit all .NET Core Apps.

## Registration

The new Identity Auth Admin UI is an example of this, which can be enabled when registering the `AuthFeature` Plugin:

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services => {
            services.AddPlugin(new AuthFeature(IdentityAuth.For<ApplicationUser>(
                options => {
                    options.SessionFactory = () => new CustomUserSession();
                    options.CredentialsAuth();
                    options.AdminUsersFeature();
                })));
        });
}
```

Which just like the ServiceStack Auth [Admin Users UI](https://docs.servicestack.net/admin-ui-users) enables a
Admin UI that's only accessible to **Admin** Users for managing **Identity Auth** users at `/admin-ui/users`.

## User Search Results

Which displays a limited view due to the minimal properties on the default `IdentityAuth` model:

<div style="height:580px">
    <img class="absolute left-0 right-0 mx-auto shadow" style="max-width:1496px" 
         src="/img/posts/identity-auth-admin-ui/admin-ui-users-default.png">
</div>

### Custom Search Result Properties

These User's search results are customizable by specifying the `ApplicationUser` properties to display instead, e.g:

```csharp
options.AdminUsersFeature(feature =>
{
    feature.QueryIdentityUserProperties =
    [
        nameof(ApplicationUser.Id),
        nameof(ApplicationUser.DisplayName),
        nameof(ApplicationUser.Email),
        nameof(ApplicationUser.UserName),
        nameof(ApplicationUser.LockoutEnd),
    ];
});
```

<div style="height:580px">
    <img class="absolute left-0 right-0 mx-auto shadow" style="max-width:1496px" 
         src="/img/posts/identity-auth-admin-ui/admin-ui-users-custom.png">
</div>

### Custom Search Result Behavior

The default display Order of Users is also customizable:

```csharp
feature.DefaultOrderBy = nameof(ApplicationUser.DisplayName);
```

As well as the Search behavior which can be replaced to search any custom fields, e.g:

```csharp
feature.SearchUsersFilter = (q, query) =>
{
    var queryUpper = query.ToUpper();
    return q.Where(x =>
        x.DisplayName!.Contains(query) ||
        x.Id.Contains(queryUpper) ||
        x.NormalizedUserName!.Contains(queryUpper) ||
        x.NormalizedEmail!.Contains(queryUpper));
};
```

## Default Create and Edit Users Forms

The default Create and Edit Admin Users UI are also limited to editing the minimal `IdentityAuth` properties:

<div style="height:640px">
    <img class="absolute left-0 right-0 mx-auto shadow" style="max-width:1200px" 
         src="/img/posts/identity-auth-admin-ui/admin-ui-users-create.png">
</div>

Whilst the Edit page includes standard features to lockout users, change user passwords and manage their roles:

<div style="height:650px">
    <img class="absolute left-0 right-0 mx-auto shadow" style="max-width:1200px" 
         src="/img/posts/identity-auth-admin-ui/admin-ui-users-edit.png">
</div>

### Custom Create and Edit Forms

By default Users are locked out indefinitely, but this can also be changed to lock users out to a specific date, e.g:

```csharp
feature.ResolveLockoutDate = user => DateTimeOffset.Now.AddDays(7);
```

The forms editable fields can also be customized to include additional properties, e.g:

```csharp
feature.FormLayout =
[
    Input.For<ApplicationUser>(x => x.UserName, c => c.FieldsPerRow(2)),
    Input.For<ApplicationUser>(x => x.Email, c => { 
        c.Type = Input.Types.Email;
        c.FieldsPerRow(2); 
    }),
    Input.For<ApplicationUser>(x => x.FirstName, c => c.FieldsPerRow(2)),
    Input.For<ApplicationUser>(x => x.LastName, c => c.FieldsPerRow(2)),
    Input.For<ApplicationUser>(x => x.DisplayName, c => c.FieldsPerRow(2)),
    Input.For<ApplicationUser>(x => x.PhoneNumber, c =>
    {
        c.Type = Input.Types.Tel;
        c.FieldsPerRow(2); 
    }),
];
```

That can override the new `ApplicationUser` Model that's created and any Validation:

### Custom User Creation

```csharp
feature.CreateUser = () => new ApplicationUser { EmailConfirmed = true };
feature.CreateUserValidation = async (req, createUser) =>
{
    await IdentityAdminUsers.ValidateCreateUserAsync(req, createUser);
    var displayName = createUser.GetUserProperty(nameof(ApplicationUser.DisplayName));
    if (string.IsNullOrEmpty(displayName))
        throw new ArgumentNullException(nameof(AdminUserBase.DisplayName));
    return null;
};
```

<div style="height:640px">
    <img class="absolute left-0 right-0 mx-auto shadow" style="max-width:1200px" 
         src="/img/posts/identity-auth-admin-ui/admin-ui-users-create-custom.png">
</div>

### Admin User Events

Should you need to, Admin User Events can use used to execute custom logic before and after creating, updating and 
deleting users, e.g:

```csharp
feature.OnBeforeCreateUser = (request, user) => { ... };
feature.OnAfterCreateUser  = (request, user) => { ... };
feature.OnBeforeUpdateUser = (request, user) => { ... };
feature.OnAfterUpdateUser  = (request, user) => { ... };
feature.OnBeforeDeleteUser = (request, userId) => { ... };
feature.OnAfterDeleteUser  = (request, userId) => { ... };
```


# System.Text.Json ServiceStack APIs
Source: https://servicestack.net/posts/system-text-json-apis

In continuing our focus to enable ServiceStack to become a deeply integrated part of .NET 8 Application's, ServiceStack
latest .NET 8 templates now default to using standardized ASP.NET Core features wherever possible, including:

- [ASP.NET Core Identity Auth](/posts/net8-identity-auth)
- [ASP.NET Core IOC](/posts/servicestack-endpoint-routing#asp.net-core-ioc)
- [Endpoint Routing](/posts/servicestack-endpoint-routing#endpoint-routing)
- [Swashbuckle for Open API v3 and Swagger UI](/posts/openapi-v3-support)
- [System.Text.Json APIs](/posts/system-text-json-apis)

This reduces friction for integrating ServiceStack into existing .NET 8 Apps, encourages greater knowledge and reuse and
simplifies .NET development as developers have a reduced number of concepts to learn, fewer technology implementations to
configure and maintain that are now applied across their entire .NET App.

The last integration piece supported was utilizing **System.Text.Json** - the default high-performance async JSON serializer
used in .NET Applications, can now be used by ServiceStack APIs to serialize and deserialize its JSON API Responses
that's enabled by default when using **Endpoint Routing**.

This integrates ServiceStack APIs more than ever where just like Minimal APIs and Web API,
uses **ASP.NET Core's IOC** to resolve dependencies, uses **Endpoint Routing** to Execute APIs that's secured with
**ASP.NET Core Identity Auth** then uses **System.Text.Json** to deserialize and serialize its JSON payloads.

### Enabled by Default when using Endpoint Routing

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
});
```

### Enhanced Configuration

ServiceStack uses a custom `JsonSerializerOptions` to improve compatibility with existing ServiceStack DTOs and
ServiceStack's rich ecosystem of generic [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference)
Service Clients, which is configured to:

- Not serialize `null` properties
- Supports Case Insensitive Properties
- Uses `CamelCaseNamingPolicy` for property names
- Serializes `TimeSpan` and `TimeOnly` Data Types with [XML Schema Time format](https://www.w3.org/TR/xmlschema-2/#isoformats)
- Supports `[DataContract]` annotations
- Supports Custom Enum Serialization

### Benefits all Add ServiceStack Reference Languages

This compatibility immediately benefits all of ServiceStack's [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference) 
native typed integrations for **11 programming languages** which all utilize ServiceStack's JSON API endpoints - now serialized with System.Text.Json 

### Support for DataContract Annotations

Support for .NET's `DataContract` serialization attributes was added using a custom `TypeInfoResolver`, specifically it supports:

- `[DataContract]` - When annotated, only `[DataMember]` properties are serialized
- `[DataMember]` - Specify a custom **Name** or **Order** of properties
- `[IgnoreDataMember]` - Ignore properties from serialization
- `[EnumMember]` - Specify a custom value for Enum values

### Custom Enum Serialization

Below is a good demonstration of the custom Enum serialization support which matches ServiceStack.Text's behavior:

```csharp
public enum EnumType { Value1, Value2, Value3 }

[Flags]
public enum EnumTypeFlags { Value1, Value2, Value3 }

public enum EnumStyleMembers
{
    [EnumMember(Value = "lower")]
    Lower,
    [EnumMember(Value = "UPPER")]
    Upper,
}

return new EnumExamples {
    EnumProp = EnumType.Value2, // String value by default
    EnumFlags = EnumTypeFlags.Value2 | EnumTypeFlags.Value3, // [Flags] as int
    EnumStyleMembers = EnumStyleMembers.Upper, // Serializes [EnumMember] value
    NullableEnumProp = null, // Ignores nullable enums
};
```

Which serializes to:

```json
{
  "enumProp": "Value2",
  "enumFlags": 3,
  "enumStyleMembers": "UPPER"
}
```

### Custom Configuration

You can further customize the `JsonSerializerOptions` used by ServiceStack by using `ConfigureJsonOptions()` to add
any customizations that you can optionally apply to ASP.NET Core's JSON APIs and MVC with:

```csharp
builder.Services.ConfigureJsonOptions(options => {
    options.PropertyNamingPolicy = JsonNamingPolicy.SnakeCaseLower;
})
.ApplyToApiJsonOptions()  // Apply to ASP.NET Core's JSON APIs
.ApplyToMvcJsonOptions(); // Apply to MVC
```

### Control over when and where System.Text.Json is used

Whilst `System.Text.Json` is highly efficient, it's also very strict in the inputs it accepts where you may want to
revert back to using ServiceStack's JSON Serializer for specific APIs, especially when you need to support external
clients that can't be updated.

This can done by annotating Request DTOs with `[SystemJson]` attribute, e.g: you can limit to only use `System.Text.Json`
for an **APIs Response** with:

```csharp
[SystemJson(UseSystemJson.Response)]
public class CreateUser : IReturn<IdResponse>
{
    //...
}
```

Or limit to only use `System.Text.Json` for an **APIs Request** with:

```csharp
[SystemJson(UseSystemJson.Request)]
public class CreateUser : IReturn<IdResponse>
{
    //...
}
```

Or not use `System.Text.Json` at all for an API with:

```csharp
[SystemJson(UseSystemJson.Never)]
public class CreateUser : IReturn<IdResponse>
{
    //...
}
```

### JsonApiClient Support

When Endpoints Routing is configured, the `JsonApiClient` will also be configured to utilize the same `System.Text.Json`
options to send and receive its JSON API Requests which also respects the `[SystemJson]` specified behavior.

Clients external to the .NET App can be configured to use `System.Text.Json` with:

```csharp
ClientConfig.UseSystemJson = UseSystemJson.Always;
```

Whilst any custom configuration can be applied to its `JsonSerializerOptions` with:

```csharp
TextConfig.ConfigureSystemJsonOptions(options => {
    options.PropertyNamingPolicy = JsonNamingPolicy.SnakeCaseLower;
});
```

### Scoped JSON Configuration

We've also added partial support for [Customized JSON Responses](https://docs.servicestack.net/customize-json-responses)
for the following customization options:

:::{.table,w-full}
| Name                         | Alias |
|------------------------------|-------|
| EmitCamelCaseNames           | eccn  |
| EmitLowercaseUnderscoreNames | elun  |
| EmitPascalCaseNames          | epcn  |
| ExcludeDefaultValues         | edv   |
| IncludeNullValues            | inv   |
| Indent                       | pp    |
:::

These can be applied to the JSON Response by returning a decorated `HttpResult` with a custom `ResultScope`, e.g:

```csharp
return new HttpResult(responseDto) {
    ResultScope = () => 
        JsConfig.With(new() { IncludeNullValues = true, ExcludeDefaultValues = true })
};
```

They can also be requested by API consumers by adding a `?jsconfig` query string with the desired option or its alias, e.g:

```csharp
/api/MyRequest?jsconfig=EmitLowercaseUnderscoreNames,ExcludeDefaultValues
/api/MyRequest?jsconfig=eccn,edv
```

### SystemJsonCompatible

Another configuration automatically applied when `System.Text.Json` is enabled is:

```csharp
JsConfig.SystemJsonCompatible = true;
```

Which is being used to make ServiceStack's JSON Serializer more compatible with `System.Text.Json` output so it's easier
to switch between the two with minimal effort and incompatibility. Currently this is only used to override
`DateTime` and `DateTimeOffset` behavior which uses `System.Text.Json` for its Serialization/Deserialization.


# OpenAPI v3 and Swagger UI
Source: https://servicestack.net/posts/openapi-v3

In the ServiceStack v8.1 release, we have introduced a way to better incorporate your ServiceStack APIs into the larger 
ASP.NET Core ecosystem by mapping your ServiceStack APIs to standard [ASP.NET Core Endpoints](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/routing?view=aspnetcore-8.0#endpoints). 
This enables your ServiceStack APIs integrate with your larger ASP.NET Core application in the same way other 
middleware does, opening up more opportunities for reuse of your ServiceStack APIs.

This opens up the ability to use common third party tooling. A good example of this is adding OpenAPI v3 specification 
generation for your endpoints offered by the `Swashbuckle.AspNetCore` package.

<lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="zAq9hp7ojn4" style="background-image: url('https://img.youtube.com/vi/zAq9hp7ojn4/maxresdefault.jpg')"></lite-youtube>

Included in the v8.1 Release is the `ServiceStack.AspNetCore.OpenApi` package to make this integration 
as easy as possible, and incorporate additional information from your ServiceStack APIs into Swagger metadata.

![](/img/posts/openapi-v3/openapi-v3-swagger-ui.png)

Previously, without the ability to map Endpoints, we've maintained a ServiceStack specific OpenAPI specification generation 
via the `OpenApiFeature` plugin. While this provided a lot of functionality by accurately describing your ServiceStack APIs, 
it could be tricky to customize those API descriptions to the way some users wanted to.

In this post we will look at how you can take advantage of the new OpenAPI v3 Swagger support using mapped Endpoints, 
customizing the generated specification, as well as touch on other related changes to ServiceStack v8.1.

## AppHost Initialization

To use ServiceStack APIs as mapped Endpoints, the way ServiceStack is initialized in . 
To convert your App to use [Endpoint Routing and ASP.NET Core IOC](/posts/servicestack-endpoint-routing) your ASPNET Core 
application needs to be updated to replace any usage of `Funq` IoC container to use ASP.NET Core's IOC.

Previously, the following was used to initialize your ServiceStack `AppHost`:

#### Program.cs
```csharp
app.UseServiceStack(new AppHost());
```

The `app` in this example is a `WebApplication` resulting from an `IHostApplicationBuilder` calling `builder.Build()`. 

Whilst we still need to call `app.UseServiceStack()`, we also need to move the discovery of your ServiceStack APIs to earlier 
in the setup before the `WebApplication` is built, e.g:

```csharp
// Register ServiceStack APIs, Dependencies and Plugins:
services.AddServiceStack(typeof(MyServices).Assembly);

var app = builder.Build();
//...

// Register ServiceStack AppHost
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
});

app.Run();
```

Once configured to use Endpoint Routing we can the [mix](https://docs.servicestack.net/mix-tool) tool to apply the 
[openapi3](https://gist.github.com/gistlyn/dac47b68e77796902cde0f0b7b9c6ac2) Startup Configuration with:

:::sh
x mix openapi3
:::

### Manually Configure OpenAPI v3 and Swagger UI 

This will install the required ASP.NET Core Microsoft, Swashbuckle and ServiceStack Open API NuGet packages:

```xml
<PackageReference Include="Microsoft.AspNetCore.OpenApi" Version="8.*" />
<PackageReference Include="Swashbuckle.AspNetCore" Version="6.*" />
<PackageReference Include="ServiceStack.AspNetCore.OpenApi" Version="8.*" />
```

Then add the `Configure.OpenApi.cs` [Modular Startup](https://docs.servicestack.net/modular-startup) class to your project:

```csharp
[assembly: HostingStartup(typeof(MyApp.ConfigureOpenApi))]

namespace MyApp;

public class ConfigureOpenApi : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) =>
        {
            if (context.HostingEnvironment.IsDevelopment())
            {
                services.AddEndpointsApiExplorer();
                services.AddSwaggerGen(); // Swashbuckle

                services.AddServiceStackSwagger();
                services.AddBasicAuth<ApplicationUser>(); // Enable HTTP Basic Auth
                //services.AddJwtAuth(); // Enable & Use JWT Auth

                services.AddTransient<IStartupFilter, StartupFilter>();
            }
        });

    public class StartupFilter : IStartupFilter
    {
        public Action<IApplicationBuilder> Configure(Action<IApplicationBuilder> next)
            => app => {
                // Provided by Swashbuckle library
                app.UseSwagger();
                app.UseSwaggerUI();
                next(app);
            };
    }
}
```


All this setup is done for you in ServiceStack's updated [Identity Auth .NET 8 Templates](https://servicestack.net/start), 
but for existing applications, you will need to do 
[convert to use Endpoint Routing](https://docs.servicestack.net/endpoints-migration) to support this new way of running your 
ServiceStack applications.

## More Control

One point of friction with our previous `OpenApiFeature` plugin was the missing customization ability to the OpenAPI spec to somewhat disconnect from the defined ServiceStack service, and related C# Request and Response Data Transfer Objects (DTOs). Since the `OpenApiFeature` plugin used class and property attributes on your Request DTOs, making the *structure* of the OpenAPI schema mapping quite ridged, preventing the ability for certain customizations.

For example, if we have an `UpdateTodo` Request DTO that looks like the following:

```csharp
[Route("/todos/{Id}", "PUT")]
public class UpdateTodo : IPut, IReturn<Todo>
{
    public long Id { get; set; }
    [ValidateNotEmpty]
    public string Text { get; set; }
    public bool IsFinished { get; set; }
}
```

Previously, we would get a default Swagger UI that enabled all the properties as `Paramters` to populate.

![](/img/posts/openapi-v3/openapi-v2-defaults.png)

While this correctly describes the Request DTO structure, sometimes as developers we get requirements for how we want to present our APIs to our users from within the Swagger UI. 

With the updated SwaggerUI, and the use of the `Swashbuckle` library, we get the following UI by default.

![](/img/posts/openapi-v3/openapi-v3-defaults-application-json.png)

These are essentially the same, we have a CRUD Todo API that takes a `UpdateTodo` Request DTO, and returns a `Todo` Response DTO. ServiceStack needs to have uniquely named Request DTOs, so we can't have a `Todo` schema as the Request DTO despite the fact that it is the same structure as our `Todo` model. 
This is a good thing, as it allows us to have a clean API contract, and separation of concerns between our Request DTOs and our models. 
However, it might not be desired to present this to our users, since it can be convenient to think about CRUD services as taking the same resource type as the response.

To achieve this, we use the Swashbuckle library to customize the OpenAPI spec generation. Depending on what you want to customize, you can use the `SchemaFilter` or `OperationFilter` options. In this case, we want to customize the matching operation to reference the `Todo` schema for the Request Body.

First, we create a new class that implements the `IOperationFilter` interface.

```csharp
public class OperationRenameFilter : IOperationFilter
{
    public void Apply(OpenApiOperation operation, OperationFilterContext context)
    {
        if (context.ApiDescription.HttpMethod == "PUT" &&
            context.ApiDescription.RelativePath == "todos/{Id}")
        {
            operation.RequestBody.Content["application/json"].Schema.Reference = 
                new OpenApiReference {
                    Type = ReferenceType.Schema,
                    Id = "Todo"
                };
        }
    }
}
```

The above matches some information about the `UpdateTodo` request we want to customize, and then sets the `Reference` property of the `RequestBody` to the `Todo` schema.
We can then add this to the `AddSwaggerGen` options in the `Program.cs` file.

```csharp
builder.Services.AddSwaggerGen(o =>
{
    o.OperationFilter<OperationRenameFilter>();
});
```

The result is the following Swagger UI.

![](/img/posts/openapi-v3/openapi-v3-customized-application-json.png)

This is just one simple example of how you can customize the OpenAPI spec generation, and `Swashbuckle` has some great documentation on the different ways you can customize the generated spec.
And these customizations impact any of your ASP.NET Core Endpoints, not just your ServiceStack APIs.

## Closing

Now that ServiceStack APIs can be mapped to standard ASP.NET Core Endpoints, it opens up a lot of possibilities for integrating your ServiceStack APIs into the larger ASP.NET Core ecosystem. 
The use of the `Swashbuckle` library via the `ServiceStack.AspNetCore.OpenApi` library is just one example of how you can take advantage of this new functionality.


# ServiceStack Endpoint Routing
Source: https://servicestack.net/posts/servicestack-endpoint-routing

In an effort to reduce friction and improve integration with ASP.NET Core Apps, we've continued the trend from last year
for embracing ASP.NET Core's built-in features and conventions which saw the latest ServiceStack v8 release converting 
all its newest .NET 8 templates to adopt [ASP.NET Core Identity Auth](https://docs.servicestack.net/auth/identity-auth).

This is a departure from building upon our own platform-agnostic abstractions which allowed the same ServiceStack code-base
to run on both .NET Core and .NET Framework. Our focus going forward will be to instead adopt De facto standards and conventions
of the latest .NET platform which also means ServiceStack's new value-added features are only available in the latest **.NET 8+** release.

### ServiceStack Middleware

Whilst ServiceStack integrates into ASP.NET Core Apps as custom middleware into ASP.NET Core's HTTP Request Pipeline,
it invokes its own black-box of functionality from there, implemented using its own suite of overlapping features.

Whilst this allows ServiceStack to have full control over how to implement its features, it's not as integrated as it could be,
with there being limits on what ServiceStack Functionality could be reused within external ASP .NET Core MVC Controllers, Razor Pages, etc.
and inhibited the ability to apply application-wide authorization policies across an Application entire surface area,
using and configuring different JSON Serialization implementations.

### Areas for tighter integration

The major areas we've identified that would benefit from tighter integration with ASP.NET Core include:

 - [Funq IOC Container](https://docs.servicestack.net/ioc)
 - [ServiceStack Routing](https://docs.servicestack.net/routing) and [Request Pipeline](https://docs.servicestack.net/order-of-operations)
 - [ServiceStack.Text JSON Serializer](https://docs.servicestack.net/json-format)

### ServiceStack v8.1 is fully integrated!

We're happy to announce the latest release of ServiceStack v8.1 now supports utilizing the optimal ASP.NET Core's 
standardized features to reimplement all these key areas - fostering seamless integration and greater reuse which
you can learn about below:

- [ASP.NET Core Identity Auth](https://docs.servicestack.net/auth/identity-auth)
- [ASP.NET Core IOC](https://docs.servicestack.net/releases/v8_01#asp.net-core-ioc)
- [Endpoint Routing](https://docs.servicestack.net/releases/v8_01#endpoint-routing)
- [System.Text.Json APIs](https://docs.servicestack.net/releases/v8_01#system.text.json)
- [Open API v3 and Swagger UI](https://docs.servicestack.net/releases/v8_01#openapi-v3)
- [ASP.NET Core Identity Auth Admin UI](https://docs.servicestack.net/releases/v8_01#asp.net-core-identity-auth-admin-ui)
- [JWT Identity Auth](https://docs.servicestack.net/releases/v8_01#jwt-identity-auth)

Better yet, this new behavior is enabled by default in all of ServiceStack's new ASP .NET Identity Auth .NET 8 templates!

### Migrating to ASP.NET Core Endpoints

To assist ServiceStack users in upgrading their existing projects we've created a migration guide walking through 
the steps required to adopt these new defaults:

<lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="RaDHkk4tfdU" style="background-image: url('https://img.youtube.com/vi/RaDHkk4tfdU/maxresdefault.jpg')"></lite-youtube>

### ASP .NET Core IOC

The primary limitation of ServiceStack using its own Funq IOC is that any dependencies registered in Funq are not injected
into Razor Pages, Blazor Components, MVC Controllers, etc. 

That's why our [Modular Startup](https://docs.servicestack.net/modular-startup) configurations recommend utilizing
custom `IHostingStartup` configurations to register application dependencies in ASP .NET Core's IOC where they can be 
injected into both ServiceStack Services and ASP.NET Core's external components, e.g:

```csharp
[assembly: HostingStartup(typeof(MyApp.ConfigureDb))]

namespace MyApp;

public class ConfigureDb : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            services.AddSingleton<IDbConnectionFactory>(new OrmLiteConnectionFactory(
                context.Configuration.GetConnectionString("DefaultConnection"),
                SqliteDialect.Provider));
        });
}
```

But there were fundamental restrictions on what could be registered in ASP .NET Core's IOC as everything needed to be 
registered before AspNetCore's `WebApplication` was built and before ServiceStack's AppHost could be initialized, 
which prohibited being able to register any dependencies created by the AppHost including Services, AutoGen Services, 
Validators and internal functionality like App Settings, Virtual File System and Caching providers, etc.

## Switch to use ASP .NET Core IOC

To enable ServiceStack to switch to using ASP .NET Core's IOC you'll need to move registration of all dependencies and
Services to before the WebApplication is built by calling the `AddServiceStack()` extension method with the Assemblies
where your ServiceStack Services are located, e.g:

```csharp
builder.Services.AddServiceStack(typeof(MyServices).Assembly);

var app = builder.Build();

//...
app.UseServiceStack(new AppHost());
```

Which now registers all ServiceStack dependencies in ASP .NET Core's IOC, including all ServiceStack Services prior to
the AppHost being initialized which no longer needs to specify the Assemblies where ServiceStack Services are created
and no longer needs to use Funq as all dependencies should now be registered in ASP .NET Core's IOC.

### Registering Dependencies and Plugins

Additionally ASP.NET Core's IOC requirement for all dependencies needing to be registered before the WebApplication is 
built means you'll no longer be able to register any dependencies or plugins in ServiceStack's `AppHost.Configure()` method.

```csharp
public class AppHost() : AppHostBase("MyApp"), IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services => {
            // Register IOC Dependencies and ServiceStack Plugins
        });

    public override void Configure()
    {
        // DO NOT REGISTER ANY PLUGINS OR DEPENDENCIES HERE
    }
}
```

Instead anything that needs to register dependencies in ASP.NET Core IOC should now use the `IServiceCollection` extension methods:

 - Use `IServiceCollection.Add*` APIs to register dependencies
 - Use `IServiceCollection.AddPlugin` API to register ServiceStack Plugins
 - Use `IServiceCollection.RegisterService*` APIs to dynamically register ServiceStack Services in external Assemblies

This can be done whenever you have access to `IServiceCollection`, either in `Program.cs`:

```csharp
builder.Services.AddPlugin(new AdminDatabaseFeature());
```

Or in any Modular Startup `IHostingStartup` configuration class, e.g:

```csharp
public class ConfigureDb : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            services.AddSingleton<IDbConnectionFactory>(new OrmLiteConnectionFactory(
                context.Configuration.GetConnectionString("DefaultConnection"),
                SqliteDialect.Provider));
            
            // Enable Audit History
            services.AddSingleton<ICrudEvents>(c =>
                new OrmLiteCrudEvents(c.GetRequiredService<IDbConnectionFactory>()));
            
            // Enable AutoQuery RDBMS APIs
            services.AddPlugin(new AutoQueryFeature {
                 MaxLimit = 1000,
            });

            // Enable AutoQuery Data APIs
            services.AddPlugin(new AutoQueryDataFeature());
            
            // Enable built-in Database Admin UI at /admin-ui/database
            services.AddPlugin(new AdminDatabaseFeature());
        })
        .ConfigureAppHost(appHost => {
            appHost.Resolve<ICrudEvents>().InitSchema();
        });
}
```

The `ConfigureAppHost()` extension method can continue to be used to execute any startup logic that requires access to 
registered dependencies.

### Authoring ServiceStack Plugins

To enable ServiceStack Plugins to support both Funq and ASP .NET Core IOC, any dependencies and Services a plugin needs
should be registered in the `IConfigureServices.Configure(IServiceCollection)` method as seen in the refactored
[ServerEventsFeature.cs](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack/src/ServiceStack/ServerEventsFeature.cs)
plugin, e.g:

```csharp
public class ServerEventsFeature : IPlugin, IConfigureServices
{
    //...
    public void Configure(IServiceCollection services)
    {
        if (!services.Exists<IServerEvents>())
        {
            services.AddSingleton<IServerEvents>(new MemoryServerEvents
            {
                IdleTimeout = IdleTimeout,
                HouseKeepingInterval = HouseKeepingInterval,
                OnSubscribeAsync = OnSubscribeAsync,
                OnUnsubscribeAsync = OnUnsubscribeAsync,
                OnUpdateAsync = OnUpdateAsync,
                NotifyChannelOfSubscriptions = NotifyChannelOfSubscriptions,
                Serialize = Serialize,
                OnError = OnError,
            });
        }
        
        if (UnRegisterPath != null)
            services.RegisterService<ServerEventsUnRegisterService>(UnRegisterPath);

        if (SubscribersPath != null)
            services.RegisterService<ServerEventsSubscribersService>(SubscribersPath);
    }

    public void Register(IAppHost appHost)
    {
        //...
    }
}
```

#### All Plugins refactored to support ASP .NET Core IOC

All of ServiceStack's Plugins have been refactored to make use of `IConfigureServices` which supports registering in both 
Funq and ASP.NET Core's IOC when enabled.  

#### Funq IOC implements IServiceCollection and IServiceProvider interfaces

To enable this Funq now implements both `IServiceCollection` and`IServiceProvider` interfaces to enable 100% source-code 
compatibility for registering and resolving dependencies with either IOC, which we now recommend using over Funq's
native Registration and Resolution APIs to simplify migration efforts to ASP.NET Core's IOC in future.

## Dependency Injection

The primary difference between the IOC's is that ASP.NET Core's IOC does not support property injection by default, 
which will require you to refactor your ServiceStack Services to use constructor injection of dependencies, although
this has become a lot more pleasant with C# 12's [Primary Constructors](https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/tutorials/primary-constructors)
which now requires a lot less boilerplate to define, assign and access dependencies, e.g:

```csharp
public class TechStackServices(IAutoQueryDb autoQuery) : Service
{
    public async Task<object> Any(QueryTechStacks request)
    {
        using var db = autoQuery.GetDb(request, base.Request);
        var q = autoQuery.CreateQuery(request, Request, db);
        return await autoQuery.ExecuteAsync(request, q, db);
    }
}
```

This has become our preferred approach for injecting dependencies in ServiceStack Services which have all been refactored
to use constructor injection utilizing primary constructors in order to support both IOC's.

To make migrations easier we've also added support for property injection convention in **ServiceStack Services** using 
ASP.NET Core's IOC where you can add the `[FromServices]` attribute to any public properties you want to be injected, e.g:

```csharp
public class TechStackServices : Service
{
    [FromServices]
    public required IAutoQueryDb AutoQuery { get; set; }

    [FromServices]
    public MyDependency? OptionalDependency { get; set; }
}
```

This feature can be useful for Services wanting to access optional dependencies that may or may not be registered. 

:::info NOTE
`[FromServices]` is only supported in ServiceStack Services (i.e. not other dependencies)
:::

### Built-in ServiceStack Dependencies

This integration now makes it effortless to inject and utilize optional ServiceStack features like
[AutoQuery](https://docs.servicestack.net/autoquery/) and [Server Events](https://docs.servicestack.net/server-events)
in other parts of ASP.NET Core inc. Blazor Components, Razor Pages, MVC Controllers, Minimal APIs, etc.

Whilst the Built-in ServiceStack features that are registered by default and immediately available to be injected, include:
 - `IVirtualFiles` - Read/Write [Virtual File System](https://docs.servicestack.net/virtual-file-system), defaults to `FileSystemVirtualFiles` at `ContentRootPath`
 - `IVirtualPathProvider` - Multi Virtual File System configured to scan multiple read only sources, inc `WebRootPath`, In Memory and Embedded Resource files  
 - `ICacheClient` and `ICacheClientAsync` - In Memory Cache, or distributed Redis cache if [ServiceStack.Redis](https://docs.servicestack.net/redis/) is configured
 - `IAppSettings` - Multiple [AppSettings](https://docs.servicestack.net/appsettings) configuration sources

With ASP.NET Core's IOC now deeply integrated we moved onto the next area of integration: API Integration and Endpoint Routing.

## Endpoint Routing

Whilst ASP.NET Core's middleware is a flexible way to compose and execute different middleware in a HTTP Request pipeline,
each middleware is effectively their own island of functionality that's able to handle HTTP Requests in which ever way
they see fit.

In particular ServiceStack's middleware would execute its own [Request Pipeline](https://docs.servicestack.net/order-of-operations) 
which would execute ServiceStack API's registered at user-defined routes with its own [ServiceStack Routing](https://docs.servicestack.net/routing).

We're happy to announce that ServiceStack **.NET 8** Apps support an entirely new and integrated way to run all of ServiceStack 
requests including all APIs, metadata and built-in UIs with support for 
[ASP.NET Core Endpoint Routing](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/routing) -
enabled by calling the `MapEndpoints()` extension method when configuring ServiceStack, e.g:

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints();
});
```

Which configures ServiceStack APIs to be registered and executed along-side Minimal APIs, Razor Pages, SignalR, MVC 
and Web API Controllers, etc, utilizing the same routing, metadata and execution pipeline.

#### View ServiceStack APIs along-side ASP.NET Core APIs

Amongst other benefits, this integration is evident in endpoint metadata explorers like the `Swashbuckle` library 
which can now show ServiceStack APIs in its Swagger UI along-side other ASP.NET Core APIs in ServiceStack's new
[Open API v3](/posts/openapi-v3) support.

### Routing

Using Endpoint Routing also means using ASP.NET Core's Routing System which now lets you use ASP.NET Core's
[Route constraints](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/routing#route-constraints)
for defining user-defined routes for your ServiceStack APIs, e.g:

```csharp
[Route("/users/{Id:int}")]
[Route("/users/{UserName:string}")]
public class GetUser : IGet, IReturn<User>
{
    public int? Id { get; set; }
    public int? UserName { get; set; }
}
```

For the most part ServiceStack Routing implements a subset of ASP.NET Core's Routing features so your existing user-defined 
routes should continue to work as expected. 

### Wildcard Routes

The only incompatibility we found was when using wildcard paths which in ServiceStack Routing would use an '*' suffix, e.g:
`[Route("/wildcard/{Path*}")]` which will need to change to use a ASP.NET Core's Routing prefix, e.g: 

```csharp
[Route("/wildcard/{*Path}")]
[Route("/wildcard/{**Path}")]
public class GetFile : IGet, IReturn<byte[]>
{
    public string Path { get; set; }
}
```

#### ServiceStack Routing Compatibility

To improve compatibility with ASP.NET Core's Routing, ServiceStack's Routing (when not using Endpoint Routing) now 
supports parsing ASP.NET Core's Route Constraints but as they're inert you would need to continue to use
[Custom Route Rules](https://docs.servicestack.net/routing#custom-rules) to distinguish between different routes matching 
the same path at different specificity:

```csharp
[Route("/users/{Id:int}", Matches = "**/{int}")]
[Route("/users/{UserName:string}")]
public class GetUser : IGet, IReturn<User>
{
    public int? Id { get; set; }
    public int? UserName { get; set; }
}
```

It also supports defining Wildcard Routes using ASP.NET Core's syntax which we now recommend using instead 
for compatibility when switching to use Endpoint Routing:

```csharp
[Route("/wildcard/{*Path}")]
[Route("/wildcard/{**Path}")]
public class GetFile : IGet, IReturn<byte[]>
{
    public string Path { get; set; }
}
```

### Primary HTTP Method

Another difference is that an API will only register its Endpoint Route for its [primary HTTP Method](https://docs.servicestack.net/api-design#all-apis-have-a-preferred-default-method),
if you want an API to be registered for multiple HTTP Methods you can specify them in the `Route` attribute, e.g:

```csharp
[Route("/users/{Id:int}", "GET,POST")]
public class GetUser : IGet, IReturn<User>
{
    public required int Id { get; set; }
}
```

As such we recommend using the IVerb `IGet`, `IPost`, `IPut`, `IPatch`, `IDelete` interface markers to specify the primary HTTP Method
for an API. This isn't needed for [AutoQuery Services](https://docs.servicestack.net/autoquery/) which are implicitly configured
to use their optimal HTTP Method.

If no HTTP Method is specified, the Primary HTTP Method defaults to HTTP **POST**.

### Authorization

Using Endpoint Routing also means ServiceStack's APIs are authorized the same way, where ServiceStack's 
[Declarative Validation attributes](https://docs.servicestack.net/auth/#declarative-validation-attributes) are converted
into ASP.NET Core's `[Authorize]` attribute to secure the endpoint:

```csharp
[ValidateIsAuthenticated]
[ValidateIsAdmin]
[ValidateHasRole(role)]
[ValidateHasClaim(type,value)]
[ValidateHasScope(scope)]
public class Secured {}
```

#### Authorize Attribute on ServiceStack APIs

Alternatively you can now use ASP.NET Core's `[Authorize]` attribute directly to secure ServiceStack APIs should
you need more fine-grained Authorization:

```csharp
[Authorize(Roles = "RequiredRole")]
[Authorize(Policy = "RequiredPolicy")]
[Authorize(AuthenticationSchemes = "Identity.Application,Bearer")]
public class Secured {}
```

#### Configuring Authentication Schemes

ServiceStack will default to using the major Authentication Schemes configured for your App to secure the APIs endpoint with, 
this can be overridden to specify which Authentication Schemes to use to restrict ServiceStack APIs by default, e.g:

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.AuthenticationSchemes = "Identity.Application,Bearer";
    options.MapEndpoints();
});
```

### Hidden ServiceStack Endpoints

Whilst ServiceStack Requests are registered and executed as endpoints, most of them are marked with
`builder.ExcludeFromDescription()` to hide them from polluting metadata and API Explorers like Swagger UI and 
[API Explorer](https://docs.servicestack.net/api-explorer).

To also hide your ServiceStack APIs you can use `[ExcludeMetadata]` attribute to hide them from all metadata services
or use `[Exclude(Feature.ApiExplorer)]` to just hide them from API Explorer UIs:

```csharp
[ExcludeMetadata]
[Exclude(Feature.ApiExplorer)]
public class HiddenRequest {}
```

### Content Negotiation

An example of these hidden routes is the support for invoking and returning ServiceStack APIs in different Content Types
via hidden Endpoint Routes mapped with the format `/api/{Request}.{format}`, e.g:

- [/api/QueryBookings](https://blazor-vue.web-templates.io/api/QueryBookings)
- [/api/QueryBookings.jsonl](https://blazor-vue.web-templates.io/api/QueryBookings.jsonl)
- [/api/QueryBookings.csv](https://blazor-vue.web-templates.io/api/QueryBookings.csv)
- [/api/QueryBookings.xml](https://blazor-vue.web-templates.io/api/QueryBookings.xml)
- [/api/QueryBookings.html](https://blazor-vue.web-templates.io/api/QueryBookings.html)

#### Query String Format

That continues to support specifying the Mime Type via the `?format` query string, e.g:
 
- [/api/QueryBookings?format=jsonl](https://blazor-vue.web-templates.io/api/QueryBookings?format=jsonl)
- [/api/QueryBookings?format=csv](https://blazor-vue.web-templates.io/api/QueryBookings?format=csv)

### Predefined Routes

Endpoints are only created for the newer `/api/{Request}` [pre-defined routes](https://docs.servicestack.net/routing#pre-defined-routes),
which should be easier to use with less conflicts now that ServiceStack APIs are executed along-side other endpoint routes 
APIs which can share the same `/api` base path with non-conflicting routes, e.g: `app.MapGet("/api/minimal-api")`.

As a result clients configured to use the older `/json/reply/{Request}` pre-defined route will need to be configured
to use the newer `/api` base path.

No change is required for C#/.NET clients using the recommended `JsonApiClient` JSON Service Client which is already
configured to use the newer `/api` base path.

```csharp
var client = new JsonApiClient(baseUri);
```

Older .NET clients can be configured to use the newer `/api` pre-defined routes with:

```csharp
var client = new JsonServiceClient(baseUri) {
    UseBasePath = "/api"
};
var client = new JsonHttpClient(baseUri) {
    UseBasePath = "/api"
};
```

To further solidify that `/api` as the preferred pre-defined route we've also **updated all generic service clients** of
other languages to use `/api` base path by default:

#### JavaScript/TypeScript

```ts
const client = new JsonServiceClient(baseUrl)
```

#### Dart

```dart
var client = ClientFactory.api(baseUrl);
```

#### Java/Kotlin

```java
JsonServiceClient client = new JsonServiceClient(baseUrl);
```

#### Python

```python
client = JsonServiceClient(baseUrl)
```

#### PHP

```php
$client = new JsonServiceClient(baseUrl);
```

### Revert to Legacy Predefined Routes

You can unset the base path to revert back to using the older `/json/reply/{Request}` pre-defined route, e.g:

#### JavaScript/TypeScript

```ts
client.basePath = null;
```

#### Dart

```dart
var client = ClientFactory.create(baseUrl);
```

#### Java/Kotlin

```java
client.setBasePath();
```

#### Python

```python
client.set_base_path()
```

#### PHP

```php
$client->setBasePath();
```

### Customize Endpoint Mapping

You can register a RouteHandlerBuilders to customize how ServiceStack APIs endpoints are registered which is also
what ServiceStack uses to annotate its API endpoints to enable its new [Open API v3](/posts/openapi-v3) support:

```csharp
options.RouteHandlerBuilders.Add((builder, operation, method, route) =>
{
    builder.WithOpenApi(op => { ... });
});
```

### Endpoint Routing Compatibility Levels

The default behavior of `MapEndpoints()` is the strictest and recommended configuration that we want future ServiceStack Apps to use,
however if you're migrating existing App's you may want to relax these defaults to improve compatibility with existing behavior.

The configurable defaults for mapping endpoints are:

```csharp
app.UseServiceStack(new AppHost(), options => {
    options.MapEndpoints(use:true, force:true, useSystemJson:UseSystemJson.Always);
});
```

- `use` - Whether to use registered endpoints for executing ServiceStack APIs
- `force` - Whether to only allow APIs to be executed through endpoints
- `useSystemJson` - Whether to use System.Text.Json for JSON API Serialization

So you could for instance register endpoints and not `use` them, where they'll be visible in endpoint API explorers like
[Swagger UI](https://docs.servicestack.net/releases/v8_01#openapi-v3) but continue to execute in ServiceStack's Request Pipeline.

`force` disables fallback execution of ServiceStack Requests through ServiceStack's Request Pipeline for requests that
don't match registered endpoints. You may need to disable this if you have clients calling ServiceStack APIs through
multiple HTTP Methods, as only the primary HTTP Method is registered as an endpoint.

When enabled `force` ensures the only ServiceStack Requests that are not executed through registered endpoints are
`IAppHost.CatchAllHandlers` and `IAppHost.FallbackHandler` handlers.

`useSystemJson` is a new feature that lets you specify when to use `System.Text.Json` for JSON API Serialization, which
is our next exciting feature to standardize on using  
[ASP.NET Core's fast async System.Text.Json](https://docs.servicestack.net/releases/v8_01#system.text.json) Serializer.

## Endpoint Routing Everywhere

Whilst the compatibility levels of Endpoint Routing can be relaxed, we recommend new projects use the strictest and most
integrated defaults that's now configured on all [ASP.NET Core Identity Auth .NET 8 Projects](/start).

For additional testing we've also upgraded many of our existing .NET Example Applications, which are now all running with 
our latest recommended Endpoint Routing configuration:

 - [BlazorDiffusionVue](https://github.com/NetCoreApps/BlazorDiffusionVue)
 - [BlazorDiffusionAuto](https://github.com/NetCoreApps/BlazorDiffusionAuto)
 - [TypeChatExamples](https://github.com/NetCoreApps/TypeChatExamples)
 - [TalentBlazor](https://github.com/NetCoreApps/TalentBlazor)
 - [TechStacks](https://github.com/NetCoreApps/TechStacks)
 - [Validation](https://github.com/NetCoreApps/Validation)
 - [NorthwindAuto](https://github.com/NetCoreApps/NorthwindAuto)
 - [FileBlazor](https://github.com/NetCoreApps/FileBlazor)
 - [Chinook](https://github.com/NetCoreApps/Chinook)
 - [Chat](https://github.com/NetCoreApps/Chat)


# Migrating to ASP.NET Core Identity for Authentication
Source: https://servicestack.net/posts/identity-migration

## ASP.NET Core Identity

Since the release of ServiceStack v8 we have started to include the use of [ASP.NET Core Identity for authentication](https://learn.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-8.0&tabs=visual-studio) in [our templates](https://github.com/NetCoreTemplates). This gives developers the option to use the built-in ASP.NET Core Identity authentication system or ServiceStack's own authentication system when building their next system.

This provides a closer alignment with the ASP.NET Core ecosystem and allows developers to use the built-in ASP.NET Core Identity authentication system if they are already familiar with it.

If you are already using ServiceStack's authentication system, you can continue to do so, but if you are looking to migrate to ASP.NET Core Identity, this guide will walk you through the process with a concrete example of migrating our [BlazorDiffusion](https://github.com/NetCoreApps/BlazorDiffusion) example application.

## Overview of the migration process

The migration process can be broken down into the following steps:

- Add NuGet dependencies
- Create ASP.NET Core Identity `AspNetUsers` class based on your existing custom `UserAuth` class
- Create ASP.NET Core Identity `AspNetRoles`, ensure matching primary key type to `AspNetUsers`
- Create ASP.NET Core Identity `ApplicationDbContext` class, again matching primary key type to `AspNetUsers`
- Create EntityFrameworkCore migration to initialize ASP.NET Core Identity tables
- Update `AuthFeature` registration to use ASP.NET Core Identity
- Update `Program.cs` to use ASP.NET Core Identity
- Implement the Migrate Users Task
- Migrating Roles
- Migrate Foreign Keys from UserAuth to AspNetUsers

In this guide we will walk through each of these steps in detail and show how we migrated our BlazorDiffusion example application over to ASP.NET Core Identity to help you with your own migration.

### Add ASP.NET Core Identity EntityFrameworkCore NuGet package

The first step is to add the required ASP.NET Core Identity NuGet packages to your project. This can be done using the dotnet CLI or via Visual Studio's NuGet package manager.

:::shell
dotnet add package Microsoft.AspNetCore.Identity.EntityFrameworkCore
dotnet add package Microsoft.EntityFrameworkCore.Tools
:::

Since BlazorDiffusion was an existing Blazor project, [we created a new `blazor-wasm` project](https://github.com/NetCoreTemplates/blazor-wasm) using `x new blazor-wasm BlazorDiffusion` and migrated the Services and Components over to the new project.
We can do this because the `blazor-wasm` template and others have been [updated to use ASP.NET Core Identity by default](https://docs.servicestack.net/auth/identity-auth). So if your project previously used a ServiceStack template, first check if there is an updated version of the template available with ASP.NET Core Identity support.

### Create ASP.NET Core Identity `AspNetUsers` class

Next you will need to create a class that inherits from `IdentityUser` to represent our users. [This class will be used by ASP.NET Core Identity to store user information in the database](https://learn.microsoft.com/en-us/aspnet/core/security/authentication/customize-identity-model?view=aspnetcore-8.0).
You will want to mirror customizations from your own `UserAuth` class to this new class which will have the name in the database of `AspNetUsers`.

To minimize changes, you can rename your existing `AppUser` class to something like `OldUserAuth` and then create a new `AppUser` class that inherits from `IdentityUser` and copy over any customizations from `OldUserAuth`.

:::info
In this case `AppUser` is the name of our custom `UserAuth` class.
:::

You will still need to reference your `OldAppUser` class for migrating users, so you will want to point it to the `AppUser` table by using the `[Alias("AppUser")]` attribute.

```csharp
    [Alias("AppUser")]
    public class OldAppUser
    {
        [AutoIncrement]
        public int Id { get; set; }
        public string UserName { get; set; }
        public string DisplayName { get; set; }
        public string FirstName { get; set; }
        public string LastName { get; set; }
        public string? Handle { get; set; }
        public string Email { get; set; }
        public string PasswordHash { get; set; }
        public string? ProfileUrl { get; set; }
        public string? Avatar { get; set; } //overrides ProfileUrl
        public string? LastLoginIp { get; set; }
        public DateTime? LastLoginDate { get; set; }
        public string RefIdStr { get; set; }
        public DateTime? LockedDate { get; set; }
        public DateTime CreatedDate { get; set; }
        public DateTime ModifiedDate { get; set; }
    }
```

When creating your new `AppUser` class, you will want to copy over any customizations from your `OldAppUser` class. In this case we have added a `Handle` property to our `OldAppUser` class, so this will need to be included in the new `AppUser` class as well.

Essentially your custom EF IdentityUser will want a copy of all the properties you want to migrate other than Id, Email, and PasswordHash that's already defined in the base IdentityUser class.

```csharp
// Add profile data for application users by adding properties to the AppUser class
[Alias("AspNetUsers")]
public class AppUser : IdentityUser<int>
{
    public string? FirstName { get; set; }
    public string? LastName { get; set; }
    public string? DisplayName { get; set; }
    public string? ProfileUrl { get; set; }
    [Input(Type = "file"), UploadTo("avatars")]
    public string? Avatar { get; set; } //overrides ProfileUrl
    public string? Handle { get; set; }
    public int? RefId { get; set; }
    public string RefIdStr { get; set; } = Guid.NewGuid().ToString();
    public bool IsArchived { get; set; }
    public DateTime? ArchivedDate { get; set; }
    public string? LastLoginIp { get; set; }
    public DateTime? LastLoginDate { get; set; }
    public DateTime CreatedDate { get; set; } = DateTime.UtcNow;
    public DateTime ModifiedDate { get; set; } = DateTime.UtcNow;
}
```

### Create ASP.NET Core Identity `AspNetRoles`

Next you will need to create a class that inherits from `IdentityRole` to represent your user roles. This class will be used by ASP.NET Core Identity to store role information in the database.

```csharp
[Alias("AspNetRoles")]
public class AppRole : IdentityRole<int>
{
    public AppRole() {}
    public AppRole(string roleName) : base(roleName) {}
}
```

Again, because our `AppUser` class is using a different primary key type than the default `string` type, you will need to specify a matching primary key type for your `AppRole` class.

### Create ASP.NET Core Identity `ApplicationDbContext` class

Now to use our `AppUser` and `AppRole` classes, you will need to create a class that inherits from `IdentityDbContext` to represent our database context. Just like with any EntityFrameworkCore database context, this class will be used to query and save data to the database.

```csharp
public class ApplicationDbContext(DbContextOptions<ApplicationDbContext> options) 
    : IdentityDbContext<AppUser, AppRole, int>(options)
{
    protected override void OnModelCreating(ModelBuilder builder)
    {
        base.OnModelCreating(builder);
        builder.Entity<AppUser>()
            .HasIndex(x => x.Handle)
            .IsUnique();
    }
}
```

Above uses the `Handle` property on the `AppUser` class to create a unique index on the `Handle` column in the `AspNetUsers` table. You can add other custom restrictions to your schema here as well as needed.

### Create EntityFrameworkCore migration to initialize ASP.NET Core Identity tables

Now that you have your `AppUser` and `AppRole` classes, and can access them via your newly created `ApplicationDbContext` class, you can create an EntityFrameworkCore migration to initialize the ASP.NET Core Identity tables.

[You can generate your initial migration using the dotnet CLI or via Visual Studio's Package Manager Console](https://learn.microsoft.com/en-us/ef/core/cli/dotnet).

:::shell
dotnet ef migrations add CreateIdentitySchema
:::

You should run this command from the AppHost project directory, which in our case is `BlazorDiffusion`. This will generate your new EntityFrameworkCore migration in the `Migrations` directory of your AppHost project, along side your ServiceStack migrations.

With your new migration created, you can now update your database schema to include the ASP.NET Core Identity tables.

:::shell
dotnet ef database update
:::

Using the [dotnet EntityFramework CLI is great for local development](https://learn.microsoft.com/en-us/ef/core/cli/dotnet), but for production deployments you will need to run the migrations on your server.
You can do this using ServiceStack's AppTasks feature prior to the standard ServiceStack migrations.

```csharp
public class ConfigureDbMigrations : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => {
            var migrator = new Migrator(appHost.Resolve<IDbConnectionFactory>(), typeof(Migration1000).Assembly);
            AppTasks.Register("migrate", _ =>
            {
                var log = appHost.GetApplicationServices().GetRequiredService<ILogger<ConfigureDbMigrations>>();

                log.LogInformation("Running EF Migrations...");
                var scopeFactory = appHost.GetApplicationServices().GetRequiredService<IServiceScopeFactory>();
                using (var scope = scopeFactory.CreateScope())
                {
                    using var dbContext = scope.ServiceProvider.GetRequiredService<ApplicationDbContext>();
                    dbContext.Database.EnsureCreated();
                    dbContext.Database.Migrate();
                }
            });
```

In the above example we are ensuring the database is created which creates the required schema, and then running the migrations to update the schema to the latest version.

### Update `AuthFeature` registration to use ASP.NET Core Identity

With your ASP.NET Core Identity tables created, you can now update your [`AuthFeature` registration](https://docs.servicestack.net/auth/authentication-and-authorization) to use ASP.NET Core Identity.

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => {
            appHost.Plugins.Add(new AuthFeature(IdentityAuth.For<AppUser,int>(options => {
                options.EnableCredentialsAuth = true;
                options.SessionFactory = () => new CustomUserSession();
            })));
        });
}
```

In the above example we are using the `IdentityAuth` class to register ASP.NET Core Identity with ServiceStack. This class is a wrapper around the standard ASP.NET Core Identity registration process and allows you to configure ASP.NET Core Identity options.

[ServiceStack uses a compatible Identity v2 password hashing format](https://docs.servicestack.net/auth/migrate-to-identity-auth), which should let you migrate your users to Identity Auth without the need to reset their passwords.

### Update `Program.cs` to use ASP.NET Core Identity

Now you will need to configure IdentityCore middleware in your `Program.cs` file.

```csharp
services.AddAuthentication(options =>
    {
        options.DefaultScheme = IdentityConstants.ApplicationScheme;
        options.DefaultSignInScheme = IdentityConstants.ExternalScheme;
    })
    .AddIdentityCookies();
services.AddDataProtection()
    .PersistKeysToFileSystem(new DirectoryInfo("App_Data"));

// $ dotnet ef migrations add CreateIdentitySchema
// $ dotnet ef database update
var connectionString = config.GetConnectionString("DefaultConnection") ?? throw new InvalidOperationException("Connection string 'DefaultConnection' not found.");
services.AddDbContext<ApplicationDbContext>(options =>
    options.UseSqlite(connectionString, b => b.MigrationsAssembly(nameof(BlazorDiffusion))));
services.AddDatabaseDeveloperPageExceptionFilter();

services.AddIdentityCore<AppUser>(options => options.SignIn.RequireConfirmedAccount = true)
    .AddRoles<AppRole>()
    .AddEntityFrameworkStores<ApplicationDbContext>()
    .AddSignInManager()
    .AddDefaultTokenProviders();
```

Since BlazorDiffusion is a .NET 8 Blazor WASM application, we also needed some additional dependencies setup.

```csharp
services.AddCascadingAuthenticationState();
services.AddScoped<IdentityUserAccessor>();
services.AddScoped<IdentityRedirectManager>();
services.AddScoped<AuthenticationStateProvider, PersistingRevalidatingAuthenticationStateProvider>();
```

If you are migrating to Identity from an existing Blazor application, our templates have [tailwind-css styled login and register pages that you can use to get started](https://github.com/NetCoreTemplates/blazor-wasm/tree/main/MyApp/Components/Account).

For these, you will also need the additional IdentityEndpoints mapped.

```csharp
// Add additional endpoints required by the Identity /Account Razor components.
app.MapAdditionalIdentityEndpoints();
```

### Implement the Migrate Users Task

So far we have prepared the application to use ASP.NET Core Identity, but we still need to migrate our existing users to the new ASP.NET Core Identity tables.

This will require:

- Migrating users from the `AppUser` table to the `AspNetUsers` table
- Migrating custom roles table to the `AspNetRoles` table
- Migrating foreign keys from the `UserAuth` table to the `AspNetUsers` table
- Migrating foreign keys from the `UserAuthRole` table to the `AspNetUserRoles` table (if any)

To do this we will create a new AppTask that will migrate our users to the new ASP.NET Core Identity tables.

```csharp
AppTasks.Register("migrate.users", _ => {
    var log = appHost.GetApplicationServices().GetRequiredService<ILogger<ConfigureDbMigrations>>();

    log.LogInformation("Running migrate.users...");
    var scopeFactory = appHost.GetApplicationServices().GetRequiredService<IServiceScopeFactory>();
    using var scope = scopeFactory.CreateScope();
    using var dbContext = scope.ServiceProvider.GetRequiredService<ApplicationDbContext>();
    using var db = scope.ServiceProvider.GetRequiredService<IDbConnectionFactory>().Open();
    var migrateUsers = db.Select(db.From<OldAppUser>().OrderBy(x => x.Id));

    log.LogInformation("Migrating {Count} Existing ServiceStack Users to Identity Auth Users...", migrateUsers.Count);
    MigrateExistingUsers(dbContext, scope.ServiceProvider, migrateUsers).Wait();
});
```

In the above example we are using the `IDbConnectionFactory` to open a connection to our database and select all of our existing users from the `AppUser` table.

The `MigrationExistingUsers` method will then migrate our existing users to the new ASP.NET Core Identity tables.

```csharp
private async Task MigrateExistingUsers(ApplicationDbContext dbContext, IServiceProvider services, 
    List<OldAppUser> migrateUsers, string tempPassword="p@55wOrd")
{
    var userManager = services.GetRequiredService<UserManager<AppUser>>();
    var now = DateTime.UtcNow;

    foreach (var user in migrateUsers)
    {
        var appUser = new AppUser
        {
            Id = user.Id,
            UserName = user.Email,
            Email = user.Email,
            DisplayName = user.DisplayName,
            FirstName = user.FirstName,
            LastName = user.LastName,
            Handle = user.Handle,
            ProfileUrl = user.ProfileUrl,
            Avatar = user.Avatar,
            RefIdStr = user.RefIdStr ?? Guid.NewGuid().ToString(),
            LockoutEnabled = true,
            LockoutEnd = user.LockedDate != null ? now.AddYears(10) : now,
            LastLoginDate = user.LastLoginDate,
            LastLoginIp = user.LastLoginIp,
            CreatedDate = user.CreatedDate,
            ModifiedDate = user.ModifiedDate,
            EmailConfirmed = true,
        };
        await userManager.CreateAsync(appUser, tempPassword);
        if (user.PasswordHash != null)
        {
            // Update raw PasswordHash (which uses older ASP.NET Identity v2 format), after users successfully signs in
            // the password will be re-hashed using the latest ASP.NET Identity v3 implementation
            dbContext.Users
                .Where(x => x.Id == user.Id)
                .ExecuteUpdate(setters => setters.SetProperty(x => x.PasswordHash, user.PasswordHash));
        }
    }
}
```

In the above example we are using [the `UserManager`](https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.identity.usermanager-1?view=aspnetcore-8.0) to create a new `AppUser` for each of our existing users and then updating the `PasswordHash` property from the `OldAppUser` table.

### Migrating Roles

If you are using custom roles, you will also need to migrate these to the new ASP.NET Core Identity tables, and then assign them to your users based on their existing roles in your previous setup.

Your ServiceStack Authentication roles will be stored in the `UserAuthRole` table separately or in the `Roles` property of your `UserAuth` class. You will need to migrate these roles to the new ASP.NET Core Identity tables and then assign them to your users.

```csharp
foreach (var roleName in allRoles)
{
    var roleExist = await roleManager.RoleExistsAsync(roleName);
    if (!roleExist)
    {
        //create the roles and seed them to the database
        assertResult(await roleManager.CreateAsync(new AppRole(roleName)));
    }
}
```

If your use of roles is static, you can create the list of all your roles from your applications code. If your use of roles is dynamic, you will need to query your database for all the roles that exist in your `UserAuthRole` table.

One difference between ServiceStack Authentication and ASP.NET Core Identity is the use of the `Admin` role. In ServiceStack Authentication, the `Admin` role is a special role that gives the user access to all protected resources. In ASP.NET Core Identity, the `Admin` role is just a regular role that can be assigned to users.

This means for users with the `Admin` role in your existing application, additional roles will need to be assigned to them in ASP.NET Core Identity to give them access to the same protected resources.

In BlazorDiffusion we used the Admin role as well as others, but to preserve the existing behavior, we assigned all roles to users with the `Admin` role.

```csharp
foreach (var user in Users.All)
{
    var appUser = new AppUser
    {
        Id = user.Id,
        Email = user.Email,
        DisplayName = user.DisplayName,
        UserName = user.Email,
        Handle = user.Handle,
        Avatar = user.Avatar,
        EmailConfirmed = true,
    };
    if(appUser.Email == "admin@email.com")
        await EnsureUserAsync(appUser, "p@55wOrd", AppRoles.All);
    else
        await EnsureUserAsync(appUser, "p@55wOrd", user.Roles);
}
```

The `EnsureUserAsync` method will assign the roles to the user.

```csharp
async Task EnsureUserAsync(AppUser user, string password, string[]? roles = null)
{
    var existingUser = await userManager.FindByEmailAsync(user.Email!);
    if (existingUser != null) return;

    await userManager!.CreateAsync(user, password);
    if (roles?.Length > 0)
    {
        var newUser = await userManager.FindByEmailAsync(user.Email!);
        assertResult(await userManager.AddToRolesAsync(user, roles));
    }
}
```

### Migrate Foreign Keys from UserAuth to AspNetUsers

If you are using foreign keys in your existing application, you will need to migrate these to the new ASP.NET Core Identity tables.

For databases like PostgreSQL, you can use the `ALTER TABLE` command to add a foreign key constraint to the `AspNetUsers` table, and we will want to remove the foreign key constraint from the `UserAuth` or `AppUser` table.

In BlazorDiffusion for example, the `Creative` table was using a foreign key to the `AppUser` table, so we needed to replace this with a foreign key to the `AspNetUsers` table.

```sql
ALTER TABLE "Creative" DROP CONSTRAINT "FK_Creative_AppUser_UserId";
ALTER TABLE "Creative" ADD CONSTRAINT "FK_Creative_AspNetUsers_UserId" FOREIGN KEY ("UserId") REFERENCES "AspNetUsers" ("Id") ;
```

For databases like SQLite, you will need to create a new table with the foreign key constraint and then copy the data over from the old table.

We use SQLite for BlazorDiffusion since it makes it easy deploy the application, and SQLite is a great option for small applications that don't need to scale.

Since we have to migrate several tables, we can create a `ReplaceForeignKeyConstraint` method to handle this for us.

```csharp
private void ReplaceForeignKeyConstraint<TModel>()
{
    var modelDef = typeof(TModel).GetModelMetadata();
    
    var createTable = SqliteDialect.Provider.ToCreateTableStatement(typeof(TModel));

    var sql = $@"PRAGMA foreign_keys = OFF;
ALTER TABLE {modelDef.ModelName} RENAME TO {modelDef.ModelName}_old;
{createTable}
INSERT INTO {modelDef.ModelName} SELECT * FROM {modelDef.ModelName}_old;
-- DROP TABLE {modelDef.ModelName}_old;
PRAGMA foreign_keys = ON;";
    Db.ExecuteSql(sql);
}
```

When replacing the tables like this, you will need to be aware of the order in which you replace the tables. For example, if you have a foreign key from the `Creative` table to the `AppUser` table, and a foreign key from the `Artifact` table to the `Creative` table, you will need to replace the `Creative` table first, and then the `Artifact` table.

This is because the `Artifact` table has a foreign key to the `Creative` table, and if you replace the `Artifact` table first, the foreign key will still be pointing to the old `Creative` table.

#### Incorrect migration order result

<div class="my-8 ml-20 flex justify-center">
    <img style="max-height:450px" src="/img/posts/identity-migration/incorrect-mapping-diagram.png" alt="Incorrect DB mapping after migration.">
</div>

#### Correct migration order result

<div class="my-8 ml-20 flex justify-center">
    <img style="max-height:450px" src="/img/posts/identity-migration/correct-mapping-diagram.png" alt="Correct DB mapping after migration.">
</div>

The rule of thumb is you will want to replace the tables from most depended on to least depended on.

Another limitation of the SQL above is that the order of the columns in the new table must match the order of the columns in the old table. `INSERT INTO` will insert the data into the new table based on the order of the columns in the new table, so if the order of the columns is different, the data will be inserted into the wrong columns.

During the migration of BlazorDiffusion, we hit this issue with the `Artifact` table. The `Artifact` C# class uses the `AuditBase` base class which has the `CreatedDate` and `ModifiedDate` properties. In a previous migration we added some additional columns as features were added. So when creating a copy of the `Artifact` class in the ServiceStack migration to handle fixing the foreign key, the order of the columns was different.

Thankfully, since internal classes used in migrations are completely separate for repeatable migrations, we can just create the `Artfact` class to be specific for this migration. So instead of inheriting from `AuditBase`, we can just copy the properties from `AuditBase` into the `Artifact` class in the order required.

Putting it all together, we have a migration `Up` method that looks like:

```csharp
public override void Up()
{
    var appHost = HostContext.AppHost;
    var log = appHost.GetApplicationServices().GetRequiredService<ILogger<ConfigureDbMigrations>>();
    
    log.LogInformation("Migrating FKs from AppUser to AspNetUsers...");
    ReplaceForeignKeyConstraint<Creative>();
    ReplaceForeignKeyConstraint<Artifact>();
    ReplaceForeignKeyConstraint<Album>();
    
    ReplaceForeignKeyConstraint<ArtifactLike>();
    ReplaceForeignKeyConstraint<ArtifactComment>();
    ReplaceForeignKeyConstraint<ArtifactCommentReport>();
    
    ReplaceForeignKeyConstraint<AlbumArtifact>();
    ReplaceForeignKeyConstraint<AlbumLike>();
    
    ReplaceForeignKeyConstraint<CreativeModifier>();
    ReplaceForeignKeyConstraint<CreativeArtist>();
    
    ReplaceForeignKeyConstraint<ArtifactCommentVote>();
    
    ReplaceForeignKeyConstraint<ArtifactReport>();
}
```

In a separate migration, we can then drop the old tables after confirming the migration was successful, and the previous data has been migrated to the new tables.

```csharp
public class Migration1007 : MigrationBase
{
    public override void Up()
    {
        DropOldTable<Migration1006.AlbumLike>();
        DropOldTable<Migration1006.AlbumArtifact>();
        DropOldTable<Migration1006.Album>();
        
        DropOldTable<Migration1006.ArtifactLike>();
        DropOldTable<Migration1006.ArtifactCommentReport>();
        DropOldTable<Migration1006.ArtifactCommentVote>();
        DropOldTable<Migration1006.ArtifactComment>();
        DropOldTable<Migration1006.ArtifactReport>();
        DropOldTable<Migration1006.Artifact>();
        
        DropOldTable<Migration1006.CreativeArtist>();
        DropOldTable<Migration1006.CreativeModifier>();
        DropOldTable<Migration1006.Creative>();
    }
    
    private void DropOldTable<TModel>()
    {
        var modelDef = typeof(TModel).GetModelMetadata();
        Db.ExecuteSql($@"PRAGMA foreign_keys = OFF;
DROP TABLE IF EXISTS {modelDef.ModelName}_old;
PRAGMA foreign_keys = ON;");
    }
}
```

### Migrate Foreign Keys from UserAuthRole to AspNetUserRoles

If you have any tables that use a foreign key to your custom `UserAuthRole` table, you will need to do the same as above and migrate these to the new ASP.NET Core Identity tables.

## Why Migrate to ASP.NET Core Identity?

ServiceStack's built-in authentication system is a great option for many applications. It provides a simple and easy to use authentication system that works out of the box with ServiceStack's built-in features like Sessions, Caching, and OrmLite.

However, as a part of making ServiceStack more compatible with the ASP.NET Core ecosystem, we have started to include the use of ASP.NET Core Identity since a lot of ASP.NET developers are already familiar with it.

It also provides features like two-factor authentication, external authentication providers, and more that are not available in ServiceStack's built-in authentication system.

![](/img/posts/identity-migration/two-factor-auth-example.png)

So while you don't have to migrate to ASP.NET Core Identity, it is a great option if you are already familiar with it, or if you are looking to use some of the additional features it provides.

If you are looking to migrate to ASP.NET Core Identity, we hope this guide helps you with your migration. If you have any questions, feel free to reach out on our [forums](https://forums.servicestack.net).


# Docker Containerization in .NET 8
Source: https://servicestack.net/posts/net8-docker-containers

### All .NET Project Templates upgraded to .NET 8

Included in the release of [ServiceStack v8](https://docs.servicestack.net/releases/v8_00) all of ServiceStack's 
[.NET project templates](https://github.com/NetCoreTemplates/) have been upgraded to use **ServiceStack v8** and **.NET 8** target framework, in addition 
the built-in CI/CD deployment GitHub Actions have been upgraded to use the [secure rootless Linux Docker containers](https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/)
that's now built into .NET 8 which allow you to effortlessly deploy your containerized .NET 8 Apps with Docker and
GitHub Registry via SSH to any Linux Server.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
      <svg class="w-28 h-28" xmlns="http://www.w3.org/2000/svg" width="256" height="185" viewBox="0 0 256 185"><path fill="#2396ED" d="M250.716 70.497c-5.765-4-18.976-5.5-29.304-3.5c-1.2-10-6.725-18.749-16.333-26.499l-5.524-4l-3.844 5.75c-4.803 7.5-7.205 18-6.485 28c.24 3.499 1.441 9.749 5.044 15.249c-3.362 2-10.328 4.5-19.455 4.5H1.155l-.48 2c-1.682 9.999-1.682 41.248 18.014 65.247c14.892 18.249 36.99 27.499 66.053 27.499c62.93 0 109.528-30.25 131.386-84.997c8.647.25 27.142 0 36.51-18.75c.24-.5.72-1.5 2.401-5.249l.961-2l-5.284-3.25ZM139.986 0h-26.42v24.999h26.42V0Zm0 29.999h-26.42v24.999h26.42v-25Zm-31.225 0h-26.42v24.999h26.42v-25Zm-31.225 0H51.115v24.999h26.421v-25ZM46.311 59.998H19.89v24.999h26.42v-25Zm31.225 0H51.115v24.999h26.421v-25Zm31.225 0h-26.42v24.999h26.42v-25Zm31.226 0h-26.422v24.999h26.422v-25Zm31.225 0H144.79v24.999h26.422v-25Z"/></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="docker-containers" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    .NET 8 Docker Containers
</h3></div>
<p class="mx-auto mt-5 max-w-3xl text-xl text-gray-500">
    Learn about the latest streamlined containerization support built into .NET 8
</p>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="y57c-9jeIww" style="background-image: url('https://img.youtube.com/vi/y57c-9jeIww/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

### .NET 8 Docker Containerization

.NET 8 simplifies Docker integration using functionality built into the .NET SDK tooling where it's able to use `dotnet publish`
to publish your .NET 8 App to a container image without a Dockerfile, adhering to the latest Least privilege and hardened security
best practices of running App's as non-root by default.

This **publish to container** feature also supports creating Docker images for different architectures like ARM64 which
sees [significant improvements in .NET 8](https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/)
making deploying your .NET Apps to ARM64 an [even better value proposition](https://servicestack.net/posts/cloud-value-between-architectures).

If you need more control over your Docker image, you can still use a Dockerfile to customize your image, and there are [even tools to generate Dockerfiles for you from your project](https://github.com/tmds/build-image).

### GitHub Action Deployments

In today's DevOps ecosystem, [GitHub Actions](https://github.com/features/actions) stand out as an invaluable asset for
automating CI/CD workflows directly within your GitHub repository. The introduction of .NET 8 takes this a step further,
offering a streamlined approach to generating Docker images through the `<PublishProfile>DefaultContainer</PublishProfile>`
setting in your `.csproj`. This ensures consistent application packaging, making it deployment-ready by just using `dotnet publish`.

ServiceStack's project templates bring additional flexibility, by utilizing foundational tools like
[Docker](https://www.docker.com) for containerization and [SSH](https://en.wikipedia.org/wiki/Secure_Shell) for secure deployments,
it's able to deploy your Dockerized .NET applications to any Linux server, whether self-hosted or on any cloud provider.

#### Live Demos use their GitHub Actions to deploy themselves

Each template's Live demo are themselves utilizing their included GitHub Actions to deploy itself to a Linux server running
on a **€13.60 /month** shared 8BG RAM [Hetzner Cloud VM](https://www.hetzner.com/cloud) that's currently running 50+ Docker Containers.

This guide aims to walk you through the hosting setup and the GitHub Actions release process as introduced in the
ServiceStack's latest .NET 8 project templates.

## Deployment Files

Deployment files that are included in project templates to facilitate GitHub Actions deployments:

#### .deploy/
- [nginx-proxy-compose.yml](https://github.com/NetCoreTemplates/blazor/blob/master/.deploy/nginx-proxy-compose.yml) - Manage nginx reverse proxy and Lets Encrypt companion container (one-time setup per server)
- [docker-compose.yml](https://github.com/NetCoreTemplates/blazor/blob/main/.deploy/docker-compose.yml) - Manage .NET App Docker Container

#### .github/workflows/
- [build.yml](https://github.com/NetCoreTemplates/blazor/blob/master/.github/workflows/build.yml) - Build .NET Project and Run Tests
- [release.yml](https://github.com/NetCoreTemplates/blazor/blob/master/.github/workflows/release.yml) - Build container, Push to GitHub Packages Registry, SSH deploy to Linux server, Run DB Migrations and start new Docker Container if successful otherwise revert Migration

## Prerequisites

Before your Linux server can accept GitHub Actions deployments, we need to setup your Linux deployment server.
For a step-by-step walk through of these steps and more information about this solution, checkout our video guide below:

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
      <svg class="w-28 h-28" xmlns="http://www.w3.org/2000/svg" width="256" height="185" viewBox="0 0 256 185"><path fill="#2396ED" d="M250.716 70.497c-5.765-4-18.976-5.5-29.304-3.5c-1.2-10-6.725-18.749-16.333-26.499l-5.524-4l-3.844 5.75c-4.803 7.5-7.205 18-6.485 28c.24 3.499 1.441 9.749 5.044 15.249c-3.362 2-10.328 4.5-19.455 4.5H1.155l-.48 2c-1.682 9.999-1.682 41.248 18.014 65.247c14.892 18.249 36.99 27.499 66.053 27.499c62.93 0 109.528-30.25 131.386-84.997c8.647.25 27.142 0 36.51-18.75c.24-.5.72-1.5 2.401-5.249l.961-2l-5.284-3.25ZM139.986 0h-26.42v24.999h26.42V0Zm0 29.999h-26.42v24.999h26.42v-25Zm-31.225 0h-26.42v24.999h26.42v-25Zm-31.225 0H51.115v24.999h26.421v-25ZM46.311 59.998H19.89v24.999h26.42v-25Zm31.225 0H51.115v24.999h26.421v-25Zm31.225 0h-26.42v24.999h26.42v-25Zm31.226 0h-26.422v24.999h26.422v-25Zm31.225 0H144.79v24.999h26.422v-25Z"/></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="docker-containers" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    Use GitHub Actions for Auto Deployments
</h3></div>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="7dardvqBFbE" style="background-image: url('https://img.youtube.com/vi/7dardvqBFbE/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

### Setup Deployment Server

#### 1. Install Docker and Docker-Compose

Follow [Docker's installation instructions](https://docs.docker.com/engine/install/ubuntu/)
to install the latest version of Docker.

#### 2. Configure SSH for GitHub Actions

Generate a dedicated SSH key pair to be used by GitHub Actions:

:::sh
ssh-keygen -t rsa -b 4096 -f ~/.ssh/github_actions
:::

Add the **public key** to your server's SSH **authorized_keys**:

:::sh
cat ~/.ssh/github_actions.pub >> ~/.ssh/authorized_keys
:::

Add the **private key** to your repo's `DEPLOY_KEY` GitHub Action Secret which GitHub Actions will use to
securely SSH into the server.

#### 3. Set Up nginx-reverse-proxy

You should have a `docker-compose` file similar to the `nginx-proxy-compose.yml` in your repository. Upload this file to your server:

:::sh
scp nginx-proxy-compose.yml user@your_server:~/
:::

To bring up the nginx reverse proxy and its companion container for handling TLS certificates, run:

:::sh
docker compose -f ~/nginx-proxy-compose.yml up -d
:::

This will start an nginx reverse proxy along with a companion LetsEncrypt container that will automatically watch for
additional Docker containers on the same network and initialize them with valid TLS certificates.

### Ready to host containerized .NET Apps

Your Linux server is now ready to accept multiple .NET App deployments from GitHub Actions. The guide below walks through
the process of setting up your GitHub repository to deploy new ServiceStack .NET Apps to your Linux server.

## Step-by-Step Guide

### 1. Create Your ServiceStack Application

Start by creating your ServiceStack application, either from [ServiceStack's Start Page](https://servicestack.net/start)
or by using the [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::sh
x new blazor ProjectName
:::

Replace `ProjectName` with your desired project name to generate a new ServiceStack application pre-configured
with the necessary Docker compose files and GitHub Action workflows as above.

### 2. Configure DNS for Your Application

You need a domain to point to your Linux server. Create an A Record in your DNS settings that points to the IP address
of your Linux server:

- **Subdomain**: `app.example.org`
- **Record Type**: A
- **Value/Address**: IP address of your Linux server

This ensures that any requests to `app.example.org` are directed to your server.

### 3. Setting Up GitHub Secrets

Navigate to your GitHub repository's settings, find the "Secrets and variables" section, and add the following secrets:

- `DEPLOY_HOST`: IP address or hostname of your Linux server
- `DEPLOY_USERNAME`: SSH Username to use for deployments
- `DEPLOY_KEY`: Private key generated for GitHub Actions to SSH into your server
- `LETSENCRYPT_EMAIL`: Your email address for Let's Encrypt notifications

#### Using GitHub CLI for Secret Management

You can use the [GitHub CLI](https://cli.github.com/manual/gh_secret_set) for a quicker setup of these GitHub Action Secrets, e.g:

```bash
gh secret set DEPLOY_HOST --body="linux-server-host"
gh secret set DEPLOY_USERNAME --body="linux-server-username"
gh secret set DEPLOY_KEY --bodyFile="path/to/ssh-private-key"
gh secret set LETSENCRYPT_EMAIL --body="your-email@example.org"
```

These secrets will populate environment variables within your GitHub Actions workflow and other configuration files,
enabling secure and automated deployment of your ServiceStack applications.

### 4. Push to Main Branch to Trigger Deployment

With everything set up, pushing code to the main branch of your repository will trigger the GitHub Action workflow, initiating the deployment process:

```bash
git add .
git commit -m "Initial commit"
git push origin main
```

### 5. Verifying the Deployment

After the GitHub Actions workflow completes, you can verify the deployment by:

- Checking the workflow's logs in your GitHub repository to ensure it completed successfully
- Navigating to your application's URL (e.g., `https://app.example.org`) in a web browser. You should see your ServiceStack application up and running with a secure HTTPS connection

## Features

### DB Migrations

The GitHub Actions workflow includes a step to run database migrations on the remote server in the **Run remote db migrations** step
which automatically runs the `migrate` AppTask in the `app-migration` companion Docker container on the Linux Host Server
to validate migration was successful before completing deployment of the new App. A failed migration will cause deployment to fail
and the previous App version to continue to run.

### Patch appsettings.json with production secrets

One way to maintain sensitive information like API keys and connection strings for your Production App outside of its
source code GitHub repository is to patch the `appsettings.json` file with a [JSON Patch](https://jsonpatch.com) that's
stored in your repo's `APPSETTINGS_PATCH` GitHub Action Secret which will be applied with the deployed App's `appsettings.json` file.

For example this JSON Patch below will replace values and objects in your App's **appsettings.json**:

```json
[
    { "op":"add", "path":"/oauth.facebook.AppSecret",  "value":"xxxx" },
    { "op":"add", "path":"/oauth.microsoft.AppSecret", "value":"xxxx" },
    { "op":"add", "path":"/smtp", "value":{
        "UserName": "xxxx",
        "Password": "xxxx",
        "Host": "smtp-server.example.org",
        "Port": 587,
        "From": "noreply@example.org",
        "FromName": "No Reply"
      } 
    }
]
```

You can test your JSON Patch by saving it to `appsettings.json.patch` and applying it with the
[patch feature](https://docs.servicestack.net/dotnet-tool#patch-json-files) of the `x` dotnet tool:

:::sh
x patch appsettings.json.patch
:::

## Anatomy of GitHub Actions Workflow

GitHub Actions workflows are defined in YAML files, and they provide a powerful way to automate your development process.
This guide will take you through the key sections of the workflow to give you a comprehensive understanding of how it functions.

## Permissions

In this workflow, two permissions are specified:

- `packages: write`: Allows the workflow to upload Docker images to GitHub Packages
- `contents: write`: Required to access the repository content

Specifying permissions ensures that the GitHub Actions runner has just enough access to perform the tasks in the workflow.

## Jobs

This workflow consists of two jobs: `push_to_registry` and `deploy_via_ssh`.

### push_to_registry

This job runs on an Ubuntu 22.04 runner and is responsible for pushing the Docker image to the GitHub Container Registry. It proceeds only if the previous workflow did not fail. The job includes the following steps:

1. **Checkout**: Retrieves the latest or specific tag of the repository's code
2. **Env variable assignment**: Assigns necessary environment variables for subsequent steps
3. **Login to GitHub Container Registry**: Authenticates to the GitHub Container Registry
4. **Setup .NET Core**: Prepares the environment for .NET 8
5. **Build and push Docker image**: Creates and uploads the Docker image to GitHub Container Registry (ghcr.io)

### deploy_via_ssh

This job also runs on an Ubuntu 22.04 runner and depends on the successful completion of the `push_to_registry` job. Its role is to deploy the application via SSH. The steps involved are:

1. **Checkout**: Retrieves the latest or specific tag of the repository's code
2. **Repository name fix and env**: Sets up necessary environment variables
3. **Create .env file**: Generates a .env file required for deployment
4. **Copy files to target server via scp**: Securely copies files to the remote server
5. **Run remote db migrations**: Executes database migrations on the remote server
6. **Remote docker-compose up via ssh**: Deploys the Docker image with the application

## Triggers (on)

The workflow is designed to be triggered by:

1. **New GitHub Release**: Activates when a new release is published
2. **Successful Build action**: Runs whenever the specified Build action completes successfully on the main or master branches
3. **Manual trigger**: Allows for rollback to a specific release or redeployment of the latest release, with an input for specifying the version tag

Understanding these sections will help you navigate and modify the workflow as per your needs, ensuring a smooth and automated deployment process.

## Deployment Server Setup Expanded

### Ubuntu as the Reference Point

Though our example leverages Ubuntu, it's important to emphasize that the primary requirements for this deployment architecture are a Linux operating system, Docker, and SSH. Many popular Linux distributions like CentOS, Fedora, or Debian will work just as efficiently, provided they support Docker and SSH.

### The Crucial Role of SSH in GitHub Actions

**SSH** (Secure SHell) is not just a protocol to remotely access your server's terminal. In the context of GitHub Actions:

- SSH offers a **secure channel** between GitHub Actions and your Linux server
- Enables GitHub to **execute commands directly** on your server
- Provides a mechanism to **transfer files** (like Docker-compose configurations or environment files) from the GitHub repository to the server

By generating a dedicated SSH key pair specifically for GitHub Actions (as above), we ensure a secure and isolated access mechanism. 
Only the entities possessing the private key (in this case, only GitHub Actions) can initiate an authenticated connection.

### Docker & Docker-Compose: Powering the Architecture

**Docker** encapsulates your ServiceStack application into containers, ensuring consistency across different environments. Some of its advantages include:

- **Isolation**: Your application runs in a consistent environment, irrespective of where Docker runs.
- **Scalability**: Easily replicate containers to handle more requests.
- **Version Control for Environments**: Create, maintain, and switch between different container images.

**Docker-Compose** extends Docker's benefits by orchestrating the deployment of multi-container applications:

- **Ease of Configuration**: Describe your application's entire stack, including the application, database, cache, etc., in a single YAML file.
- **Consistency Across Multiple Containers**: Ensures that containers are spun up in the right order and with the correct configurations.
- **Simplifies Commands**: Instead of a long string of Docker CLI commands, a single `docker-compose up` brings your whole stack online.

### NGINX Reverse Proxy: The Silent Workhorse

Using an **nginx reverse proxy** in this deployment design offers several powerful advantages:

- **Load Balancing**: Distributes incoming requests across multiple ServiceStack applications, ensuring optimal resource utilization.
- **TLS Management**: Together with its companion container, nginx reverse proxy automates the process of obtaining and renewing TLS certificates. This ensures your applications are always securely accessible over HTTPS.
- **Routing**: Directs incoming traffic to the correct application based on the domain or subdomain.
- **Performance**: Caches content to reduce load times and reduce the load on your ServiceStack applications.

With an nginx reverse proxy, you can host multiple ServiceStack (or non-ServiceStack) applications on a single server while providing each with its domain or subdomain.

## Additional Resources

### Docker & Docker-Compose

- **[Docker Documentation](https://docs.docker.com/)**: Core concepts, CLI usage, and practical applications
- **[Docker-Compose Documentation](https://docs.docker.com/compose/)**: Define and manage multi-container applications

### GitHub Actions

- **[GitHub Actions Documentation](https://docs.github.com/en/actions)**: Creating workflows, managing secrets, and automation tips
- **[Starter Workflows](https://github.com/actions/starter-workflows)**: Templates for various languages and tools

### SSH & Security

- **[SSH Key Management](https://www.ssh.com/academy/ssh/keygen)**: Guidelines on generating and managing SSH keys
- **[GitHub Actions Secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets)**: Securely store and use sensitive information


# PHP typed client DTOs for .NET APIs
Source: https://servicestack.net/posts/php-typed-apis

We're happy to announce the **11th** [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference)
language to enjoy end-to-end typed support for calling .NET APIs - [PHP](https://www.php.net)!

The **Add ServiceStack Reference** feature enables a simple way for PHP clients and Applications to generate native PHP DTO classes 
to access to your ServiceStack APIs.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
      <svg class="w-60 h-60" xmlns="http://www.w3.org/2000/svg" width="128" height="128" viewBox="0 0 128 128"><path fill="#4F5B93" d="M64 30.332C28.654 30.332 0 45.407 0 64s28.654 33.668 64 33.668c35.345 0 64-15.075 64-33.668S99.346 30.332 64 30.332zm-5.982 9.81h7.293v.003l-1.745 8.968h6.496c4.087 0 6.908.714 8.458 2.139c1.553 1.427 2.017 3.737 1.398 6.93l-3.053 15.7h-7.408l2.902-14.929c.33-1.698.208-2.855-.365-3.473c-.573-.617-1.793-.925-3.658-.925h-5.828L58.752 73.88h-7.291l6.557-33.738zM26.73 49.114h14.133c4.252 0 7.355 1.116 9.305 3.348c1.95 2.232 2.536 5.346 1.758 9.346c-.32 1.649-.863 3.154-1.625 4.52c-.763 1.364-1.76 2.613-2.99 3.745c-1.468 1.373-3.098 2.353-4.891 2.936c-1.794.585-4.08.875-6.858.875h-6.294l-1.745 8.97h-7.35l6.557-33.74zm57.366 0h14.13c4.252 0 7.353 1.116 9.303 3.348h.002c1.95 2.232 2.538 5.346 1.76 9.346c-.32 1.649-.861 3.154-1.623 4.52c-.763 1.364-1.76 2.613-2.992 3.745c-1.467 1.373-3.098 2.353-4.893 2.936c-1.794.585-4.077.875-6.855.875h-6.295l-1.744 8.97h-7.35l6.557-33.74zm-51.051 5.325l-2.742 14.12h4.468c2.963 0 5.172-.556 6.622-1.673c1.45-1.116 2.428-2.981 2.937-5.592c.485-2.507.264-4.279-.666-5.309c-.93-1.032-2.79-1.547-5.584-1.547h-5.035zm57.363 0l-2.744 14.12h4.47c2.965 0 5.17-.556 6.622-1.673c1.449-1.116 2.427-2.981 2.935-5.592c.487-2.507.266-4.279-.664-5.309c-.93-1.032-2.792-1.547-5.584-1.547h-5.035z"/></svg>
   </div>
</div>
<div class="not-prose">
    <div class="text-center"><h3 id="php" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
        End-to-end typed PHP
    </h3></div>
    <p class="mx-auto mt-5 max-w-3xl text-xl text-gray-500">
        Learn about the rich JsonServiceClient & end-to-end typed API support for PHP 
    </p>
    <div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
        <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="ZLVdaJ38vwc" style="background-image: url('https://img.youtube.com/vi/ZLVdaJ38vwc/maxresdefault.jpg')"></lite-youtube>
    </div>
</div>

### PhpStorm ServiceStack Plugin

PHP developers of [PhpStorm](https://www.jetbrains.com/phpstorm/) can get a simplified development experience for consuming
ServiceStack Services by installing the [ServiceStack Plugin](https://plugins.jetbrains.com/plugin/7749-servicestack) 
from the JetBrains Marketplace:

[![](/img/posts/php-typed-apis/phpstorm-servicestack-plugin.webp)](https://plugins.jetbrains.com/plugin/7749-servicestack)

Where you'll be able to right-click on a directory and click on **ServiceStack Reference** on the context menu:

![](/img/posts/php-typed-apis/phpstorm-add-servicestack-reference.webp)

To launch the **Add PHP ServiceStack Reference** dialog where you can enter the remote URL of the ServiceStack endpoint you wish to call to generate the Typed PHP DTOs for all APIs which by default will saved to `dtos.php`:

![](/img/posts/php-typed-apis/phpstorm-add-servicestack-reference-dialog.webp)

Then just import the DTOs and `JsonServiceClient` to be able to consume any of the remote ServiceStack APIs:

```php
<?php

require_once __DIR__ . '/vendor/autoload.php'; // Autoload files using Composer autoload
require_once 'dtos.php';

use dtos\FindTechnologies;
use Servicestack\JsonServiceClient;

$client = JsonServiceClient::create("https://techstacks.io");

$response = $client->send(new FindTechnologies(
    ids: [1,2,4,6],
    vendorName: "Google"));

print_r($response);
```

If any of the the remote APIs change their DTOs can be updated by right-clicking on `dtos.php` and clicking **Update ServiceStack Reference**:

![](/img/posts/php-typed-apis/phpstorm-update-servicestack-reference.webp)

### Install PHP ServiceStack Client

The only requirements for PHP apps to perform typed API Requests are the generated PHP DTOs and the generic `JsonServiceClient`
which can be installed in Composer projects with:

```bash
$ composer require servicestack/client
```

Or by adding the package to your `composer.json` then installing the dependencies:

```json
{
  "require": {
    "servicestack/client": "^1.0"
  }
}
```

## First class development experience

[PHP](https://www.php.net) is one of the worlds most popular programming languages thanks to its ease of use,
platform independence, large standard library, flexibility and fast development experience which sees it excels as
a popular language for web development and for development of popular CMS products like WordPress, Drupal and Joomla
thanks to its flexibility, embeddability and ease of customization.

To maximize the experience for calling ServiceStack APIs within these environments ServiceStack now supports PHP as a
1st class Add ServiceStack Reference supported language which gives PHP developers an end-to-end typed API for consuming
ServiceStack APIs, complete with IDE integration in [PhpStorm](https://www.jetbrains.com/phpstorm/) as well as
[built-in support in x dotnet tool](https://docs.servicestack.net/dotnet-tool#addupdate-servicestack-references)
to generate Typed and annotated PHP DTOs for a remote ServiceStack instance from a single command-line.

### Ideal idiomatic Typed Message-based API

To maximize the utility of PHP DTOs and enable richer tooling support and greater development experience, PHP DTOs are generated as
Typed [JsonSerializable](https://www.php.net/manual/en/class.jsonserializable.php) classes with
[promoted constructors](https://www.php.net/manual/en/language.oop5.decon.php#language.oop5.decon.constructor.promotion)
and annotated with [PHPDoc Types](https://phpstan.org/writing-php-code/phpdoc-types) - that's invaluable when scaling
large PHP code-bases and greatly improves discoverability of a remote API. DTOs are also enriched with interface markers
and Annotations which enables its optimal end-to-end typed API:

The PHP DTOs and `JsonServiceClient` library follow
[PHP naming conventions](https://infinum.com/handbook/wordpress/coding-standards/php-coding-standards/naming)
so they'll naturally fit into existing PHP code bases. Here's a sample of [techstacks.io](https://techstacks.io)
generated PHP DTOs containing string and int Enums, an example AutoQuery and a standard Request & Response DTO showcasing
the rich typing annotations and naming conventions used:

```php
enum TechnologyTier : string
{
    case ProgrammingLanguage = 'ProgrammingLanguage';
    case Client = 'Client';
    case Http = 'Http';
    case Server = 'Server';
    case Data = 'Data';
    case SoftwareInfrastructure = 'SoftwareInfrastructure';
    case OperatingSystem = 'OperatingSystem';
    case HardwareInfrastructure = 'HardwareInfrastructure';
    case ThirdPartyServices = 'ThirdPartyServices';
}

enum Frequency : int
{
    case Daily = 1;
    case Weekly = 7;
    case Monthly = 30;
    case Quarterly = 90;
}

// @Route("/technology/search")
#[Returns('QueryResponse')]
/**
 * @template QueryDb of Technology
 * @template QueryDb1 of TechnologyView
 */
class FindTechnologies extends QueryDb implements IReturn, IGet, JsonSerializable
{
    public function __construct(
        /** @var array<int>|null */
        public ?array $ids=null,
        /** @var string|null */
        public ?string $name=null,
        /** @var string|null */
        public ?string $vendorName=null,
        /** @var string|null */
        public ?string $nameContains=null,
        /** @var string|null */
        public ?string $vendorNameContains=null,
        /** @var string|null */
        public ?string $descriptionContains=null
    ) {
    }

    /** @throws Exception */
    public function fromMap($o): void {
        parent::fromMap($o);
        if (isset($o['ids'])) $this->ids = JsonConverters::fromArray('int', $o['ids']);
        if (isset($o['name'])) $this->name = $o['name'];
        if (isset($o['vendorName'])) $this->vendorName = $o['vendorName'];
        if (isset($o['nameContains'])) $this->nameContains = $o['nameContains'];
        if (isset($o['vendorNameContains'])) $this->vendorNameContains = $o['vendorNameContains'];
        if (isset($o['descriptionContains'])) $this->descriptionContains = $o['descriptionContains'];
    }
    
    /** @throws Exception */
    public function jsonSerialize(): mixed
    {
        $o = parent::jsonSerialize();
        if (isset($this->ids)) $o['ids'] = JsonConverters::toArray('int', $this->ids);
        if (isset($this->name)) $o['name'] = $this->name;
        if (isset($this->vendorName)) $o['vendorName'] = $this->vendorName;
        if (isset($this->nameContains)) $o['nameContains'] = $this->nameContains;
        if (isset($this->vendorNameContains)) $o['vendorNameContains'] = $this->vendorNameContains;
        if (isset($this->descriptionContains)) $o['descriptionContains'] = $this->descriptionContains;
        return empty($o) ? new class(){} : $o;
    }
    public function getTypeName(): string { return 'FindTechnologies'; }
    public function getMethod(): string { return 'GET'; }
    public function createResponse(): mixed { return QueryResponse::create(genericArgs:['TechnologyView']); }
}

// @Route("/orgs/{Id}", "DELETE")
class DeleteOrganization implements IReturnVoid, IDelete, JsonSerializable
{
    public function __construct(
        /** @var int */
        public int $id=0
    ) {
    }

    /** @throws Exception */
    public function fromMap($o): void {
        if (isset($o['id'])) $this->id = $o['id'];
    }
    
    /** @throws Exception */
    public function jsonSerialize(): mixed
    {
        $o = [];
        if (isset($this->id)) $o['id'] = $this->id;
        return empty($o) ? new class(){} : $o;
    }
    public function getTypeName(): string { return 'DeleteOrganization'; }
    public function getMethod(): string { return 'DELETE'; }
    public function createResponse(): void {}
}
```

The smart PHP `JsonServiceClient` available in the [servicestack/client](https://packagist.org/packages/servicestack/client)
packagist package enables the same productive, typed API development experience available in our other 1st-class supported
client platforms.

Using promoted constructors enables DTOs to be populated using a single constructor expression utilizing named parameters
which together with the generic `JsonServiceClient` enables end-to-end typed API Requests in a single LOC:

```php
use Servicestack\JsonServiceClient;
use dtos\Hello;

$client = new JsonServiceClient("https://test.servicestack.net");

/** @var HelloResponse $response */
$response = client->get(new Hello(name:"World"));
```

> The `HelloResponse` optional type hint doesn't change runtime behavior but enables static analysis tools and IDEs like PyCharm to provide rich intelli-sense and development time feedback.

For more usage examples and information about ServiceStack's PHP support checkout the
[PHP Add ServiceStack Reference](https://docs.servicestack.net/php-add-servicestack-reference) docs.


# ASP.NET Core Identity Auth in .NET 8
Source: https://servicestack.net/posts/net8-identity-auth

### ASP.NET Core Identity Auth now the default

A significant change from **ServiceStack v8** is the adoption of the same ASP.NET Core Identity Authentication
that's configured in Microsoft's default Projects templates in ServiceStack's new Project Templates.

## History of ServiceStack Authentication

ServiceStack has always maintained its own [Authentication and Authorization](https://docs.servicestack.net/auth/authentication-and-authorization) provider model,
primarily as it was the only way to provide an integrated and unified Authentication model that worked across all our
supported hosting platforms, inc. .NET Framework, ASP.NET Core on .NET Framework, HttpListener and .NET (fka .NET Core).

Whilst the Authentication story in ASP.NET has undergone several cycles of changes over the years, the ServiceStack
Auth Model has  remained relatively consistent and stable, with no schema changes required since release whilst still
providing flexible options for [extending UserAuth tables](https://docs.servicestack.net/auth/auth-repository#extending-userauth-tables) and typed [User Sessions](https://docs.servicestack.net/auth/sessions#using-typed-sessions-in-servicestack).

### .NET Framework considered legacy

Although the multi-platform support of the unified Authentication model has been vital for Organizations migrating their systems
to .NET (Core) where ServiceStack Customers have been able to enjoy [Exceptional Code reuse](https://docs.servicestack.net/netcore#exceptional-code-reuse),
it's become clear that the .NET platform (e.g. .NET 8) is the only platform that should be considered for new projects and
that .NET Framework should only be considered a stable legacy platform for running existing systems on.

Given Microsoft has committed to [Authentication Improvements in .NET 8](https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/)
it's become more important to easily integrate ServiceStack with new and existing .NET projects to access these new features
than to continue recommending ServiceStack's unified Auth Providers as the default option for new projects.

### ServiceStack will use Identity Auth in new projects 

ASP.NET Core Identity Auth is the default Auth Model adopted in new ServiceStack projects which closely follows the same
approach as the Microsoft Project Template it integrates ServiceStack with, e.g. the .NET 8
**Blazor** and **Blazor Vue** project templates adopts the exact same Auth configuration as Microsoft's default Blazor Project
Template configured with **Individual** Identity Auth, likewise with the **Bootstrap** and **Tailwind** styled **MVC** and
**Razor Pages** templates.

You can find ServiceStack Integrated Identity Auth Templates for each of ASP.NET Core's major Blazor, Razor Pages and MVC
Project Templates:

<div class="not-prose mx-auto">
  <h3 id="identityauth-template" class="mb-4 text-4xl tracking-tight font-extrabold text-gray-900">
      Create a Project with ASP.NET Identity Auth
  </h3>
  <identity-auth-templates></identity-auth-templates>
</div>

### Identity Auth Template Live Demos

For a quick preview of what these look like, checkout out their Internet Hosted Live Demos:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://blazor.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://blazor-vue.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor-vue.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-vue.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://razor.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/razor.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">razor.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://mvc.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/mvc.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">mvc.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://razor-bootstrap.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/razor-bootstrap.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">razor-bootstrap.web-templates.io</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://mvc-bootstrap.web-templates.io">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/mvc-bootstrap.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">mvc-bootstrap.web-templates.io</div>
    </a>
</div>


The configuration and source code for the above projects are a good reference for how to configure ServiceStack with
Identity Auth in your own projects:

- [blazor](https://github.com/NetCoreTemplates/blazor)
- [blazor-vue](https://github.com/NetCoreTemplates/blazor-vue)
- [razor](https://github.com/NetCoreTemplates/razor)
- [mvc](https://github.com/NetCoreTemplates/mvc)
- [razor-bootstrap](https://github.com/NetCoreTemplates/razor-bootstrap)
- [mvc-bootstrap](https://github.com/NetCoreTemplates/mvc-bootstrap)

The **Bootstrap** versions use same Individual Identity Auth Pages that Microsoft's **Razor Pages** and **MVC** templates use,
whilst the **Tailwind** versions have been enhanced to use **Tailwind CSS** instead of Bootstrap,
includes a **visual QR Code** implementation that was missing and includes an
`IEmailSender` SMTP solution that's easily enabled via Configuration to use your preferred **SMTP Server**.

## Migrating to ASP.NET Core Identity Auth

Migrating from ServiceStack Auth to Identity Auth should be relatively straight-forward as ServiceStack uses a compatible
Identity v2 password hashing format, which should let you migrate your users to Identity Auth without them noticing.

## ServiceStack's Identity Auth Integration

ServiceStack's Identity Auth integration is focused on high compatibility so existing ServiceStack Customers
require minimal effort to migrate existing code bases to use the new Identity Auth integration, despite Identity Auth
being an entirely different Auth Provider model and implementation.

It does this by retaining a lot of the existing user-facing Authentication and Session abstractions that ServiceStack APIs
use for Authorization as well as existing endpoints and Request/Response DTOs that ServiceStack Clients use to Authenticate,
but replace their internal implementation to use ASP.NET Identity Auth instead.

The new Identity Auth integration is contained in the .NET 6+ **ServiceStack.Extensions** NuGet package:

```xml
<PackageReference Include="ServiceStack.Extensions" Version="8.*" />
```

Which at a minimum lets you configure ServiceStack to use Identity Auth by simply registering the existing `AuthFeature`
plugin with the Application's custom EF `ApplicationUser` Data Model:

```csharp
Plugins.Add(new AuthFeature(IdentityAuth.For<ApplicationUser>()));
```

It requires minimal configuration as all Authorization is configured using ASP.NET Core's
standard APIs, any configuration in this plugin is then just used to customize Identity Auth's integration with ServiceStack.

There's also no new concepts to learn as all ASP .NET Core endpoints, pages and controllers continue to Authenticate against
the populated `ClaimsPrincipal` whilst all ServiceStack APIs continue to Authenticate against the populated typed
[User Session](https://docs.servicestack.net/auth/sessions).

The `AuthFeature` works by registering the following Identity Auth Providers:

### Identity Auth Providers

- [IdentityApplicationAuthProvider](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack/src/ServiceStack.Extensions/Auth/IdentityApplicationAuthProvider.cs) - Converts an Identity Auth `ClaimsPrincipal` into a ServiceStack Session
- [IdentityCredentialsAuthProvider](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack/src/ServiceStack.Extensions/Auth/IdentityCredentialsAuthProvider.cs) - Implements ServiceStack's `Authenticate` API using Identity Auth
- [IdentityJwtAuthProvider](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack/src/ServiceStack.Extensions/Auth/IdentityJwtAuthProvider.cs) - Converts an Identity Auth JWT into an Authenticated ServiceStack Session

Only the `IdentityApplicationAuthProvider` is registered by default which is required to convert Identity Auth's `ClaimPrincipal`
into an Authenticated ServiceStack [Session](https://docs.servicestack.net/auth/sessions). The other Auth Providers are required if you want to enable authentication with
ServiceStack's endpoints. E.g. ServiceStack's [Built-in UIs](https://servicestack.net/auto-ui) would require the Credentials Auth
to be enabled to authenticate via the built-in Sign In dialogs.

### Configuring Auth Providers

Which is what all the Blazor and MVC Identity Auth templates enable by default in
[Configure.Auth.cs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Configure.Auth.cs):

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => 
        {
            appHost.Plugins.Add(new AuthFeature(IdentityAuth.For<ApplicationUser>(
                // Configure ServiceStack's Integration with Identity Auth
                options => {
                    options.EnableCredentialsAuth = true;
                    options.SessionFactory = () => new CustomUserSession();
                })
            ));
        });
}
```

If you're using a `CustomUserSession` you'll also need to register it with the `SessionFactory` for it to be used.

Each of the Identity Auth Providers can also be customized individually:

```csharp
Plugins.Add(new AuthFeature(IdentityAuth.For<ApplicationUser>(options => {
        // Configure IdentityApplicationAuthProvider
        options.AuthApplication...

        // Configure IdentityCredentialsAuthProvider
        options.EnableCredentialsAuth = true;
        options.AuthCredentials...

        // Configure IdentityJwtAuthProvider
        options.EnableJwtAuth = true;
        options.AuthJwt...
    })
));
```

Typically you'll want to use the included Identity UI Pages and dependencies to register new users and assign roles,
but if you have any existing client integrations that use ServiceStack APIs they can also be enabled with:

```csharp
Plugins.Add(new AuthFeature(IdentityAuth.For<ApplicationUser>(options => {
    // Include ServiceStack's Register API
    options.IncludeRegisterService = true;
    
    // Include ServiceStack's AssignRoles and UnAssignRoles APIs
    options.IncludeAssignRoleServices = true;
));
```

### Extending Identity Auth Cookies and User Sessions

By default all [well known Claim Names](https://github.com/ServiceStack/ServiceStack/blob/3ab3d23c85cf48435b8cd9386f25afab79fcb542/ServiceStack/src/ServiceStack.Extensions/Auth/IdentityApplicationAuthProvider.cs#L49)
are used to populate the User Session, but you can also include additional claims in the Identity Auth Cookie
and use them to populate the User Session by overriding `PopulateFromClaims()` in your
[CustomUserSession.cs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp.ServiceInterface/Data/CustomUserSession.cs), e.g:

```csharp
public class CustomUserSession : AuthUserSession
{
    public override void PopulateFromClaims(IRequest httpReq, ClaimsPrincipal principal)
    {
        // Populate Session with data from Identity Auth Claims
        ProfileUrl = principal.FindFirstValue(JwtClaimTypes.Picture);
    }
}

// Add additional claims to the Identity Auth Cookie
public class AdditionalUserClaimsPrincipalFactory(UserManager<ApplicationUser> userManager, RoleManager<IdentityRole> roleManager, IOptions<IdentityOptions> optionsAccessor)
    : UserClaimsPrincipalFactory<ApplicationUser,IdentityRole>(userManager, roleManager, optionsAccessor)
{
    public override async Task<ClaimsPrincipal> CreateAsync(ApplicationUser user)
    {
        var principal = await base.CreateAsync(user);
        var identity = (ClaimsIdentity)principal.Identity!;

        var claims = new List<Claim>();
        // Add additional claims here
        if (user.ProfileUrl != null)
        {
            claims.Add(new Claim(JwtClaimTypes.Picture, user.ProfileUrl));
        }

        identity.AddClaims(claims);
        return principal;
    }
}
```

### Custom Application User Primary Key

The default `IdentityUser` uses a `string` as the primary key populated with a Guid, but you could also change it to use an 
`int` by having your EF IdentityUser Data Model inherit from `IdentityUser<int>` instead:

```csharp
public class AppUser : IdentityUser<int>
{
    //...
}
```

You'll also need to specify the Key Type when registering the `AuthFeature` plugin:

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => {
            appHost.Plugins.Add(new AuthFeature(IdentityAuth.For<AppUser,int>(
                options => {
                    options.EnableCredentialsAuth = true;
                    options.SessionFactory = () => new CustomUserSession();
                })
            ));
        });
}
```

Which the new .NET 8 BlazorDiffusion App does in [Configure.Auth.cs](https://github.com/NetCoreApps/BlazorDiffusionVue/blob/main/BlazorDiffusion/Configure.Auth.cs)
to be compatible with its existing ServiceStack `UserAuth` tables which used an `int` primary key.

## Using Identity Auth in ServiceStack Apps

One of the primary benefits of adopting Identity Auth is the wealth of documentation and resources available for it, 
which also applies to how you would use Identity Auth to secure your own Apps.

If you're new to Identity Auth we recommend starting with the official introduction from Microsoft:

 - [Introduction to Identity on ASP.NET Core](https://learn.microsoft.com/en-us/aspnet/core/security/authentication/identity)

To learn about securing Blazor Apps, go to:

 - [ASP.NET Core Blazor authentication and authorization](https://learn.microsoft.com/en-us/aspnet/core/blazor/security/)

### Declarative Validation Attributes

The recommended way to protect your ServiceStack APIs is to continue to use the [Declarative Validation](https://docs.servicestack.net/declarative-validation) 
attributes which are decoupled from any implementation so be safely annotated on Request DTOs without adding 
any implementation dependencies, where they're also accessible to Clients and UIs using the Request DTOs to invoke your APIs. 

The available Typed Authorization Attributes include:

| Attribute                   | Description                                            |
|-----------------------------|--------------------------------------------------------|
| `[ValidateIsAuthenticated]` | Restrict access toAuthenticated Users only             |
| `[ValidateIsAdmin]`         | Restrict access to Admin Users only                    |
| `[ValidateHasRole]`         | Restrict access to only Users assigned with this Role  |
| `[ValidateHasClaim]`        | Restrict access to only Users assigned with this Claim |
| `[ValidateHasScope]`        | Restrict access to only Users assigned with this Scope |

That can be annotated on **Request DTOs** to protect APIs:

```csharp
[ValidateIsAuthenticated]
[ValidateIsAdmin]
[ValidateHasRole(role)]
[ValidateHasClaim(type,value)]
[ValidateHasScope(scope)]
public class Secured {}
```

## Migrating from ServiceStack Auth

Migrating from ServiceStack Auth to Identity Auth should be relatively straight-forward as ServiceStack uses a compatible
Identity v2 password hashing format, which should let you migrate your users to Identity Auth without them noticing.

:::info TIP
Please ensure your App database is backed up before running any migrations 
:::

#### 1. Rename old AppUser table

You'll want to use a different name so it doesn't conflict with the new Identity Auth `AppUser` Data Model. This
would only be needed to query the User data to migrate to Identity Auth, you'll be able to remove it after 
successfully migrating all your Users.

You don't need to include all the properties of the `UserAuth` base table, just the ones you want to migrate to Identity Auth,
which for Blazor Diffusion was only:

```csharp
// Used by OrmLite to fetch User data to migrate from old ServiceStack `AppUser` table
[Alias("AppUser")]
public class OldAppUser
{
    [AutoIncrement]
    public int Id { get; set; }
    public string UserName { get; set; }
    public string DisplayName { get; set; }
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public string? Handle { get; set; }
    public string Email { get; set; }
    public string PasswordHash { get; set; }
    public string? ProfileUrl { get; set; }
    public string? Avatar { get; set; } //overrides ProfileUrl
    public string? LastLoginIp { get; set; }
    public DateTime? LastLoginDate { get; set; }
    public string RefIdStr { get; set; }
    public DateTime? LockedDate { get; set; }
    public DateTime CreatedDate { get; set; }
    public DateTime ModifiedDate { get; set; }
}
```

#### 2. Create Identity Auth Data Model

If you have a lot of existing references to the `AppUser` name you'll want to retain the same name so the existing references 
wont need to be updated. Essentially your custom EF IdentityUser will want a copy of all the properties you want to migrate
other than `Id`, `Email`, and `PasswordHash` that's already defined in the base `IdentityUser` class:

```csharp
[Alias("AspNetUsers")] // Tell OrmLite which table this EF Data Model maps to
public class AppUser : IdentityUser<int>
{
    public string? FirstName { get; set; }
    public string? LastName { get; set; }
    public string? DisplayName { get; set; }
    public string? ProfileUrl { get; set; }
    [Input(Type = "file"), UploadTo("avatars")]
    public string? Avatar { get; set; } //overrides ProfileUrl
    public string? Handle { get; set; }
    public int? RefId { get; set; }
    public string RefIdStr { get; set; } = Guid.NewGuid().ToString();
    public bool IsArchived { get; set; }
    public DateTime? ArchivedDate { get; set; }
    public string? LastLoginIp { get; set; }
    public DateTime? LastLoginDate { get; set; }
    public DateTime CreatedDate { get; set; } = DateTime.UtcNow;
    public DateTime ModifiedDate { get; set; } = DateTime.UtcNow;
}
```

The `AppUser` Data Model and `int` primary key would also need to be registered in your
[Configure.Auth.cs](https://github.com/NetCoreApps/BlazorDiffusionVue/blob/main/BlazorDiffusion/Configure.Auth.cs)
configuration class:

```csharp
public class ConfigureAuth : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => {
            appHost.Plugins.Add(new AuthFeature(IdentityAuth.For<AppUser,int>(
                options => {
                    options.EnableCredentialsAuth = true;
                    options.SessionFactory = () => new CustomUserSession();
                })
            ));
        });
}
```

#### 3. Add Authentication Configuration

You'll need to configure Entity Framework and add your desired ASP.NET Identity Auth configuration to your App's `Program.cs`. 

We'd recommend copying from a new Microsoft or [ServiceStack .NET 8 Project](https://docs.servicestack.net/auth/identity-auth) 
which closely matches the Authentication options you want to enable, e.g. you can start with the recommended Authentication 
for a new Blazor Project from its [Program.cs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Program.cs):

```csharp
services.AddAuthentication(IdentityConstants.ApplicationScheme)
    .AddIdentityCookies();
services.AddDataProtection()
    .PersistKeysToFileSystem(new DirectoryInfo("App_Data"));

// $ dotnet ef migrations add CreateIdentitySchema
// $ dotnet ef database update
var connectionString = config.GetConnectionString("DefaultConnection") ?? throw new InvalidOperationException("Connection string 'DefaultConnection' not found.");
services.AddDbContext<ApplicationDbContext>(options =>
    options.UseSqlite(connectionString, b => b.MigrationsAssembly(nameof(MyApp))));
services.AddDatabaseDeveloperPageExceptionFilter();

services.AddIdentityCore<ApplicationUser>(options => options.SignIn.RequireConfirmedAccount = true)
    .AddRoles<IdentityRole>()
    .AddEntityFrameworkStores<ApplicationDbContext>()
    .AddSignInManager()
    .AddDefaultTokenProviders();

services.AddSingleton<IEmailSender, NoOpEmailSender>();
services.AddScoped<IUserClaimsPrincipalFactory<ApplicationUser>, AdditionalUserClaimsPrincipalFactory>();
```

Alternatively if you want to add support for external OAuth logins you can copy from the **MVC Tailwind** Authentication 
configuration in its [Program.cs](https://github.com/NetCoreTemplates/mvc/blob/main/MyApp/Program.cs) which will 
also require adding the NuGet dependencies of the OAuth providers you want to support which you can get from its
[MyApp.csproj](https://github.com/NetCoreTemplates/mvc/blob/main/MyApp/MyApp.csproj)

#### 4. Create and Run EF Migrations

After your App is properly configured you'll want to create the EF Migrations for your the Identity Auth User tables
by installing the [dotnet-ef tool](https://learn.microsoft.com/en-us/ef/core/cli/dotnet) and running:

:::sh
dotnet ef migrations add CreateIdentitySchema
:::

Which should create the EF Migrations in the `/Migrations` folder, you can then run the migrations to create the
Identity Auth tables in your App's configured database:

:::sh
dotnet ef database update
:::

#### 5. Implement the Migrate Users Task

This could be implemented in a separate Application or Unit Test although we've found the easiest way to migrate existing users 
is to implement a custom [App Task](https://docs.servicestack.net/app-tasks) as it's able to make use of your App's configured Authentication, EF and OrmLite dependencies
that can then be run from the command-line. 

The implementation should be fairly straight-forward, you'll basically just need to create a new Identity Auth User 
using the `UserManager<AppUser>` dependency for each of your existing users: 

```csharp
public class ConfigureDbMigrations : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureAppHost(appHost => {
            AppTasks.Register("migrate.users", _ => {
                var log = appHost.GetApplicationServices().GetRequiredService<ILogger<ConfigureDbMigrations>>();

                log.LogInformation("Running migrate.users...");
                var scopeFactory = appHost.GetApplicationServices().GetRequiredService<IServiceScopeFactory>();
                using var scope = scopeFactory.CreateScope();
                using var dbContext = scope.ServiceProvider.GetRequiredService<ApplicationDbContext>();
                using var db = scope.ServiceProvider.GetRequiredService<IDbConnectionFactory>().Open();
                var migrateUsers = db.Select(db.From<OldAppUser>().OrderBy(x => x.Id));

                log.LogInformation("Migrating {Count} Existing ServiceStack Users to Identity Auth Users...", migrateUsers.Count);
                MigrateExistingUsers(dbContext, scope.ServiceProvider, migrateUsers).Wait();
            });
            AppTasks.Run();
        });

    private async Task MigrateExistingUsers(ApplicationDbContext dbContext, IServiceProvider services, 
        List<OldAppUser> migrateUsers, string tempPassword="p@55wOrd")
    {
        var userManager = services.GetRequiredService<UserManager<AppUser>>();
        var now = DateTime.UtcNow;

        foreach (var user in migrateUsers)
        {
            var appUser = new AppUser
            {
                Id = user.Id,
                UserName = user.Email,
                Email = user.Email,
                DisplayName = user.DisplayName,
                FirstName = user.FirstName,
                LastName = user.LastName,
                Handle = user.Handle,
                ProfileUrl = user.ProfileUrl,
                Avatar = user.Avatar,
                RefIdStr = user.RefIdStr ?? Guid.NewGuid().ToString(),
                LockoutEnabled = true,
                LockoutEnd = user.LockedDate != null ? now.AddYears(10) : now,
                LastLoginDate = user.LastLoginDate,
                LastLoginIp = user.LastLoginIp,
                CreatedDate = user.CreatedDate,
                ModifiedDate = user.ModifiedDate,
                // Verify you want existing Users emails to be confirmed
                EmailConfirmed = true,
            };
            await userManager.CreateAsync(appUser, tempPassword);

            // Update raw Password Hash using EF
            if (user.PasswordHash != null)
            {
                dbContext.Users
                    .Where(x => x.Id == user.Id)
                    .ExecuteUpdate(setters => setters.SetProperty(x => x.PasswordHash, user.PasswordHash));
            }
        }
    }
}    
```

As there's no official API for updating the raw `PasswordHash` you'll need to use EF's `ExecuteUpdate()` API to update it 
on the `AspNetUsers` table directly.

It should be noted that ServiceStack Auth still uses ASP.NET Core's previous Identity v2 format for hashing its passwords,
this will be automatically re-hashed using the latest ASP.NET Identity v3 format after users successfully sign in.

#### Optimizing the PasswordHash Update 

Whilst migrating users should be a once-off task, if you have a lot of users you may want to optimize the `PasswordHash` update
from a **N+1** query per user to a single query that updates all users in a single command. 

You'll need to use the **UPDATE FROM** syntax that's supported by your RDBMS's, here's an example of how to do it in SQLite:

```sql
UPDATE AspNetUsers
SET PasswordHash = u.PasswordHash
FROM (SELECT Email, PasswordHash FROM AppUser WHERE PasswordHash is NOT NULL) AS u
WHERE u.Email = AspNetUsers.Email;
```

#### Migrating Roles

Migrating Roles will depend how their stored in your App, you'll first need to ensure each role is created in the `AspNetRoles` 
table with:

```csharp
string[] allRoles = [...]; // All Roles in your App
var roleManager = services.GetRequiredService<RoleManager<IdentityRole>>();
foreach (var roleName in allRoles)
{
    var roleExist = await roleManager.RoleExistsAsync(roleName);
    if (!roleExist)
    {
        await roleManager.CreateAsync(new IdentityRole(roleName));
    }
}
```

You can then assign Roles to Users using the `UserManager<AppUser>`, e.g:

```csharp
string[] roles = [...]; // Roles to assign to User 
var newUser = await userManager.FindByEmailAsync(user.Email!);
await userManager.AddToRolesAsync(user, roles);
```

#### 6. Run the migrate.users Task

With everything in place, all that's left is to run the `migrate.users` App Task from the command-line:

:::sh
dotnet run --AppTasks=migrate.users
:::

#### 7. Verify Users can Sign In

After successfully migrating all your users you should check the new `IdentityUser` table to verify all the User data 
you want has been migrated as well as verifying they can sign in with their existing credentials.

#### Create a new ASP.NET Identity Auth Project to copy from

The easiest way to include the Identity Auth UI Pages to your App is to copy your Application into a new .NET 8 Project
that already includes them, you can create a new Blazor App with:

:::sh
x new blazor ProjectName
:::

Or create a new Razor Pages Tailwind or Bootstrap App:

:::sh
x new razor ProjectName
:::

:::sh
x new razor-bootstrap ProjectName
:::

Or new MVC Tailwind or Bootstrap App with:

:::sh
x new mvc ProjectName
:::

:::sh
x new mvc-bootstrap ProjectName
:::

Alternatively you can manually copy the pages from the project template repositories, for Blazor most of the Identity Auth
UI Pages are in the 
[Components/Identity](https://github.com/NetCoreTemplates/blazor/tree/main/MyApp/Components/Identity) and 
[Pages/Account](https://github.com/NetCoreTemplates/blazor/tree/main/MyApp/Components/Pages/Account) folders.

For MVC, most of the Identity UI are in the 
[Account](https://github.com/NetCoreTemplates/mvc/blob/main/MyApp/Controllers/AccountController.cs)
and [Manage](https://github.com/NetCoreTemplates/mvc/blob/main/MyApp/Controllers/ManageController.cs) controllers
as well as their
[Views/Account](https://github.com/NetCoreTemplates/mvc/tree/main/MyApp/Views/Account) and
[Views/Manage](https://github.com/NetCoreTemplates/mvc/tree/main/MyApp/Views/Manage) folders.

### SMTP IEmailSender

The .NET 8 Templates also include a nice solution for sending Identity Auth emails through the `IEmailSender` interface 
which drops the Email Request in the registered Background MQ in 
[Configure.Mq.cs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Configure.Mq.cs)
which uses it to invoke the `SendEmail` API in 
[EmailServices](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp.ServiceInterface/EmailServices.cs) in a 
managed background worker:

```csharp
public class EmailSender(IMessageService messageService) : IEmailSender
{
    public Task SendEmailAsync(string email, string subject, string htmlMessage)
    {
        using var mqClient = messageService.CreateMessageProducer();
        mqClient.Publish(new SendEmail
        {
            To = email,
            Subject = subject,
            BodyHtml = htmlMessage,
        });

        return Task.CompletedTask;
    }
}
```

To enable it you'll need to register your preferred SMTP Server in your App's `appsettings.json`:

```json
{
  "SmtpConfig": {
    "Username": "username",
    "Password": "password",
    "Host": "smtp.mailtrap.io",
    "Port": 587,
    "FromEmail": "mail@example.org"
  }
}
```

Then uncomment the `EmailSender` registration in your `Program.cs` 

```csharp
services.AddSingleton<IEmailSender, EmailSender>();
```

### Send any App Email

The nice part about this solution is that it's not limited to just sending Identity Auth emails, you can also use it to send
any App Email, either by publishing a message to the registered MQ with `PublishMessage` or by using the 
[Service Gateway](https://docs.servicestack.net/service-gateway) to invoke the API directly, e.g:

```csharp
public class MyServices : Service
{
    public object Any(MyRequest request)
    {
        // Send Email in managed Background MQ Worker
        PublishMessage(new SendEmail {
            To = email,
            Subject = subject,
            BodyHtml = body,
        });

        // Block until Email is sent to SMTP Server
        Gateway.Send(new SendEmail {
            To = email,
            Subject = subject,
            BodyHtml = body,
        });
    }
}
```


# .NET 8's Best Blazor is not Blazor as we know it
Source: https://servicestack.net/posts/net8-best-blazor

The best way to find out what's new in .NET 8 Blazor is to watch the excellent 
[Full stack web UI with Blazor in .NET 8](https://www.youtube.com/watch?v=QD2-DwuOfKM) presentation by Daniel Roth and Steve Sanderson, 
which covers how Blazor has become a Full Stack UI Web Technology for developing any kind of .NET Web App.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="YwZdtLEtROA" style="background-image: url('https://img.youtube.com/vi/YwZdtLEtROA/maxresdefault.jpg')"></lite-youtube>
</div>

## Your first .NET 8 Blazor App

You don't get to appreciate what this means until you create your first .NET 8 Blazor App where you'll be pleasantly
surprised that Blazor Apps render fast, clean HTML without needing to load large Web Assembly assets needed for 
Blazor WebAssembly Apps or starting a stateful Web Socket connection required for Blazor Server Interactive Apps.

This is because the **default rendering mode** for Blazor uses neither of these technologies, instead it returns to 
traditional Web App development where Blazor Pages now return clean, glorious HTML - courtesy of Blazor's 
[Static render mode](https://learn.microsoft.com/en-us/aspnet/core/blazor/components/render-modes).

[![](/img/posts/net8-best-blazor/blazor-ssr.png)](https://learn.microsoft.com/en-us/aspnet/core/blazor/components/render-modes)

## Choose your compromise

Previously we were forced to choose upfront whether we wanted to build a Blazor Web Assembly App or a Blazor Server App and
the compromises that came with them, which for public Internet Web Apps wasn't even a choice as Blazor Server Apps perform poorly
over high latency Internet connections. 

This meant choosing Blazor Web Assembly Apps which required downloading a large Web Assembly runtime 
with users experiencing a long delay before the App was functional. To minimize this impact our Blazor WebAssembly Tailwind template 
included [built-in prerendering](https://github.com/LegacyTemplates/blazor-tailwind/blob/main/MyApp.Client/wwwroot/content/prerender.md) where as part of deployment would 
generate **static .html pages** that were deployed with the Blazor Web Assembly front-end UI that can be hosted on CDN 
edge networks to further improve load times. 

Whilst this meant the App's UI would be rendered immediately, it still wouldn't be functional until the Web Assembly runtime was 
downloaded and initialized, which would flicker as the static UI was replaced with Blazor's WASM rendered UI, then later 
Authenticated users would experience further delay and UI jank whilst the App signs in the Authenticated User. 
Whilst prerendering is an improvement over Blazor WASM's default blank loading screen, it's still not ideal for public facing Web Apps.

## .NET 8 Blazor is a Game Changer

The situation has greatly improved in .NET 8 where your entire App no longer needs to be bound to a single Interactivity mode.
Even better, Blazor's default **static rendering** mode results in the best UX where the Website Layout and important 
landing pages can be rendered instantly.

### Interactive only when you need it

Only pages that need Blazor's interactivity features can opt-in to whichever Blazor interactive rendering mode makes 
the most sense, either on a page-by-page or component basis, or by choosing `RenderMode.InteractiveAuto` which uses
**InteractiveWebAssembly** if the WASM runtime is loaded or **InteractiveServer** if it isn't.

### Enhanced Navigation FTW

Ultimately I expect Blazor's new **Enhanced Navigation** is likely the feature that will deliver the biggest UX improvement 
users will experience given it's enabled by default and gives traditional statically rendered Web Apps instant SPA-like 
navigation responsiveness where new pages are swapped in without needing to perform expensive full page reloads.

It's beauty lies in being able to do this as a mostly transparent detail without the traditional SPA complexity of needing 
to manage complex state or client-side routing. It's a smart implementation that's able to perform fine-grained
DOM updates to only parts of pages that have changed, providing the ultimate UX of preserving page state,
like populated form fields and scroll position, to deliver a fast and responsive UX that previously wasn't attainable
from the simplicity of a Server Rendered App.

Its implementation does pose some challenges in implementing certain features, but we'll cover some approaches 
below we've used to overcome them below.

### Full Stack Web UI

Blazor's static rendering with enhanced navigation and its opt-in flexibility makes .NET 8 Blazor a game changer,
expanding it from a very niche set of use-cases that weren't too adversely affected by its Interactivity mode downsides,
to becoming a viable solution for developing any kind of .NET Web App, especially as it can also be utilized within
existing ASP.NET MVC and Razor Pages Apps.

### Benefits over MVC and Razor Pages

In addition, Blazor's superior component model allows building better encapsulated, more reusable and easier-to-use UI components
which has enabled Blazor's rich 3rd Party library ecosystem to flourish, that we ourselves utilize to develop
the high productivity Tailwind Components in the [ServiceStack.Blazor](https://blazor-gallery.servicestack.net) component library.

So far there's only upsides for .NET Web App development, the compromises only kick in when you need Blazor's interactivity features,
luckily these can now be scoped to just the Pages and Components that need them. But how often do we need them?

### When do you need Blazor's Interactivity features?

It ultimately depends on what App your building, but a lot of Websites can happily display dynamic content, navigate quickly 
with enhanced navigation, fill out and submit forms - all in Blazor's default static rendering mode.

Not even advanced features like **Streaming Rendering** used in Blazor Template's
[Weather.razor](https://github.com/dotnet/aspnetcore/blob/v8.0.0-rc.2.23480.2/src/ProjectTemplates/Web.ProjectTemplates/content/BlazorWeb-CSharp/BlazorWeb-CSharp/Components/Pages/Weather.razor)
page require interactivity, as its progressive rendered UI updates are achieved in a single request without interactivity.

In fact the only time `@rendermode InteractiveServer` is needed in the default Blazor template is in the 
[Counter.razor](https://github.com/dotnet/aspnetcore/blob/v8.0.0-rc.2.23480.2/src/ProjectTemplates/Web.ProjectTemplates/content/BlazorWeb-CSharp/BlazorWeb-CSharp/Components/Pages/Counter.razor#L3)
page whose C# Event Handling require it.

Ultimately some form of Interactivity is going to be required in order to add behavior or client-side functionality that 
runs after pages have been rendered, but you still have some options left before being forced to opt into an Interactive Blazor solution.

### Interactive Feature Options

We can see some of these options utilized in the Blazor Template 
[NavMenu.razor](https://github.com/dotnet/aspnetcore/blob/v8.0.0-rc.2.23480.2/src/ProjectTemplates/Web.ProjectTemplates/content/BlazorWeb-CSharp/BlazorWeb-CSharp/Components/Layout/NavMenu.razor)
component which uses JavaScript `onclick` event handlers to add client-side behavior to simulate mouse clicks to toggle UI elements:

```html
<div class="nav-scrollable" onclick="document.querySelector('.navbar-toggler').click()">
```

and submitting forms to Logout users:

```html
<LogoutForm id="logout-form" />
<NavLink class="nav-link" onclick="document.getElementById('logout-form').submit(); return false;">
    <span class="bi bi-arrow-bar-left" aria-hidden="true"></span> Logout
</NavLink>
```

Effectively adding interactivity to Blazor static rendered pages with client-side JavaScript to avoid paying Blazor's Interactivity tax.

#### Avoid using Interactivity in Layouts

This is especially important for any features you want to add to the Websites Layout or Chrome UI which you'll always want to be
statically rendered so landing pages can load fast and render SEO-friendly server rendered content. 

This meant we couldn't use ServiceStack.Blazor's existing [DarkModeToggle.razor](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack.Blazor/src/ServiceStack.Blazor/Components/Tailwind/DarkModeToggle.razor)
component for toggling on/off DarkMode since its statically rendered inside the Websites Layout and requires Interactivity to work.

### Vanilla JS Blazor Components

Fortunately utilizing simple element JavaScript callbacks was enough to be able to re-implement its functionality with Vanilla JS 
in the new [DarkModeToggleLite.razor](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack.Blazor/src/ServiceStack.Blazor/Components/Tailwind/DarkModeToggleLite.razor)
component which works in all Blazor rendering modes, in both full-page or enhanced navigation loads:

```html
<button type="button" onclick="toggleDarkMode()" class=@ClassNames(DarkModeToggle.ButtonClasses, Class) role="switch" aria-checked="false" @attributes="AdditionalAttributes">
    <span class="@DarkModeToggle.InnerClasses" data-class-light="translate-x-5" data-class-dark="translate-x-0">
        <span class="@DarkModeToggle.IconClasses" data-class-light="opacity-0 ease-out duration-100" data-class-dark="opacity-100 ease-in duration-200" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 text-gray-400" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3Z" /></svg>
        </span>
        <span class="@DarkModeToggle.IconClasses" data-class-light="opacity-100 ease-in duration-200" data-class-dark="opacity-0 ease-out duration-100" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 text-indigo-600" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6ZM5.394 6.813L6.81 5.399l3.505 3.506L8.9 10.319zM2 15.005h5v2H2zm3.394 10.193L8.9 21.692l1.414 1.414l-3.505 3.506zM15 25.005h2v5h-2zm6.687-1.9l1.414-1.414l3.506 3.506l-1.414 1.414zm3.313-8.1h5v2h-5zm-3.313-6.101l3.506-3.506l1.414 1.414l-3.506 3.506zM15 2.005h2v5h-2z" /></svg>
        </span>
    </span>
</button>

<script>
window.toggleDarkMode = (function() {
    let isDark = localStorage.getItem('color-scheme') === 'dark'
    const html = document.documentElement
    function renderDarkMode() {
        html.style.setProperty('color-scheme', isDark ? 'dark' : null)
        html.classList.toggle('dark', isDark)
        document.querySelectorAll('[data-class-light]').forEach(el => {
            const removeClasses = isDark
                    ? el.dataset.classLight
                    : el.dataset.classDark
            const addClasses = isDark
                    ? el.dataset.classDark
                    : el.dataset.classLight

            removeClasses.split(' ').forEach(c => el.classList.remove(c))
            addClasses.split(' ').forEach(c => el.classList.add(c))
        })
    }
    renderDarkMode()

    document.addEventListener('DOMContentLoaded', () =>
            Blazor.addEventListener('enhancedload', () => {
                isDark = localStorage.getItem('color-scheme') === 'dark'
                html.classList.toggle('dark', isDark)
                renderDarkMode()
            }))

    return function() {
        isDark = !isDark
        localStorage.setItem('color-scheme', isDark ? 'dark' : 'light')
        renderDarkMode()
    }
})()
</script>
```

To support enhanced navigation you'll need to be aware that `<script>` tags are **only executed once** on initial page load.
You'll instead need to register a callback with Blazor's `enhancedload` event for any startup logic that needs re-executing, 
which is fired after Blazor merges the new page's DOM with the existing DOM, and is where the `<DarkModeToggleLite>` 
component re-renders itself with the correct state.

When using callbacks to invoke global functions like this it's recommended to wrap them in an [IIFE](https://developer.mozilla.org/en-US/docs/Glossary/IIFE) 
for better encapsulation of internal component state and functionality to avoid polluting the global namespace. 

### Try it out!

With that it's ready for action, try it out in a new [blazor](https://github.com/NetCoreTemplates/blazor) Project 
or from its Live Demo by toggling on/off Dark Mode Component in the top right corner:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://blazor-vue.web-templates.io/?light">
        <img class="p-2" src="/img/posts/net8-best-blazor/blazor-light.webp">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-vue.web-templates.io?light</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://blazor-vue.web-templates.io/?dark">
        <img class="p-2" src="/img/posts/net8-best-blazor/blazor-dark.webp">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">blazor-vue.web-templates.io?dark</div>
    </a>
</div>

### Declarative JavaScript Modules  

Unfortunately a lot of other approaches won't work with Blazor's Enhanced Navigation, for example whilst the built-in 
ASP.NET Identity Pages all work without Blazor's Interactivity, the [EnableAuthenticator.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Account/Manage/EnableAuthenticator.razor)
page doesn't actually include a solution for providing a visual QR Code barcode which mobile phones can easily scan to 
setup 2FA Authentication.

Whilst the placeholders are there, that implementation detail is left to us to workout how we best want to implement it 
within our Apps, perhaps because they don't want to force an Interactivity rendering mode in the default template.

To avoid a degraded UX with Blazor Interactivity you'll naturally want to implement this with JavaScript using the popular 
[qrcodejs](https://davidshimjs.github.io/qrcodejs/) library by following its instructions and adding a simple inline script to the page:

```html
<div data-permanent id="qrCode"></div>
<div id="qrCodeData" data-url="@_authenticatorUri"></div>

<script src="lib/js/qrcode.min.js"></script>
<script>
new QRCode(document.getElementById('qrCode'), 
    document.getElementById('qrCodeData').dataset.url)
</script>
```

Whilst this works as expected in full page reloads, it doesn't work in Blazor's Enhanced Navigation as the `<script>` tag
is only executed once on initial page load and not re-executed when the page is loaded with enhanced navigation.

Your options are to change all links to that page with `data-enhance-nav="false"` to turn off enhanced navigation 
to that page, or we need to find another way.

The solution that worked best for us is to use declarative instructions to specify which JavaScript modules should be loaded
for any page, which we can do by adding a `data-module` attribute to any element, e.g:

```html
<div data-module="pages/Account/Manage/EnableAuthenticator.mjs">
```

These instructions are then handled by [app.mjs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/wwwroot/mjs/app.mjs)
on each navigation which loads the specified JavaScript module and calls its `load()` function if it exists:

```js
export async function remount() {
    document.querySelectorAll('[data-module]').forEach(async el => {
        let modulePath = el.dataset.module
        if (!modulePath) return
        if (!modulePath.startsWith('/') && !modulePath.startsWith('.')) {
            modulePath = `../${modulePath}`
        }
        try {
            const module = await import(modulePath)
            if (typeof module.default?.load == 'function') {
                module.default.load()
            }
        } catch (e) {
            console.error(`Couldn't load module ${el.dataset.module}`, e)
        }
    })
}

document.addEventListener('DOMContentLoaded', () =>
    Blazor.addEventListener('enhancedload', remount))
```

Which for `EnableAuthenticator.razor` page loads the 
[EnableAuthenticator.mjs](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/wwwroot/pages/Account/Manage/EnableAuthenticator.mjs)
JavaScript Module which dynamically loads the `qrcode.min.js` library and initializes the QR Code on its exported `load()` function:

```js
import { addScript, $1 } from "@servicestack/client"
const loadJs = addScript('lib/js/qrcode.min.js')

export default {
    async load() {
        await loadJs
        new QRCode($1("#qrCode"), $1('#qrCodeData').dataset.url)
    }
}
```

Which now works as expected in both full page reloads and Blazor's Enhanced Navigation:

[![](/img/posts/net8-best-blazor/blazor-identityauth-qrcode.png)](https://blazor.web-templates.io/Account/Manage/EnableAuthenticator)

## Blazor without Blazor Interactivity

So right now we have a Blazor App that's predominantly statically rendered, with fast and SEO-friendly without any downsides 
of the Blazor's Interactivity options, but how much of our App's functionality can we implement without Blazor Interactivity?

### What doesn't work with Enhanced Navigation

As of now we've managed to implement most of the required functionality with Vanilla JS, however for any moderately complex
UI you'll likely want to use one of the popular JavaScript UI libraries, of which we believe [Vue.js](https://vuejs.org) 
is the best library for progressively enhancing statically rendered content which offers the best balance of features, 
performance and size.

The problem being that the natural way to use Vue.js to progressively enhance HTML content doesn't work with Blazor's 
Enhanced Navigation.

E.g the natural way to implement Blazor's [Counter.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Counter.razor)
page in Vue is to [implement its UI](https://vuejs.org/guide/essentials/template-syntax.html) in HTML and use JavaScript 
to mount the component with the element:

```html
<div id="content">
    <p class="my-4">Current count: {{currentCount}}</p>

    <primary-button v-on:click="incrementCount">Click me</primary-button>
</div>
<script type="module">
import { ref } from 'vue'
import { mount } from 'app.mjs'

const App = {
    setup() {
        const currentCount = ref(0)
        const incrementCount = () => currentCount.value++

        return { currentCount, incrementCount }
    }
}
mount('#content', App)
</script>
```

Which as you'd expect works in full page reloads, but not with Enhanced Navigation, where no JavaScript
is re-executed upon navigation, leaving it as inert HTML.

## Declarative Vue Components

Thankfully we can use the same approach we used for loading JavaScript modules to load Vue.js components, by using the 
`data-component` attribute to specify which **global** Vue component to mount with any properties optionally
specified in the`data-props` attribute, e.g:

```html
<div data-component="GettingStarted" data-props="{template:'blazor'}"></div> 
```

This does require ensuring all components loaded this way are registered as a global component, as done in:

```js
import GettingStarted from "./components/GettingStarted.mjs"

/** Shared Global Components */
const Components = {
    GettingStarted,
}

export function mount(sel, component, props) {
    const app = createApp(component, props)
    Object.keys(Components).forEach(name => {
        app.component(name, Components[name])
    })
    app.mount(document.querySelector(sel))
}
```

However this also means that all global components would need to be downloaded before any Vue Components can be rendered
the first time a website is accessed. Which wont be an issue after the first page is loaded after the browser caches all 
its JS Module dependencies, but we can do better.

### Lazy Loading Vue Components

To avoid this we can instead use the `data-component` attribute to specify the path to the Vue component to load,
ensuring that only the Vue components required for the current page is loaded, e.g:

```html
<div data-component="pages/Counter.mjs"></div> 
```

Which is how we can implement Vue Components that work in both statically rendered and enhanced navigation pages:

```js
import { ref } from 'vue'

export default {
    template: `
        <p class="my-4">Current count: {{currentCount}}</p>

        <PrimaryButton @click="incrementCount">Click me</PrimaryButton>
    `,
    setup() {
        const currentCount = ref(0)
        const incrementCount = () => currentCount.value++

        return { currentCount, incrementCount }
    }
}
```

:::{.text-center}
#### Blazor Counter in Vue.js

<counter></counter>
:::

## The new Blazor Vue Template 

This ends up being how the Interactive features in the new [blazor-vue](https://github.com/NetCoreTemplates/blazor-vue/) template 
are implemented - ideal for building fast, SEO-friendly statically rendered Blazor Web Apps where all its dynamic functionally
uses Vue.js to progressively enhance its static rendered content - eliminating Blazor's current limitations of being able to 
use Blazor static SSR to implement an entire App with:

![](/img/posts/net8-best-blazor/blazor-ssr-advantages.webp)

### Blazor Vue Tailwind Template

The new [blazor-vue](https://github.com/NetCoreTemplates/blazor-vue) template implements all the features of the
[blazor](https://github.com/NetCoreTemplates/blazor) template but reimplements all its interactive features with
Vue.js to and the [Vue Components](/vue/) library.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
      <svg class="w-28 h-28 text-purple-500" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M23.834 8.101a13.912 13.912 0 0 1-13.643 11.72a10.105 10.105 0 0 1-1.994-.12a6.111 6.111 0 0 1-5.082-5.761a5.934 5.934 0 0 1 11.867-.084c.025.983-.401 1.846-1.277 1.871c-.936 0-1.374-.668-1.374-1.567v-2.5a1.531 1.531 0 0 0-1.52-1.533H8.715a3.648 3.648 0 1 0 2.695 6.08l.073-.11l.074.121a2.58 2.58 0 0 0 2.2 1.048a2.909 2.909 0 0 0 2.695-3.04a7.912 7.912 0 0 0-.217-1.933a7.404 7.404 0 0 0-14.64 1.603a7.497 7.497 0 0 0 7.308 7.405s.549.05 1.167.035a15.803 15.803 0 0 0 8.475-2.528c.036-.025.072.025.048.061a12.44 12.44 0 0 1-9.69 3.963a8.744 8.744 0 0 1-8.9-8.972a9.049 9.049 0 0 1 3.635-7.247a8.863 8.863 0 0 1 5.229-1.726h2.813a7.915 7.915 0 0 0 5.839-2.578a.11.11 0 0 1 .059-.034a.112.112 0 0 1 .12.053a.113.113 0 0 1 .015.067a7.934 7.934 0 0 1-1.227 3.549a.107.107 0 0 0-.014.06a.11.11 0 0 0 .073.095a.109.109 0 0 0 .062.004a8.505 8.505 0 0 0 5.913-4.876a.155.155 0 0 1 .055-.053a.15.15 0 0 1 .147 0a.153.153 0 0 1 .054.053A10.779 10.779 0 0 1 23.834 8.1zM8.895 11.628a2.188 2.188 0 1 0 2.188 2.188v-2.042a.158.158 0 0 0-.15-.15Z"></path></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="blazor-vue-template" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    Blazor Vue Template
</h3></div>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="ujbTGn4IwFs" style="background-image: url('https://img.youtube.com/vi/ujbTGn4IwFs/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

<div class="not-prose relative bg-white dark:bg-black py-4">
    <div class="mx-auto max-w-md px-4 text-center sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8">
        <p class="mt-2 text-3xl font-extrabold tracking-tight text-gray-900 dark:text-gray-50 sm:text-4xl">Create a new Blazor Vue Tailwind App</p>
        <p class="mx-auto mt-5 max-w-prose text-xl text-gray-500"> 
            Create a new Blazor Vue Tailwind project with your preferred project name:
        </p>
    </div>
    <blazor-vue-template repo="NetCoreTemplates/blazor-vue" name="Blazor Vue"></blazor-vue-template>
</div>

#### Faster iterative development

Other benefits of using Vue for Interactivity is the fast iterative feedback loop during development that even applies 
to its [Markdown-powered Blog](https://blazor-vue.web-templates.io/blog) which itself can embed rich interactive Vue Components and rich JavaScript UIs 
like Chart.js in its [Markdown Blog Posts](https://blazor-vue.web-templates.io/posts/razor-ssg-new-blog-features) thanks to its unapologetic, complexity-free 
[#NoBuild](https://world.hey.com/dhh/you-can-t-get-faster-than-no-build-7a44131c) solution.

### Blazor App Tailwind Template

Alternatively the [Blazor Project Template](/posts/net8-blazor-template) is for C# Developers who prefer 
to use Blazor end-to-end for all App functionality, which uses Blazor Server and 
[ServiceStack.Blazor Components](https://blazor-gallery.jamstacks.net/) on its Pages requiring Interactivity:

<div class="not-prose shadow rounded-sm p-4">
    <a href="/posts/net8-blazor-template">
        <img src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/blazor.png" alt=""></a>
</div>

Whilst Blazor Interactivity may remain the predominant solution amongst .NET developers we believe .NET 8 Blazor opens the doors
for progressively enhanced statically-rendered Blazor Apps which has now become our preferred solution for developing
most .NET Web Apps.

It overcomes our biggest gripe with Blazor Web Assembly, that we were unsuccessful in
[prerendering away](https://github.com/LegacyTemplates/blazor-tailwind/blob/main/MyApp.Client/wwwroot/content/prerender.md) its poor startup performance and UI jank
in Internet Apps.

## Blazor Vue Diffusion

So when we learned about .NET 8's static default rendering mode and enhanced navigation we jumped at the opportunity to
create the Blazor Vue template which was used to re-implement Blazor Diffusion with Blazor SSR and Vue.js - a statically 
rendered Blazor App that uses Vue.js for all its functionality.

<h3 class="not-prose text-center pb-8">
    <a class="text-4xl text-blue-600 hover:underline" href="https://blazordiffusion.com">https://blazordiffusion.com</a>
</h3>

[Blazor Diffusion](https://github.com/NetCoreApps/BlazorDiffusion) is our Blazor Demo App we used to showcase how you 
could use [Universal API Components](https://youtu.be/Nf5GpfRafo8) to build Blazor Components and entire Blazor Apps
whose source code runs in both Blazor Server and Blazor Web Assembly Interactive modes, which was first 
developed with [Blazor Server](https://github.com/NetCoreApps/BlazorDiffusion) then used a 
[sync.bat](https://github.com/NetCoreApps/BlazorDiffusionWasm/blob/main/sync.bat) script to export its source code into 
a [Blazor Web Assembly](https://github.com/NetCoreApps/BlazorDiffusionWasm) project that was deployed instead.

The Blazor Vue version starts from a clean slate, utilizing statically rendered Blazor for faster page loads and generating 
SEO-friendly content:

[![](/img/posts/net8-best-blazor/blazordiffusionvue.webp)](https://blazordiffusion.com/)

We're very pleased with the results, much faster loading times, enhanced navigation, no UI jankiness, better SEO - essentially 
a better UX overall, despite not needing any prerendering solution - all whilst enjoying a faster iterative development experience 
where all Vue component changes were immediately visible after save.

You can compare the differences of each Blazor Solution from the Live Demos below:

|                     | Live Demo                                                        | Source Code                                                               |
|---------------------|------------------------------------------------------------------|---------------------------------------------------------------------------|
| Blazor Vue          | [blazordiffusion.com](https://blazordiffusion.com)               | [BlazorDiffusionVue](https://github.com/NetCoreApps/BlazorDiffusionVue)   |
| Blazor Web Assembly | [api.blazordiffusion.com](https://api.blazordiffusion.com)       | [BlazorDiffusionWasm](https://github.com/NetCoreApps/BlazorDiffusionWasm) |
| Blazor Server       | [server.blazordiffusion.com](https://server.blazordiffusion.com) | [BlazorDiffusion](https://github.com/NetCoreApps/BlazorDiffusion)         |

> All Live Demos are hosted on a shared [Hetzner Cloud VM](http://cloud.hetzner.com) using SQLite that's replicated to [Cloudflare R2](https://developers.cloudflare.com/r2/) with [Litestream](https://docs.servicestack.net/ormlite/litestream)


# New .NET 8 Blazor Tailwind Template
Source: https://servicestack.net/posts/net8-blazor-template

With the release of **.NET 8**, we're happy to announce ServiceStack's new [Blazor](https://blazor.web-templates.io/)
Tailwind project template that takes advantage of .NET 8 Blazor's new features that redefines modern Web Development in C#.

In this video overview we'll explore how the template, adopts Blazor's familiar **ASP.NET Core Identity** 
for its authentication, utilizes the modern [Tailwind CSS](https://tailwindcss.com) framework for beautiful responsive design
and adopts .NET's best-practice
[Docker Containerization](https://learn.microsoft.com/en-us/dotnet/core/docker/publish-as-container) support for its built-in
[GitHub Action Deployments](https://blazor.web-templates.io/deploy) which enables a simple ready-made CI solution for deployment to any
Linux Host via SSH and Docker compose.

We’ll also discuss the project's structure, usage of **ASP.NET Core Identity** for Authorization and utilizing
**ServiceStack Blazor Components** for data handling, all integrated to maximize developer efficiency in building Web Applications.

<div class="not-prose mt-16 flex flex-col items-center">
   <div class="flex">
      <svg class="w-28 h-28 text-purple-500" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M23.834 8.101a13.912 13.912 0 0 1-13.643 11.72a10.105 10.105 0 0 1-1.994-.12a6.111 6.111 0 0 1-5.082-5.761a5.934 5.934 0 0 1 11.867-.084c.025.983-.401 1.846-1.277 1.871c-.936 0-1.374-.668-1.374-1.567v-2.5a1.531 1.531 0 0 0-1.52-1.533H8.715a3.648 3.648 0 1 0 2.695 6.08l.073-.11l.074.121a2.58 2.58 0 0 0 2.2 1.048a2.909 2.909 0 0 0 2.695-3.04a7.912 7.912 0 0 0-.217-1.933a7.404 7.404 0 0 0-14.64 1.603a7.497 7.497 0 0 0 7.308 7.405s.549.05 1.167.035a15.803 15.803 0 0 0 8.475-2.528c.036-.025.072.025.048.061a12.44 12.44 0 0 1-9.69 3.963a8.744 8.744 0 0 1-8.9-8.972a9.049 9.049 0 0 1 3.635-7.247a8.863 8.863 0 0 1 5.229-1.726h2.813a7.915 7.915 0 0 0 5.839-2.578a.11.11 0 0 1 .059-.034a.112.112 0 0 1 .12.053a.113.113 0 0 1 .015.067a7.934 7.934 0 0 1-1.227 3.549a.107.107 0 0 0-.014.06a.11.11 0 0 0 .073.095a.109.109 0 0 0 .062.004a8.505 8.505 0 0 0 5.913-4.876a.155.155 0 0 1 .055-.053a.15.15 0 0 1 .147 0a.153.153 0 0 1 .054.053A10.779 10.779 0 0 1 23.834 8.1zM8.895 11.628a2.188 2.188 0 1 0 2.188 2.188v-2.042a.158.158 0 0 0-.15-.15Z"></path></svg>
   </div>
</div>
<div class="not-prose mt-4 px-4 sm:px-6">
<div class="text-center"><h3 id="blazor-template" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    Blazor Tailwind Template
</h3></div>
<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="hqyozHSL0Nk" style="background-image: url('https://img.youtube.com/vi/hqyozHSL0Nk/maxresdefault.jpg')"></lite-youtube>
</div>
</div>

<div class="not-prose relative bg-white dark:bg-black py-4">
    <div class="mx-auto max-w-md px-4 text-center sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8">
        <p class="mt-2 text-3xl font-extrabold tracking-tight text-gray-900 dark:text-gray-50 sm:text-4xl">Create a new Blazor Tailwind App</p>
        <p class="mx-auto mt-5 max-w-prose text-xl text-gray-500"> 
            Create a new Blazor Tailwind project with your preferred project name:
        </p>
    </div>
    <blazor-template repo="NetCoreTemplates/blazor" name="Blazor"></blazor-template>
</div>

## ASP.NET Core Identity Integration

In terms of security, the template integrates ASP.NET Core Identity, offering a structured approach to authentication,
including support for email confirmation, two-factor authentication and GDPR compliance features.

Integrating ASP.NET Core Identity doesn't complicate securing ServiceStack services which can still be secured with
[Declarative Validation Attributes](https://docs.servicestack.net/auth/authentication-and-authorization#declarative-validation-attributes)
for role-based access control, e.g. using `[ValidateHasRole("Employee")]` directly on Request DTOs, bringing any 
existing knowledge and experience with ServiceStack Authentication to securing UIs and APIs with Identity Auth.

The template includes a pre-baked solution for sending general and Identity Auth Emails with your configured SMTP Server
in managed background workers with [Background MQ](https://docs.servicestack.net/background-mq) and an enhanced version of the default 
ASP.NET Core Blazor Identity UI, with all pages upgraded to use a beautiful Tailwind CSS theme as well as integration with 
[qrcode.js](https://davidshimjs.github.io/qrcodejs/) 
for providing a visual QR Code barcode which mobile phone users can easily scan to setup 2FA Authentication:

![](/img/posts/net8-best-blazor/blazor-identityauth-qrcode.png)

## Responsive and Interactive UIs with Tailwind CSS

With responsive UI out-of-the-box, thanks to Tailwind CSS, Developers can style their Blazor Apps with the modern 
popular utility-first CSS framework for creating beautiful, maintainable **Responsive UIs** with **DarkMode** support

It also takes full advantage of Blazor’s static rendering for its Website layout for optimal performance and SEO,
so only Pages and Components that require interactivity need to opt-in for Blazor Server Interactive rendering modes.

## ServiceStack.Blazor Tailwind Components

The [ServiceStack.Blazor Components](https://blazor-gallery.jamstacks.net), updated for .NET 8 enables you to rapidly
develop beautiful Blazor Apps integrated with Rich high-productivity UI Tailwind Components like
[AutoQueryGrid](https://blazor-gallery.servicestack.net/gallery/autoquerygrid) and
[AutoForms](https://blazor-gallery.servicestack.net/gallery/autoform) which interface with
[AutoQuery services](https://docs.servicestack.net/autoquery/) to provide a full CRUD data management UI with minimal effort.

## Enhanced Containerization

.NET 8 simplifies Docker integration. Using `dotnet publish`, developers can now automate the creation of Docker images 
that adhere to best security practices, such as running as a non-root user in containers that can utilize the built-in 
GitHub Actions to effortlessly deploy their containerized Blazor Apps with Docker and GitHub Registry via SSH to any Linux Server.

## Other Template Features

Other features that enhances the default ASP.NET Blazor App templates with several modern, high-productivity features, include:

- [Entity Framework](https://learn.microsoft.com/ef/) & [OrmLite](https://docs.servicestack.net/ormlite/) - Choose the best ORM for each App feature, with a unified solution that sees [OrmLite's DB Migrations](https://docs.servicestack.net/ormlite/db-migrations) run both EF and OrmLite migrations, inc. Seed Data with a single command at Development or Deployment
- [SQLite Database](https://www.sqlite.org) - Set up as the default database, it allows developers to start immediately without configuring a separate database server
- [AutoQuery](https://docs.servicestack.net/autoquery/) - Rapidly developing data-driven APIs, UIs and CRUD Apps
- [Auto Admin Pages](https://www.youtube.com/watch?v=BXjcKkaK-nM) - Quickly develop your back-office CRUD Admin UIs to manage your App's Database tables at [/admin](https://blazor.web-templates.io/admin)
- [Markdown](https://docs.servicestack.net/razor-press/syntax) - Maintain SEO-friendly documentation and content-rich pages like this one with just Markdown, beautifully styled with [@tailwindcss/typography](https://tailwindcss.com/docs/typography-plugin)
- [Built-in UIs](https://servicestack.net/auto-ui) - Use ServiceStack's Auto UIs to [Explore your APIs](https://docs.servicestack.net/api-explorer) at [/ui](https://blazor.web-templates.io/ui/)
  or Query your [App's Database Tables](https://docs.servicestack.net/admin-ui-database) at [/admin-ui/database](https://blazor.web-templates.io/admin-ui/database)
- [Universal API Components](https://youtu.be/Nf5GpfRafo8) - Effortlessly create reusable, maximally performant universal Blazor API Components that works in Blazor Server and Web Assembly Interactivity modes
- [Organized Project Structure](https://docs.servicestack.net/physical-project-structure) - Divided into AppHost, Service Interface, Service Model, and Tests projects to promote separation of concerns and maintainability.


# ServiceStack.AI - Chat GPT and Managed Cloud Providers
Source: https://servicestack.net/posts/servicestack-ai

## ServiceStack.AI Providers

As the AI landscape is actively changing we want our Apps to be able to easily switch to different **Speech-to-Text**
or ChatGPT and TypeChat providers so we're able to easily evaluate and use the best provider for our use-case.

To support this we're maintaining **FREE** implementation-agnostic abstractions for different AI and GPT Providers to enable
AI features in .NET Apps under the new [ServiceStack.AI](https://github.com/ServiceStack/ServiceStack/tree/main/ServiceStack/src/ServiceStack.Interfaces/AI)
namespace in our dependency-free **ServiceStack.Interfaces** package.

Where the implementations for these abstractions are maintained across the following NuGet packages according to their
required dependencies:

- `ServiceStack.Aws` - AI & GPT Providers for **Amazon Web Services**
- `ServiceStack.Azure` - AI & GPT Providers for **Microsoft Azure**
- `ServiceStack.GoogleCloud` - AI & GPT Providers for **Google Cloud**
- `ServiceStack.AI` - AI & GPT Providers for **OpenAI** APIs and local Whisper and Node TypeChat installs

These abstractions and their implementations enable .NET projects to add AI-powered **natural language features** to their Apps 
whilst decoupling their **Speech-to-text** and **ChatGPT** requirements from any single implementation where they can be 
easily substituted as needed.

### .NET TypeChat Examples

An easy way to evaluate these providers is with the [TypeChat .NET Examples Project](/posts/typescript-typechat-examples)
which contains .NET implementations for each TypeChat Example that are already configured to work with all available
providers whose source code is maintained at:

:::{.my-8 .text-indigo-600 .text-center .text-xl}
[https://github.com/NetCoreApps/TypeChatExamples](https://github.com/NetCoreApps/TypeChatExamples)
:::

Whose Live Demo is hosted at:

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io](https://typechat.netcore.io)
:::

[![](/img/posts/typescript-typechat-examples/typechat.png)](https://typechat.netcore.io)

## ISpeechToText

The `ISpeechToText` interface abstracts Speech-to-Text providers behind a simple API:

```csharp
public interface ISpeechToText
{
    // Once only task to run out-of-band before using the SpeechToText provider
    Task InitAsync(List<string> phrases, CancellationToken token = default);
    
    // Transcribe the Audio at recordingPath and return a JSON API Result
    Task<TranscriptResult> TranscribeAsync(string request, CancellationToken token);
}
```

Which has 5 different Speech-to-Text implementations to choose from:

- `GoogleCloudSpeechToText` - to use Google Cloud's [Speech-to-Text v2](https://cloud.google.com/speech-to-text/v2/) API
- `AwsSpeechToText` - to use [Amazon Transcribe](https://aws.amazon.com/pm/transcribe/) API
- `AzureSpeechToText` - to use [Azure Speech to text](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) API
- `WhisperApiSpeechToText` - to use [OpenAI's Whisper](https://platform.openai.com/docs/api-reference/audio) API
- `WhisperLocalSpeechToText` - to use local install of [OpenAI Whisper](https://github.com/openai/whisper)

## Virtual File System Providers

Likewise file storage is also easily substitutable with [Virtual File System](https://docs.servicestack.net/virtual-file-system)
providers allowing Audio Voice Recordings to be uploaded to your preferred provider:

- `FileSystemVirtualFiles` - stores uploads in local file system (default)
- `GoogleCloudVirtualFiles` - stores uploads in [Google Cloud Storage](https://cloud.google.com/storage)
- `S3VirtualFiles` - stores uploads in [AWS S3](https://aws.amazon.com/s3/)
- `AzureBlobVirtualFiles` - stores uploads in [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction)
- `R2VirtualFiles` - stores uploads in [Cloudflare R2](https://developers.cloudflare.com/r2/)

## OpenAI Solution

Since you'll likely be using OpenAI's ChatGPT API for converting natural language requests into a machine readable request
your App can process, the easiest solution is to also use OpenAI's Whisper API for your App's Speech-to-Text requirements,
which you can configure your App to use in a [Modular Startup](https://docs.servicestack.net/modular-startup) config:

```csharp
[assembly: HostingStartup(typeof(ConfigureOpenAi))]

public class ConfigureOpenAi : IHostingStartup
{
    const bool UseKernel = true;
    
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            services.AddSingleton<ISpeechToText, WhisperApiSpeechToText>();
            
            // Call Open AI Chat API through node TypeChat
            services.AddSingleton<ITypeChat, NodeTypeChat>();
        })
        .ConfigureAppHost(afterConfigure:appHost => {
            
            if (appHost.TryResolve<ISpeechToText>() is IRequireVirtualFiles requireVirtualFiles)
                requireVirtualFiles.VirtualFiles = appHost.VirtualFiles;
        });
}
```

Open AI providers are maintained in the **ServiceStack.AI** NuGet Package:

```xml
<PackageReference Include="ServiceStack.AI" Version="8.*" />
```

### Using Node TypeChat

If you prefer to use Microsoft's node [TypeChat library](https://github.com/microsoft/TypeChat) to utilize its 
auto schema validation and corrective auto-retry features, your [Dockerfile](https://github.com/NetCoreApps/TypeChatExamples/blob/main/Dockerfile) will 
will need to have node installed:

```bash
# install node.js and ffmpeg
RUN apt-get clean && apt-get update && apt-get upgrade -y && apt-get install -y --no-install-recommends curl gnupg ffmpeg \
    && curl -sL https://deb.nodesource.com/setup_current.x | bash - \
    && apt-get install nodejs -yq 

RUN npm install
```

#### package.json

Which installs TypeChat by listing it as a dependency in your App's `package.json`:

```json
{
  "dependencies": {
    "typechat": "^0.0.10"
  }  
}
```

You'll also need a copy of 
[typechat.mjs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/typechat.mjs) wrapper that
the .NET process calls invoke TypeChat, located in the Content Directory root of your Application.

### Using Microsoft Semantic Kernel

Alternatively you can avoid using TypeChat and node altogether by invoking ChatGPT through Microsoft's Semantic Kernel
.NET Library:

```csharp
var kernel = Kernel.Builder.WithOpenAIChatCompletionService(
        Environment.GetEnvironmentVariable("OPENAI_MODEL") ?? "gpt-3.5-turbo", 
        Environment.GetEnvironmentVariable("OPENAI_API_KEY")!)
    .Build();
services.AddSingleton(kernel);
services.AddSingleton<ITypeChat>(c => new KernelTypeChat(c.Resolve<IKernel>()));
```

If you're not using TypeChat you'll likely want to implement your own [Custom Validation and Auto-correcting](/posts/voice-activated-typechat-coffeeshop#custom-validation-with-c-semantic-kernel)
solution which can be more effective that TypeChat's schema validation errors approach.

## Supporting Safari Web Audio

If you're not using OpenAI's Whisper for transcribing you'll likely need to use `ffmpeg` to convert 
[Convert Uploaded Files](/posts/voice-activated-typechat-coffeeshop#converting-uploaded-files) into a format your
Speech-to-Text provider accepts.

## Google Cloud Solution

To use any of the Google Cloud providers your pc needs to be configured with [GoogleCloud Credentials](https://cloud.google.com/speech-to-text/docs/before-you-begin) on a project
with **Speech-to-Text** enabled.

You can then configure your App's **appsettings.json** with your Google Cloud Project and Storage where you would like
uploads to persist to:

```json
{
  "GoogleCloudConfig": {
    "Project": "servicestackdemo",
    "Location": "global",
    "Bucket": "servicestack-typechat"
  }
}
```

Which you'll be able to configure your App to use within a [Modular Startup](https://docs.servicestack.net/modular-startup) config:

```csharp
[assembly: HostingStartup(typeof(ConfigureGoogleCloud))]

public class ConfigureGoogleCloud : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {
            GoogleCloudConfig.AssertValidCredentials();

            var gcp = context.Configuration.GetSection(nameof(GoogleCloudConfig))
                .Get<GoogleCloudConfig>();
            services.AddSingleton(gcp);
            
            services.AddSingleton<ISpeechToText>(c => {
                return new GoogleCloudSpeechToText(SpeechClient.Create(),
                    gcp.ToSpeechToTextConfig(x => {
                        // x.PhraseSetId = PhraseSetId;
                    })
                ) {
                    VirtualFiles = HostContext.VirtualFiles
                };
            });
        })
        .ConfigureAppHost(afterConfigure:appHost => {
            appHost.VirtualFiles = new GoogleCloudVirtualFiles(
                StorageClient.Create(), appHost.Resolve<GoogleCloudConfig>().Bucket!);
        });
}
```

Google Cloud providers are maintained in the **ServiceStack.GoogleCloud** NuGet Package:

```xml
<PackageReference Include="ServiceStack.GoogleCloud" Version="8.*" />
```

### Speech-to-Text Factory

Google Cloud Speech-to-Text APIs supports the ability to improve transcription results by creating 
[Custom PhraseSets and Recognizers](https://cloud.google.com/speech-to-text/docs/adaptation-model) where you can specify
which phrases are likely to be used so you can boost their probability they'll be recognized correctly, they'll also
let you configure which optimized model and languages to use.

If your App uses multiple Phrasesets or Recognizers they'll need return a different configured `GoogleCloudSpeechToText`
provider dependent on the feature that's requested, which you can configure with a `SpeechToTextFactory`:

```csharp
services.AddSingleton<ISpeechToTextFactory>(c => new SpeechToTextFactory
{
    Resolve = feature =>
    {
        var config = c.Resolve<AppConfig>();
        var gcp = c.Resolve<GoogleCloudConfig>();
        var siteConfig = config.GetSiteConfig(feature);

        return new GoogleCloudSpeechToText(
            SpeechClient.Create(),
            gcp.ToSpeechToTextConfig(x => {
                x.RecognizerId = siteConfig.RecognizerId;
                x.PhraseSetId = siteConfig.PhraseSetId;
            }))
        {
            VirtualFiles = HostContext.VirtualFiles
        };
    }
});
```

### Creating Custom Recognizers

The `GoogleCloudSpeechToText` provider also supports recreating a custom recognizer with a vocabulary of boosted phrases
using the `InitAsync` API:

```csharp
ISpeechToTextFactory SpeechToTextFactory { get; set; }

//...
List<KeyValuePair<string, int>> phraseWeights = ...;
var speechProvider = SpeechToTextFactory.Get(feature);
await speechProvider.InitAsync(new() {
    PhraseWeights = phraseWeights
});
```

This will re-create the PhraseSet and Recognizer using the `PhraseSetId` and `RecognizerId` identifiers that provider
was configured with in its `GoogleCloudConfig`.

### GoogleCloud Credential Deployments

Google Cloud configures their Security credentials differently to other providers where instead of storing 
Bearer Tokens or connection strings in Environment Variables, their `GOOGLE_APPLICATION_CREDENTIALS` Environment Variable 
is used to instead specify the path where the actual JSON credentials are stored on disk.

Unfortunately passing the JSON configuration file as-is is incompatible with [Docker Secrets](https://docs.docker.com/engine/swarm/secrets/)
for when you want Production App's credentials maintained outside of the Source Code repository.

A simple workaround you can do in your GitHub Actions [release.yml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/.github/workflows/release.yml)
instead is to **Base64 encode** the credentials which can then be passed as a Docker secret:

```yaml
# Build and push new docker image, skip for manual redeploy other than 'latest'
- name: Build and push Docker images
  uses: docker/build-push-action@v3
  with:
    file: Dockerfile
    context: .
    push: true
    tags: ghcr.io/${{ env.image_repository_name }}:${{ env.TAG_NAME }}
    secrets: |
      googlecloud_credentials_base64=${{secrets.GOOGLE_APPLICATION_CREDENTIALS}}
```

Which your [Dockerfile](https://github.com/NetCoreApps/TypeChatExamples/blob/main/Dockerfile) can then decode with 
Unix `base64` tool before saving the `credentials.json` inside your Docker Container:

```bash
RUN --mount=type=secret,id=googlecloud_credentials_base64 \
    cat /run/secrets/googlecloud_credentials_base64 | base64 -d > /out/googlecloud-credentials.json
```

### IDE Tooling

A nice feature from using Cloud Services is the built-in tooling in IDEs like JetBrains
[Big Data Tools](https://plugins.jetbrains.com/plugin/12494-big-data-tools) where you can inspect new Recordings and ChatGPT
JSON responses from within your IDE, instead of SSH'ing into remote servers to inspect local volumes:

:::{.max-w-sm .mx-auto}
[![](/img/posts/building-typechat-coffeeshop-modelling/googlcloud-plugin.png)](/img/posts/building-typechat-coffeeshop-modelling/googlcloud-plugin.png)
:::

## Azure Solution

You can configure your App to use [Azure AI Speech](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) API 
to transcribe Web Audio Recordings that are persisted in Azure Blob Storage with the configuration below:  

```csharp
[assembly: HostingStartup(typeof(ConfigureAzure))]

public class ConfigureAzure : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {

            var config = context.Configuration.GetSection(nameof(AzureConfig))
                .Get<AzureConfig>();
            services.AddSingleton(config);
            
            services.AddSingleton<ISpeechToText>(c => 
                new AzureSpeechToText(config.ToSpeechConfig()) {
                    VirtualFiles = HostContext.VirtualFiles
                });
        })
        .ConfigureAppHost(afterConfigure:appHost => {

            var config = appHost.Resolve<AzureConfig>();
            appHost.VirtualFiles = new AzureBlobVirtualFiles(
                config.ConnectionString, config.ContainerName);
        });
}
```

Azure providers are maintained in the **ServiceStack.Azure** NuGet Package:

```xml
<PackageReference Include="ServiceStack.Azure" Version="8.*" />
```

### Enable Web Audio Support

As Azure AI Speech only supports limited [Audio formats](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-use-codec-compressed-audio-input-streams),
it's recommended to have [GStreamer](https://gstreamer.freedesktop.org) installed along side your App to enable 
support for more popular compressed Web Audio formats.

Refer to the [GStreamer configuration](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-use-codec-compressed-audio-input-streams#gstreamer-configuration)
docs for how install GStreamer in different Operating Systems.

## AWS Solution

Organizations hosting on AWS can configure their App to use [Amazon Transcribe](https://aws.amazon.com/transcribe/) for 
transcribing their Audio recordings to text that they can store in AWS S3 with the configuration below: 

```csharp
[assembly: HostingStartup(typeof(ConfigureAws))]

namespace GptProviders;

public class ConfigureAws : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) => {

            var config = context.Configuration.GetSection(nameof(AwsConfig)).Get<AwsConfig>();
            services.AddSingleton(config);
            
            services.AddSingleton<ISpeechToText>(c => new AwsSpeechToText(
                new AmazonTranscribeServiceClient(
                    config.AccessKey, config.SecretKey, config.ToRegionEndpoint()),
                new AwsSpeechToTextConfig {
                    Bucket = config.Bucket,
                    VocabularyName = config.VocabularyName,
                }) {
                VirtualFiles = HostContext.VirtualFiles
            });
        })
        .ConfigureAppHost(afterConfigure:appHost => {

            var config = appHost.Resolve<AwsConfig>();
            appHost.VirtualFiles = new S3VirtualFiles(
                new AmazonS3Client(config.AccessKey, config.SecretKey, config.ToRegionEndpoint()), 
                config.Bucket);
        });
}
```

Amazon Web Services providers are maintained in the **ServiceStack.Aws** NuGet Package:

```xml
<PackageReference Include="ServiceStack.Aws" Version="8.*" />
```

### AWS Speech-to-Text Factory

If your App uses custom vocabularies for different features you'll instead want register factory instead so you can 
return the correct configured `AwsSpeechToText` for the request feature:

```csharp
services.AddSingleton<ISpeechToTextFactory>(c => new SpeechToTextFactory
{
    Resolve = feature =>
    {
        var config = c.Resolve<AppConfig>();
        var aws = c.Resolve<AwsConfig>();
        var site = config.GetSiteConfig(feature);
        
        return new AwsSpeechToText(new AmazonTranscribeServiceClient(
                aws.AccessKey, aws.SecretKey, aws.ToRegionEndpoint()),
            aws.ToSpeechToTextConfig(x => x.VocabularyName = site.VocabularyName))
        {
            VirtualFiles = HostContext.VirtualFiles
        };
    }
});
```

### Creating Custom Vocabulary

The `AwsSpeechToText` provider also supports recreating a custom recognizer with a vocabulary of boosted phrases
using the `InitAsync` API:

```csharp
ISpeechToTextFactory SpeechToTextFactory { get; set; }

//...
List<KeyValuePair<string, int>> phraseWeights = ...;
var speechProvider = SpeechToTextFactory.Get(feature);
await speechProvider.InitAsync(new() {
    PhraseWeights = phraseWeights
});
```

This will re-create the Vocabulary with the `VocabularyName` the `AwsSpeechToText` was configured with:

## Cloudflare Solution

[Cloudflare AI](https://ai.cloudflare.com) newly released [AI Gateway](https://blog.cloudflare.com/announcing-ai-gateway/) offers a
managed gateway over OpenAI's APIs to cache responses, limit and retry requests, and provide analytics to help you monitor and track usage
which you can utilize by changing the `OpenAiBaseUrl` used.

You may also soon be able to use Cloudflare's OpenAI's Whisper model independently that's currently on available through their
[Workers AI](https://blog.cloudflare.com/workers-ai/) solution.

In the meantime their [Cloudflare R2](https://developers.cloudflare.com/r2/) product is one of the best value
managed storage solutions available, it's an attractive option to use to store Web Audio recordings for usage with
other Speech-to-Text providers, which your App can be configured to use with:

```csharp
[assembly: HostingStartup(typeof(ConfigureCloudflare))]

public class ConfigureCloudflare : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context, services) =>
        {
            var config = context.Configuration.GetSection(nameof(CloudflareConfig))
                .Get<CloudflareConfig>();
            services.AddSingleton(config);
            
            services.AddSingleton<ISpeechToText>(c => new WhisperApiSpeechToText {
                BaseUri = config.OpenAiBaseUrl!,
            });
        })
        .ConfigureAppHost(afterConfigure:appHost => {

            var config = appHost.Resolve<CloudflareConfig>();
            appHost.VirtualFiles = new R2VirtualFiles(
                new AmazonS3Client(config.AccessKey, config.SecretKey,
                new AmazonS3Config {
                    ServiceURL = config.ToServiceUrl(),
                }), config.Bucket);
        });
}
```

Cloudflare's `R2VirtualFiles` provider is maintained in the **ServiceStack.Aws** NuGet Package:

```xml
<PackageReference Include="ServiceStack.Aws" Version="8.*" />
```

## Local OpenAI Whisper

If your App's requirements and hardware supports it, you can save an infrastructure dependency by using a local install of
[OpenAI's Whisper](https://github.com/openai/whisper), which after installing [ffmpeg](https://ffmpeg.org/download.html)
and Python, can be installed with:

:::sh
pip install -U openai-whisper
:::

This will enable you to transcribe Audio recordings by simply specifying the recording you want a transcription of:

:::sh
whisper recording.webm
:::

#### Usage Notes

- The `--language` flag helps speed up transcriptions by avoiding needing to run auto language detection
- By default whisper will generate its transcription results in all supported `.txt`, `.json`, `.tsv`, `.srt` and `.vtt` formats
  - you can limit to just the format you want it in with `--output_format`, e.g. use `txt` if you're just interested in the transcribed text
- The default install also had [FP16](https://github.com/openai/whisper/discussions/301) and
  [Numba Deprecation](https://github.com/openai/whisper/discussions/1344) warnings

We can resolve these issues by using the modified prompt:

```bash
export PYTHONWARNINGS="ignore"
whisper --language=en --fp16 False --output_format txt recording.webm
```

Which should now generate a clean output containing the recordings transcribed text, that's also written to `recording.txt`:

```
[00:00.000 --> 00:02.000]  A latte, please.
```

This **2s** recording took **9 seconds** to transcribe on an M2 Macbook Air, a fair bit longer than the **1-2 seconds** 
it takes to upload and transcribe recordings to Google Cloud, but still within acceptable response times for real-time transcriptions.

### Switch to local OpenAI Whisper

You can configure your App to use a local **whisper** install by registering `WhisperLocalSpeechToText`:

```csharp
services.AddSingleton<ISpeechToText>(c => new WhisperLocalSpeechToText {
   WhisperPath = ProcessUtils.FindExePath("whisper"),
   TimeoutMs = 120 * 1000,
});
```

### Example Usage

TypeChat Examples `CreateRecording` API implementation in 
[GptServices.cs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples.ServiceInterface/GptServices.cs)
is used to transcribe the uploaded Web Audio recording for **all its examples** which it first creates an entry in the 
`Recording` RDBMS Table before invoking the Speech-to-Text API that is then updated after its successful or error response:

```csharp
public IAutoQueryDb AutoQuery { get; set; }
public ISpeechToTextFactory SpeechToTextFactory { get; set; }

public async Task<Recording> Any(CreateRecording request)
{
    var feature = request.Feature.ToLower();
    var recording = (Recording)await AutoQuery.CreateAsync(request, Request);
    var speechToText = SpeechToTextFactory.Get(request.Feature);

    var transcribeStart = DateTime.UtcNow;
    await Db.UpdateOnlyAsync(() => new Recording { TranscribeStart=transcribeStart },
        where: x => x.Id == recording.Id);

    ResponseStatus? responseStatus = null;
    try
    {
        var response = await speechToText.TranscribeAsync(request.Path);
        var transcribeEnd = DateTime.UtcNow;
        await Db.UpdateOnlyAsync(() => new Recording
        {
            Feature = feature,
            Provider = speechToText.GetType().Name,
            Transcript = response.Transcript,
            TranscriptConfidence = response.Confidence,
            TranscriptResponse = response.ApiResponse,
            TranscribeEnd = transcribeEnd,
            TranscribeDurationMs = (transcribeEnd-transcribeStart).TotalMilliseconds,
            Error = response.ResponseStatus.ToJson(),
        }, where: x => x.Id == recording.Id);
        responseStatus = response.ResponseStatus;
    }
    catch (Exception e)
    {
        await Db.UpdateOnlyAsync(() => new Recording { Error = e.ToString() },
            where: x => x.Id == recording.Id);
        responseStatus = e.ToResponseStatus();
    }

    recording = await Db.SingleByIdAsync<Recording>(recording.Id);

    if (responseStatus != null)
        throw new HttpError(responseStatus, HttpStatusCode.BadRequest);

    return recording;
}
```

### Client Usage Example

Now that our Server supports it we can start using this API to upload Audio Recordings by running the pre-configured
npm script to update our App's [Typed JavaScript DTOs](https://docs.servicestack.net/javascript-add-servicestack-reference):

:::sh
npm run dtos
:::

To simplify capturing Web Audio recordings we've the encapsulated reusable functionality within the [AudioRecorder.mjs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/wwwroot/mjs/AudioRecorder.mjs)
class whose `start()` method starts recording Audio from the Users microphone:

```js
import { AudioRecorder } from "/mjs/AudioRecorder.mjs"

let audioRecorder = new AudioRecorder()
await audioRecorder.start()
```

Where it captures audio chunks until `stop()` is called that are then stitched together into a `Blob` and converted into
a Blob DataURL that's returned within a populated [Audio](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement)
Media element:

```js
const audio = await audioRecorder.stop()
```

That supports the [HTMLMediaElement](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement#instance_methods) API
allowing pause and playback of recordings:

```js
audio.play()
audio.pause()
```

The `AudioRecorder` also maintains the `Blob` of its latest recording in its `audioBlob` field and the **MimeType** that it was
captured with in `audioExt` field, which we can use to upload it to the `CreateRecording` API, which if
successful will return a transcription of the Audio recording:

```js
import { JsonApiClient } from "@servicestack/client"

const client = JsonApiClient.create()

const formData = new FormData()
formData.append('path', audioRecorder.audioBlob, `file.${audioRecorder.audioExt}`)
const api = await client.apiForm(new CreateRecording({feature:'coffeeshop'}),formData)
if (api.succeeded) {
   transcript.value = api.response.transcript
}
```

This is used by all TypeChat Examples that's encapsulated in their UI Razor Pages:

- [CoffeeShop.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/CoffeeShop.cshtml)
- [Sentiment.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/Sentiment.cshtml)
- [Calendar.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/Calendar.cshtml)
- [Restaurant.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/Restaurant.cshtml)
- [Math.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/Math.cshtml)
- [Music.cshtml](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/Music.cshtml)

## Prompt Providers

Whilst not required for usage with other providers, the `IPromptProvider` interface can improve code reuse by
implementing a standard interface for generating their TypeScript `Schema` and Chat GPT `Prompt` texts.

```csharp
// The App Provider to use to generate TypeChat Schema and Prompts
public interface IPromptProvider
{
    // Create a TypeChat TypeScript Schema from a TypeChatRequest
    Task<string> CreateSchemaAsync(CancellationToken token);

    // Create a TypeChat TypeScript Prompt from a User request
    Task<string> CreatePromptAsync(string userMessage, CancellationToken token);
}
```

Implementations contain the App-specific functionality for creating its **TypeScript Schema** and **GPT Prompt**,
which each App in TypeChatExamples maintains in its `*PromptProvider.cs` classes in the
[TypeChatExamples.ServiceInterface](https://github.com/NetCoreApps/TypeChatExamples/tree/main/TypeChatExamples.ServiceInterface) project.

## TypeChat API

After receiving a text transcript of a Customer's natural language request you'll need to enlist the services of Chat GPT 
to convert it into an Order request that your App can understand.

### ITypeChat

Just as we've abstracted the substitutable Speech-to-text Services App binds to, we've also created an abstraction for 
the TypeChat provider App uses, which allows easily swap out and evaluate different solutions or Mocking in tests:

```csharp
public interface ITypeChat
{
    Task<TypeChatResponse> TranslateMessageAsync(TypeChatRequest request, 
        CancellationToken token = default);
}
```

Whilst a simple API on the surface, different execution and customizations options are available in the
`TypeChatRequest` which at a minimum requires the **Schema** & **Prompt** to use and the **UserMessage** to convert:

```csharp
// Request to process a TypeChat Request
public class TypeChatRequest
{
    public TypeChatRequest(string schema, string prompt, string userMessage)
    {
        Schema = schema;
        Prompt = prompt;
        UserMessage = userMessage;
    }

    /// TypeScript Schema
    public string Schema { get; set; }
    
    /// TypeChat Prompt
    public string Prompt { get; set; }
    
    /// Chat Request
    public string UserMessage { get; }
    
    /// Path to node exe (default node in $PATH)
    public string? NodePath { get; set; }

    /// Timeout to wait for node script to complete (default 120s)
    public int NodeProcessTimeoutMs { get; set; } = 120 * 1000;

    /// Path to node TypeChat script (default typechat.mjs)
    public string? ScriptPath { get; set; }
    
    /// TypeChat Behavior we want to use (Json | Program)
    public TypeChatTranslator TypeChatTranslator { get; set; }

    /// Path to write TypeScript Schema to (default Temp File)
    public string? SchemaPath { get; set; }
    
    /// Which directory to execute the ScriptPath (default CurrentDirectory) 
    public string? WorkingDirectory { get; set; }
}
```

There are currently 2 different Chat GPT `ITypeChat` implementations registered in TypeChat Examples
[Configure.Gpt.cs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Configure.Gpt.cs).

### Semantic Kernel TypeChat Provider

The natural approach for interfacing with OpenAI's ChatGPT API in .NET is to use [Microsoft's Semantic Kernel](https://github.com/microsoft/semantic-kernel)
to call it directly, which can be registered with: 

```csharp
var kernel = Kernel.Builder.WithOpenAIChatCompletionService(
        Environment.GetEnvironmentVariable("OPENAI_MODEL") ?? "gpt-3.5-turbo", 
        Environment.GetEnvironmentVariable("OPENAI_API_KEY")!)
    .Build();
services.AddSingleton(kernel);
services.AddSingleton<ITypeChat>(c => new KernelTypeChat(c.Resolve<IKernel>()));
```

## Node TypeChat Provider

As the [TypeChat library](https://github.com/microsoft/TypeChat) uses **typescript** it requires calling out to the 
**node** executable in order to be able to use it from .NET Apps, which can be configured withL

```csharp
services.AddSingleton<ITypeChat>(c => new NodeTypeChat());
```

It works by executing an **external process** that invokes a [typechat.mjs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/typechat.mjs) 
script wrapper around TypeChat's functionality to invoke it and return any error responses in a structured `ResponseStatus` 
format that our .NET App can understand, which can also be invoked manually from the command line with:

:::sh
node typechat.mjs json gpt\coffeeshop\schema.ts "i wanna latte macchiato with vanilla"
:::

TypeChat uses the OpenAI Environment Variables below to access ChatGPT APIs:

- `OPENAI_MODEL` - The OpenAI model name (e.g. **gpt-3.5-turbo** or **gpt-4**)
- `OPENAI_API_KEY` - Your OpenAI API key


## Using Chat GPT to process Natural Language Orders

We now have everything we need to start leveraging Chat GPT to convert our Customers **Natural Language** requests
into Machine readable instructions that our App can understand, guided by the App's TypeChat TypeScript Schema.

TypeChat Examples does this for all its apps in [GptServices.cs](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples.ServiceInterface/GptServices.cs)
that just like `CreateRecording` is a custom AutoQuery CRUD Service that uses AutoQuery to create the
initial `Chat` record, that's later updated with the GPT Chat API Response:

```csharp
public class GptServices : Service
{
    //...
    public IAutoQueryDb AutoQuery { get; set; }
    public IPromptProvider PromptProvider { get; set; }
    public ITypeChat TypeChatProvider { get; set; }
    
    public async Task<object> Any(CreateChat request)
    {
        var feature = request.Feature.ToLower();
        var promptProvider = PromptFactory.Get(feature);
        var chat = (Chat)await AutoQuery.CreateAsync(request, Request);

        var chatStart = DateTime.UtcNow;
        await Db.UpdateOnlyAsync(() => new Chat { ChatStart = chatStart },
            where: x => x.Id == chat.Id);

        ResponseStatus? responseStatus = null;
        try
        {
            var schema = await promptProvider.CreateSchemaAsync();
            var prompt = await promptProvider.CreatePromptAsync(request.UserMessage);
            var typeChatRequest = CreateTypeChatRequest(feature, schema, prompt, request.UserMessage);
            
            var response = await TypeChat.TranslateMessageAsync(typeChatRequest);
            var chatEnd = DateTime.UtcNow;
            await Db.UpdateOnlyAsync(() => new Chat
            {
                Request = request.UserMessage,
                Feature = feature,
                Provider = TypeChat.GetType().Name,
                Schema = schema,
                Prompt = prompt,
                ChatResponse = response.Result,
                ChatEnd = chatEnd,
                ChatDurationMs = (int)(chatEnd - chatStart).TotalMilliseconds,
                Error = response.ResponseStatus.ToJson(),
            }, where: x => x.Id == chat.Id);
            responseStatus = response.ResponseStatus;
        }
        catch (Exception e)
        {
            await Db.UpdateOnlyAsync(() => new Chat { Error = e.ToString() },
                where: x => x.Id == chat.Id);
            responseStatus = e.ToResponseStatus();
        }

        chat = await Db.SingleByIdAsync<Chat>(chat.Id);
        
        WriteJsonFile($"/chat/{feature}/{chat.CreatedDate:yyyy/MM/dd}/{chat.CreatedDate.TimeOfDay.TotalMilliseconds}.json", chat.ToJson());

        if (responseStatus != null)
            throw new HttpError(responseStatus, HttpStatusCode.BadRequest);
        
        return chat;
    }
}
```

### Client Usage

Just like the [CreateRecording client usage](/posts/servicestack-ai#client-usage-example) example we can invoke the
APIs using the typed JavaScript DTOs to invoke the `CreateChat` API to returns Chat GPTs JSON Response directly to the client:

```js
apiChat.value = await client.api(new CreateChat({
    feature: 'coffeeshop',
    userMessage: request.toLowerCase()
}))

if (apiChat.value.response) {
    processChatItems(JSON.parse(apiChat.value.response.chatResponse).items)
} else if (apiChat.value.error) {
    apiProcessed.value = apiChat.value
}
```

Which for the CoffeeShop example is in the structure of the TypeScript Schema's array of `Cart` LineItem's which are matched 
against the products and available customizations from the App's database before being added to the user's cart in the
[processChatItems(items)](https://github.com/NetCoreApps/TypeChatExamples/blob/main/TypeChatExamples/Pages/CoffeeShop.cshtml#L530) function.

## Feedback and Feature Requests Welcome 

Please submit any feature requests for other GPT or AI providers you'd like to see implemented to:

<h3 class="not-prose text-center pb-8">
    <a class="text-3xl text-blue-600 hover:underline" href="https://servicestack.net/ideas">https://servicestack.net/ideas</a>
</h3>


# All of TypeScript's TypeChat Examples in .NET
Source: https://servicestack.net/posts/typescript-typechat-examples

To explore how best to add AI features to .NET Apps we first looked at harnessing the reasoning capabilities of
ChatGPT by utilizing the [Chain-of-Thought](/posts/chat-gpt-agents) technique by constructing sophisticated prompts to
guide ChatGPT into desirable actionable responses which we demonstrate in the 
[ChatGPT Meeting Agent](https://gptmeetings.netcore.io) demo. 

### TypeScript's TypeChat

The TypeScript team have sought a simpler approach that instead of relying on engineering sophisticated prompts to instead 
use TypeScript type's system to define the machine readable model LLMs should return and then if necessary to use 
TypeScript compiler's Schema validation errors to enable auto correcting prompts to guide ChatGPT into returning 
valid responses that our App's can understand.

Whilst less ambitious then **Chain-of-Thought** in trying to leverage ChatGPT's reasoning abilities, it's a simpler 
and more reliable approach when wanting to add natural language features to your App.

TypeScript's TypeChat library and compiler being written in JavaScript presents challenges in being able to utilize
it within .NET Apps. To showcase how best to add TypeChat AI-Powered features into .NET Apps we've recreated 
TypeChat's CoffeeShop App in .NET which first uses [AutoQuery](https://docs.servicestack.net/autoquery/) to quickly
develop the CoffeeShop Data Models, Typed APIs and Management UI to dynamically generate its TypeScript's schema in:

- [Modelling TypeChat's CoffeeShop in .NET](/posts/building-typechat-coffeeshop-modelling)

We then show how to utilize the resulting TypeScript Schema with ChatGPT, which initially uses your choice of 
**5 different Speech-to-Text** providers from OpenAI, GoogleCloud, AWS and Azure to transcribe voice activated commands 
into text that's then submitted to ChatGPT using TypeChat's prompt to convert the natural language request into the 
machine readable response defined by the App's TypeScript Schema that is matched against its Database Products to 
create Item Order requests that are added to the Users Cart in:

- [Creating a Voice Activated CoffeeShop in .NET](/posts/voice-activated-typechat-coffeeshop)

### CoffeeShop

This results in an example of a working intelligent voice activated agent for a coffee shop which translates user intent 
into a list of CoffeeShop order items:

[![](/img/posts/typescript-typechat-examples/coffeeshop.png)](https://typechat.netcore.io/coffeeshop/)

:::{.my-8 .text-indigo-600 .text-center .text-xl}
[https://typechat.netcore.io/coffeeshop](https://typechat.netcore.io/coffeeshop)
:::

## All TypeChat Examples

To show the versatility of this approach we've implemented the [remaining TypeChat Examples](https://microsoft.github.io/TypeChat/docs/examples/)
in .NET which in addition to supporting 5 different Speech-to-text providers also supports utilizing a pure .NET
approach of generating TypeChat's prompt in C# and using [Semantic Kernel](https://servicestack.net/posts/semantic-kernel-gptmeetngs)
to connect with your preferred Chat GPT provider or utilizing node's TypeChat library to interface with ChatGPT where it
benefits from TypeScript schema validation and auto-retry of invalid responses with auto correcting prompts. 

But as we've discovered in [Custom Validation with C# Semantic Kernel](/posts/voice-activated-typechat-coffeeshop#custom-validation-with-c-semantic-kernel)
you can achieve more effective results with manual validation.

In addition all TypeChat examples supports uploading recordings to your preferred choice of **5 different Storage Providers**: 

- Local File System
- Google Cloud Storage
- Azure Blob Storage
- AWS S3
- Cloudflare R2

Feel free to explore to explore the different TypeChat examples for the implementation which best suits your use-case.

The source code for all Examples are maintained in a Combined App at:

:::{.my-8 .text-indigo-600 .text-center .text-xl}
[https://github.com/NetCoreApps/TypeChatExamples](https://github.com/NetCoreApps/TypeChatExamples)
:::

With a Live Demo of these examples available at:

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io](https://typechat.netcore.io)
:::

[![](/img/posts/typescript-typechat-examples/typechat.png)](https://typechat.netcore.io/)

### Individual Examples

If you're only interested in one of the examples for your use-case, simpler examples of each App is available in the
individual GitHub Repos below:

- [https://github.com/NetCoreApps/CoffeeShop](https://github.com/NetCoreApps/CoffeeShop)
- [https://github.com/NetCoreApps/SentimentTypeChat](https://github.com/NetCoreApps/SentimentTypeChat)
- [https://github.com/NetCoreApps/CalendarTypeChat](https://github.com/NetCoreApps/CalendarTypeChat)
- [https://github.com/NetCoreApps/RestaurantTypeChat](https://github.com/NetCoreApps/RestaurantTypeChat)
- [https://github.com/NetCoreApps/MathTypeChat](https://github.com/NetCoreApps/MathTypeChat)
- [https://github.com/NetCoreApps/MusicTypeChat](https://github.com/NetCoreApps/MusicTypeChat)

Descriptions, Screenshots and Links for each TypeChat Example is available below:

### Sentiment

A sentiment classifier which categorizes user input as negative, neutral, or positive. This is TypeChat's "hello world!"

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io/sentiment](https://typechat.netcore.io/sentiment)
:::

[![](/img/posts/typescript-typechat-examples/sentiment.png)](https://typechat.netcore.io/sentiment)
[![](/img/posts/typescript-typechat-examples/sentiment2.png)](https://typechat.netcore.io/sentiment)

### Calendar

An intelligent scheduler. This sample translates user intent into a sequence of actions to modify a calendar.

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io/calendar](https://typechat.netcore.io/calendar)
:::

[![](/img/posts/typescript-typechat-examples/calendar.png)](https://typechat.netcore.io/calendar)
[![](/img/posts/typescript-typechat-examples/calendar2.png)](https://typechat.netcore.io/calendar)

### Restaurant

An intelligent agent for taking orders at a restaurant. Similar to the coffee shop example, but uses a more complex schema 
to model more complex linguistic input. 

The prose files illustrate the line between simpler and more advanced language models in handling compound sentences, 
distractions, and corrections. This example also shows how we can use TypeScript to provide a user intent summary.

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io/restaurant](https://typechat.netcore.io/restaurant)
:::

[![](/img/posts/typescript-typechat-examples/restaurant.png)](https://typechat.netcore.io/restaurant)
[![](/img/posts/typescript-typechat-examples/restaurant2.png)](https://typechat.netcore.io/restaurant)

### Math

Translate calculations into simple programs given an API that can perform the 4 basic mathematical operators. 
This example highlights TypeChat's program generation capabilities.

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io/math](https://typechat.netcore.io/math)
:::

[![](/img/posts/typescript-typechat-examples/math.png)](https://typechat.netcore.io/math)
[![](/img/posts/typescript-typechat-examples/math2.png)](https://typechat.netcore.io/math)

### Music

An app for playing music, creating playlists, etc. on Spotify through natural language. 
Each user intent is translated into a series of actions in which correspond to a simple dataflow program, 
where each step can consume data produced from previous step.

If you Sign In with a **premium Spotify Account** you'll also be able to use natural language to control what you're listening to:  

:::{.my-8 .text-indigo-600 .text-center .text-2xl}
[https://typechat.netcore.io/music](https://typechat.netcore.io/music)
:::

[![](/img/posts/typescript-typechat-examples/music.png)](https://typechat.netcore.io/music)
[![](/img/posts/typescript-typechat-examples/music2.png)](https://typechat.netcore.io/music)


# Creating a Voice Activated CoffeeShop in .NET
Source: https://servicestack.net/posts/voice-activated-typechat-coffeeshop

In [Part 1 of our GPT CoffeeShop in .NET](/posts/building-typechat-coffeeshop-modelling) we looked at how we could easily
implement a TypeChat's CoffeeShop Application Database and Management UI which was able to dynamically generate TypeChat's
[coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts)
in .NET using:

 - [CoffeeShopPromptProvider.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceInterface/CoffeeShopPromptProvider.cs):
To create the `ScriptContext` containing all data and functionality our [#Script](https://sharpscript.net) is executed with
 - [/coffeeshop/schema.ss](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ss):
**#Script** Template that generates the TypeScript Schema

The result of which produces a functionally equivalent
[/coffeeshop/schema.ts](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ts) powered
by the data in CoffeeShop's Database.

## CoffeeShop .NET App

The purpose of which is to implement a CoffeeShop Menu which can fulfil ordering any of the products along with any 
toppings, customizations and preparation options listed by the App's database and resulting TypeScript Schema.

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="OtgrIdkvw-4" style="background-image: url('https://img.youtube.com/vi/OtgrIdkvw-4/maxresdefault.jpg')"></lite-youtube>
</div>

The resulting App implements a standard Web UI to navigate all categories and products offered in the CoffeeShop Menu,
that also supports an Audio Input letting customers order from the Menu using their **Voice** - which relies on Open AI's 
GPT services utilising Microsoft's [TypeChat](https://github.com/microsoft/TypeChat) schema prompt and node library
to convert a natural language order into a `Cart` order our App can understand, you can try at:

<h3 class="not-prose text-center pb-8">
    <a class="text-4xl text-blue-600 hover:underline" href="https://coffeeshop.netcore.io">https://coffeeshop.netcore.io</a>
</h3>

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-ui.png)](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-ui.png)
:::

<h3 class="not-prose text-center pb-8">
   <span class="text-gray-600 font-normal text-xl">
      source <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/CoffeeShop">https://github.com/NetCoreApps/CoffeeShop</a>
   </span> 
</h3>


The Web UI adopts the same build-free [Simple, Modern JavaScript](/posts/javascript) approach used to create blazor-vue's [Client Admin UI](/posts/admin-uis),
to create the CoffeeShop's reactive frontend UI utilizing progressive Vue.mjs, which is maintained in the
[Index.cshtml](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Pages/Index.cshtml) Home page.

This post explains how we implemented this functionality in .NET along with the different providers you can use to
transcribe audio recordings, save uploaded files and use GPT to convert natural language to order item requests
we can add to our cart.

### Implementing Voice Command Customer Orders

Before we're able to enlist Chat GPT to do our bidding for us, we need to capture our Customer's CoffeeShop Order
Voice Recording and transcribe it to text.

We have a few options when it comes to real-time **Speech-to-Text** translation services you can choose from, including:

 - [Google Cloud Speech-to-Text](https://cloud.google.com/speech-to-text/) - Starts from **$0.016** /minute or **$0.012** 
if you're ok with Google to use your recordings to improve their AI models
 - [Amazon Transcribe](https://aws.amazon.com/transcribe/) - Starts from **$0.024** /minute
 - [Azure AI Speech](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) - Starts from **$0.0167** /minute
 - [OpenAI Whisper API](https://openai.com/research/whisper) - Flat rate of **$0.006** /minute for using their online API
 - [OpenAI Whisper Local](https://github.com/openai/whisper) - Whisper is also available as an OSS project you can run yourself
if you prefer to manage your own servers and local Whisper install

## ServiceStack.AI Providers

As the AI landscape is actively changing we want our Apps to be able to easily switch to different Speech-to-text providers
so we're able to evaluate and use the best provider for each use-case.

To support this we're maintaining **FREE** implementation-agnostic abstractions for different AI and GPT Providers to enable 
AI features in .NET Apps under the new [ServiceStack.AI](https://github.com/ServiceStack/ServiceStack/tree/main/ServiceStack/src/ServiceStack.Interfaces/AI)
namespace in our dependency-free **ServiceStack.Interfaces** package.

Where the implementations for these abstractions are maintained across the following NuGet packages according to their
required dependencies:

- `ServiceStack.Aws` - AI & GPT Providers for Amazon Web Services
- `ServiceStack.Azure` - AI & GPT Providers for Microsoft Azure
- `ServiceStack.GoogleCloud` - AI & GPT Providers for Google Cloud
- `ServiceStack.AI` - AI & GPT Providers for OpenAI APIs and local Whisper and Node TypeChat installs

These free abstractions and implementations allow .NET project's to decouple their **Speech-to-text** or **ChatGPT**
requirements from any single implementation where they can be easily substituted.

You're welcome to submit feature requests for other providers you'd like to see at:

<h3 class="not-prose text-center pb-8">
    <a class="text-3xl text-blue-600 hover:underline" href="https://servicestack.net/ideas">https://servicestack.net/ideas</a>
</h3>

### ISpeechToText

The `ISpeechToText` interface abstracts Speech-to-text services behind a simple API:

```csharp
public interface ISpeechToText
{
    // Once only task to run out-of-band before using the SpeechToText provider
    Task InitAsync(List<string> phrases, CancellationToken token = default);
    
    // Transcribe the Audio at recordingPath and return a JSON API Result
    Task<TranscriptResult> TranscribeAsync(string request, CancellationToken token);
}
```

As of this of this writing CoffeeShop supports using 5 different Speech-to-text providers, defaulting to using
Open AI's Whisper API:

 - `GoogleCloudSpeechToText` - to use Google Cloud's [Speech-to-Text v2](https://cloud.google.com/speech-to-text/v2/) API 
 - `AwsSpeechToText` - to use [Amazon Transcribe](https://aws.amazon.com/pm/transcribe/) API
 - `AzureSpeechToText` - to use [Azure Speech to text](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) API
 - `WhisperApiSpeechToText` - to use [OpenAI's Whisper](https://platform.openai.com/docs/api-reference/audio) API
 - `WhisperLocalSpeechToText` - to use local install of [OpenAI Whisper](https://github.com/openai/whisper)

CoffeeShop can use any of these providers by configuring it in [appsettings.json](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/appsettings.json):

```json
{
  "SpeechProvider": "WhisperApiSpeechToText"
}
```

Where all available providers are configured in
[Configure.Speech.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.Speech.cs):

```csharp
var speechProvider = context.Configuration.GetValue<string>("SpeechProvider");
if (speechProvider == nameof(GoogleCloudSpeechToText))
{
    services.AddSingleton<ISpeechToText>(c => {
        var config = c.Resolve<AppConfig>();
        var gcp = c.Resolve<GoogleCloudConfig>();
        return new GoogleCloudSpeechToText(SpeechClient.Create(),
            gcp.ToSpeechToTextConfig(x => {
                x.PhraseSetId = config.CoffeeShop.PhraseSetId;
                x.RecognizerId = config.CoffeeShop.RecognizerId;
            })
        );
    });
}
else if (speechProvider == nameof(AwsSpeechToText))
{
    services.AddSingleton<ISpeechToText>(c => {
        var config = c.Resolve<AppConfig>();
        var aws = c.Resolve<AwsConfig>();
        return new AwsSpeechToText(new AmazonTranscribeServiceClient(
                aws.AccessKey, aws.SecretKey, aws.ToRegionEndpoint()),
            aws.ToSpeechToTextConfig(x => 
                x.VocabularyName = config.CoffeeShop.VocabularyName));
    });
}
else if (speechProvider == nameof(AzureSpeechToText))
{
    services.AddSingleton<ISpeechToText>(c => {
        var azure = c.Resolve<AzureConfig>();
        return new AzureSpeechToText(azure.ToSpeechConfig());
    });
}
else if (speechProvider == nameof(WhisperApiSpeechToText))
{
    services.AddSingleton<ISpeechToText, WhisperApiSpeechToText>();
}
else if (speechProvider == nameof(WhisperLocalSpeechToText))
{
    services.AddSingleton<ISpeechToText>(c => new WhisperLocalSpeechToText {
        WhisperPath = c.Resolve<AppConfig>().WhisperPath,
        TimeoutMs = c.Resolve<AppConfig>().NodeProcessTimeoutMs,
    });
}
```

### CoffeeShop OpenAI Defaults

To support a minimal infrastructure dependency solution the [CoffeeShop](https://github.com/NetCoreApps/CoffeeShop) GitHub Repo 
is configured by default to use OpenAI's Whisper API to transcribe recordings that are persisted to local disk. 

Which at a minimum to use CoffeeShop's text or voice activated commands needs the `OPENAI_API_KEY` Environment Variable 
configured with your [OpenAI API Key](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).

Alternatively use `WhisperLocalSpeechToText` with a [local install of OpenAI Whisper](https://github.com/openai/whisper)
available from your `$PATH` or `WhisperPath` configured in **appsettings.json**

### coffeeshop.netcore.io uses GoogleCloud

As we want to upload and store our Customer voice recordings in managed storage (and be able to later measure their effectiveness)
we decided to use **Google Cloud** since it offers the best value. 

It also offers the best latency when using managed storage as when configured to use [GoogleCloud Storage](https://cloud.google.com/storage),
audio recordings only need to upload it once as they can be referenced directly by its Speech-to-text APIs.

To use any of the Google Cloud providers your pc needs to be configured with [GoogleCloud Credentials](https://cloud.google.com/speech-to-text/docs/before-you-begin) on a project 
with Speech-to-Text enabled.

## Virtual File System

By default CoffeeShop is configured to upload its recordings to the local file system, this can be changed to upload
to your preferred [Virtual File System](https://docs.servicestack.net/virtual-file-system) provider:

 - `FileSystemVirtualFiles` - stores uploads in local file system (default)
 - `GoogleCloudVirtualFiles` - stores uploads in Google Cloud Storage
 - `S3VirtualFiles` - stores uploads in AWS S3
 - `AzureBlobVirtualFiles` - stores uploads in Azure Blob Storage
 - `R2VirtualFiles` - stores uploads in Cloudflare R2

By configuring **VfsProvider** in **appsettings.json**, e.g:

```json
{
  "VfsProvider": "GoogleCloudVirtualFiles"
}
```

Which are configured in [Configure.Vfs.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.Vfs.cs):

```csharp
var vfsProvider = appHost.AppSettings.Get<string>("VfsProvider");
if (vfsProvider == nameof(GoogleCloudVirtualFiles))
{
    GoogleCloudConfig.AssertValidCredentials();
    appHost.VirtualFiles = new GoogleCloudVirtualFiles(
        StorageClient.Create(), appHost.Resolve<GoogleCloudConfig>().Bucket!);
}
else if (vfsProvider == nameof(S3VirtualFiles))
{
    var aws = appHost.Resolve<AwsConfig>();
    appHost.VirtualFiles = new S3VirtualFiles(new AmazonS3Client(
        aws.AccessKey,
        aws.SecretKey,
        aws.ToRegionEndpoint()), aws.Bucket);
}
else if (vfsProvider == nameof(R2VirtualFiles))
{
    var r2 = appHost.Resolve<CloudflareConfig>();
    appHost.VirtualFiles = new R2VirtualFiles(new AmazonS3Client(
        r2.AccessKey,
        r2.SecretKey,
        new AmazonS3Config {
            ServiceURL = r2.ToServiceUrl(),
        }), r2.Bucket);
}
else if (vfsProvider == nameof(AzureBlobVirtualFiles))
{
    var azure = appHost.Resolve<AzureConfig>();
    appHost.VirtualFiles = new AzureBlobVirtualFiles(
        azure.ConnectionString, azure.ContainerName);
}
```

### Managed File Uploads

The File Uploads themselves are managed by the [Managed File Uploads](https://docs.servicestack.net/locode/files-upload-filesystem)
feature which uses the configuration below to only allows uploading **recordings**:

 - For known **Web Audio** File Types
 - Accepts uploads by **anyone**
 - A maximum size of **1MB**

Whose file upload location is derived from when it was uploaded:

```csharp
Plugins.Add(new FilesUploadFeature(
    new UploadLocation("recordings", VirtualFiles, allowExtensions:FileExt.WebAudios, writeAccessRole: RoleNames.AllowAnon,
        maxFileBytes: 1024 * 1024,
        transformFile: ctx => ConvertAudioToWebM(ctx.File),
        resolvePath: ctx => $"/recordings/{ctx.GetDto<IRequireFeature>().Feature}/{ctx.DateSegment}/{DateTime.UtcNow.TimeOfDay.TotalMilliseconds}.{ctx.FileExtension}")
));
```

## Transcribe Audio Recording API

This feature allows the `CreateRecording` [AutoQuery CRUD API](https://docs.servicestack.net/autoquery/crud) 
to declaratively support file uploads with the `[UploadTo("recordings")]` attribute: 

```csharp
public class CreateRecording : ICreateDb<Recording>, IReturn<Recording>
{
    [ValidateNotEmpty]
    public string Feature { get; set; }

    [Input(Type="file"), UploadTo("recordings")]
    public string Path { get; set; }
}
```

Where it will be uploaded to the `VirtualFiles` provider configured in the **recordings** file `UploadLocation`.

As we want the same API to also transcribe the recording, we've implemented a [Custom AutoQuery implementation](https://docs.servicestack.net/autoquery/crud#custom-autoquery-crud-implementation) in 
[GptServices.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceInterface/GptServices.cs)
that after creating the `Recording` entry with a populated relative `Path` of where the Audio file was uploaded to, 
calls `ISpeechToText.TranscribeAsync()` to kick off the recording transcription request with the configured Speech-to-text provider. 

After it completes its JSON Response is then added to the `Recording` row and saved to the configured VirtualFiles provider before
being returned in the API Response:

```csharp
public class GptServices : Service
{
    //...
    public IAutoQueryDb AutoQuery { get; set; }
    public ISpeechToText SpeechToText { get; set; }
    
    public async Task<object> Any(CreateRecording request)
    {
        var feature = request.Feature.ToLower();
        var recording = (Recording)await AutoQuery.CreateAsync(request, Request);

        var transcribeStart = DateTime.UtcNow;
        await Db.UpdateOnlyAsync(() => new Recording { TranscribeStart = transcribeStart },
            where: x => x.Id == recording.Id);

        ResponseStatus? responseStatus = null;
        try
        {
            var response = await SpeechToText.TranscribeAsync(request.Path);
            var transcribeEnd = DateTime.UtcNow;
            await Db.UpdateOnlyAsync(() => new Recording
            {
                Feature = feature,
                Provider = SpeechToText.GetType().Name,
                Transcript = response.Transcript,
                TranscriptConfidence = response.Confidence,
                TranscriptResponse = response.ApiResponse,
                TranscribeEnd = transcribeEnd,
                TranscribeDurationMs = (int)(transcribeEnd - transcribeStart).TotalMilliseconds,
                Error = response.ResponseStatus.ToJson(),
            }, where: x => x.Id == recording.Id);
            responseStatus = response.ResponseStatus;
        }
        catch (Exception e)
        {
            await Db.UpdateOnlyAsync(() => new Recording { Error = e.ToString() },
                where: x => x.Id == recording.Id);
            responseStatus = e.ToResponseStatus();
        }

        recording = await Db.SingleByIdAsync<Recording>(recording.Id);

        WriteJsonFile($"/speech-to-text/{feature}/{recording.CreatedDate:yyyy/MM/dd}/{recording.CreatedDate.TimeOfDay.TotalMilliseconds}.json", 
            recording.ToJson());

        if (responseStatus != null)
            throw new HttpError(responseStatus, HttpStatusCode.BadRequest);

        return recording;
    }
}
```

### Capturing and Uploading Web Audio Recordings

Now that our Server supports it we can start using this API to upload Audio Recordings. To simplify reuse we've 
encapsulated Web Audio API usage behind the [AudioRecorder.mjs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/wwwroot/mjs/AudioRecorder.mjs)
class whose `start()` method starts recording Audio from the Users microphone:

```js
import { AudioRecorder } from "/mjs/AudioRecorder.mjs"

let audioRecorder = new AudioRecorder()
await audioRecorder.start()
```

Where it captures audio chunks until `stop()` is called that are then stitched together into a `Blob` and converted into
a Blob DataURL that's returned within a populated [Audio](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement)
Media element:

```js
const audio = await audioRecorder.stop()
```

That supports the [HTMLMediaElement](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement#instance_methods) API
allowing pause and playback of recordings:

```js
audio.play()
audio.pause()
```

The `AudioRecorder` also maintains the `Blob` of its latest recording in its `audioBlob` field and the **MimeType** that it was
captured with in `audioExt` field, which we can use to upload it to the `CreateRecording` API, which if 
successful will return a transcription of the Audio recording: 

```js
import { JsonApiClient } from "@servicestack/client"

const client = JsonApiClient.create()

const formData = new FormData()
formData.append('path', audioRecorder.audioBlob, `file.${audioRecorder.audioExt}`)
const api = await client.apiForm(new CreateRecording({feature:'coffeeshop'}),formData)
if (api.succeeded) {
   transcript.value = api.response.transcript
}
```

## TypeChat API

Now that we have a transcript of the Customers recording we need to enlist the services of Chat GPT to convert it into 
an Order request that our App can understand. 

### ITypeChat

Just as we've abstracted the Transcription Services our App binds to, we also want to abstract the TypeChat provider
our App uses so we can easily swap out and evaluate different solutions:

```csharp
public interface ITypeChat
{
    Task<TypeChatResponse> TranslateMessageAsync(TypeChatRequest request, 
        CancellationToken token = default);
}
```

Whilst a simple API on the surface, different execution and customizations options are available in the 
`TypeChatRequest` which at a minimum requires the **Schema** & **Prompt** to use and the **UserMessage** to convert:

```csharp
// Request to process a TypeChat Request
public class TypeChatRequest
{
    public TypeChatRequest(string schema, string prompt, string userMessage)
    {
        Schema = schema;
        Prompt = prompt;
        UserMessage = userMessage;
    }

    /// TypeScript Schema
    public string Schema { get; set; }
    
    /// TypeChat Prompt
    public string Prompt { get; set; }
    
    /// Chat Request
    public string UserMessage { get; }
    
    /// Path to node exe (default node in $PATH)
    public string? NodePath { get; set; }

    /// Timeout to wait for node script to complete (default 120s)
    public int NodeProcessTimeoutMs { get; set; } = 120 * 1000;

    /// Path to node TypeChat script (default typechat.mjs)
    public string? ScriptPath { get; set; }
    
    /// TypeChat Behavior we want to use (Json | Program)
    public TypeChatTranslator TypeChatTranslator { get; set; }

    /// Path to write TypeScript Schema to (default Temp File)
    public string? SchemaPath { get; set; }
    
    /// Which directory to execute the ScriptPath (default CurrentDirectory) 
    public string? WorkingDirectory { get; set; }
}
```

### IPromptProvider

Whilst App's can use whichever strategy they prefer to generate their TypeScript `Schema` and Chat GPT `Prompt` texts, 
they can still benefit from implementing the same `IPromptProvider` interface to improve code reuse which should contain 
the App-specific functionality for creating its **TypeScript Schema** and **GPT Prompt** from a `TypeChatRequest`, 
which CoffeeShop implements in 
[CoffeeShopPromptProvider.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceInterface/CoffeeShopPromptProvider.cs):

```csharp
// The App Provider to use to generate TypeChat Schema and Prompts
public interface IPromptProvider
{
    // Create a TypeChat TypeScript Schema from a TypeChatRequest
    Task<string> CreateSchemaAsync(CancellationToken token);

    // Create a TypeChat TypeScript Prompt from a User request
    Task<string> CreatePromptAsync(string userMessage, CancellationToken token);
}
```

### Semantic Kernel TypeChat Provider

The natural approach for interfacing with OpenAI's ChatGPT API in .NET is to use [Microsoft's Semantic Kernel](https://github.com/microsoft/semantic-kernel) 
to call it directly, that CoffeeShop can be configured with by specifying to use `KernelTypeChat` provider in **appsettings.json**:

```json
{
  "TypeChatProvider": "KernelTypeChat"
}
```

Which registers the `KernelTypeChat` in [Configure.Gpt.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.Gpt.cs)

```csharp
var kernel = Kernel.Builder.WithOpenAIChatCompletionService(
        Environment.GetEnvironmentVariable("OPENAI_MODEL") ?? "gpt-3.5-turbo", 
        Environment.GetEnvironmentVariable("OPENAI_API_KEY")!)
    .Build();
services.AddSingleton(kernel);
services.AddSingleton<ITypeChat>(c => new KernelTypeChat(c.Resolve<IKernel>()));
```

## Using Chat GPT to process Natural Language Orders

We now have everything we need to start leveraging Chat GPT to convert our Customers **Natural Language** requests 
into Machine readable instructions that our App can understand, guided by TypeChat's TypeScript Schema.

The API to do this needs only a single property to capture the Customer request, that's either provided from a 
free-text text input or a Voice Input captured by Web Audio:  

```csharp
public class CreateChat : ICreateDb<Chat>, IReturn<Chat>
{
    [ValidateNotEmpty]
    public string Feature { get; set; }

    public string UserMessage { get; set; }
}
```

That just like `CreateRecording` is a custom AutoQuery CRUD Service that uses AutoQuery to create the 
initial `Chat` record, that's later updated with the GPT Chat API Response, executed from the configured
`ITypeChat` provider:

```csharp
public class GptServices : Service
{
    //...
    public IAutoQueryDb AutoQuery { get; set; }
    public IPromptProvider PromptProvider { get; set; }
    public ITypeChat TypeChatProvider { get; set; }
    
    public async Task<object> Any(CreateChat request)
    {
        var feature = request.Feature.ToLower();
        var promptProvider = PromptFactory.Get(feature);
        var chat = (Chat)await AutoQuery.CreateAsync(request, Request);

        var chatStart = DateTime.UtcNow;
        await Db.UpdateOnlyAsync(() => new Chat { ChatStart = chatStart },
            where: x => x.Id == chat.Id);

        ResponseStatus? responseStatus = null;
        try
        {
            var schema = await promptProvider.CreateSchemaAsync();
            var prompt = await promptProvider.CreatePromptAsync(request.UserMessage);
            var typeChatRequest = CreateTypeChatRequest(feature, schema, prompt, request.UserMessage);
            
            var response = await TypeChat.TranslateMessageAsync(typeChatRequest);
            var chatEnd = DateTime.UtcNow;
            await Db.UpdateOnlyAsync(() => new Chat
            {
                Request = request.UserMessage,
                Feature = feature,
                Provider = TypeChat.GetType().Name,
                Schema = schema,
                Prompt = prompt,
                ChatResponse = response.Result,
                ChatEnd = chatEnd,
                ChatDurationMs = (int)(chatEnd - chatStart).TotalMilliseconds,
                Error = response.ResponseStatus.ToJson(),
            }, where: x => x.Id == chat.Id);
            responseStatus = response.ResponseStatus;
        }
        catch (Exception e)
        {
            await Db.UpdateOnlyAsync(() => new Chat { Error = e.ToString() },
                where: x => x.Id == chat.Id);
            responseStatus = e.ToResponseStatus();
        }

        chat = await Db.SingleByIdAsync<Chat>(chat.Id);
        
        WriteJsonFile($"/chat/{feature}/{chat.CreatedDate:yyyy/MM/dd}/{chat.CreatedDate.TimeOfDay.TotalMilliseconds}.json", chat.ToJson());

        if (responseStatus != null)
            throw new HttpError(responseStatus, HttpStatusCode.BadRequest);
        
        return chat;
    }
}
```

The API then returns Chat GPTs JSON Response directly to the client:  

```js
apiChat.value = await client.api(new CreateChat({
    feature: 'coffeeshop',
    userMessage: request.toLowerCase()
}))

if (apiChat.value.response) {
    processChatItems(JSON.parse(apiChat.value.response.chatResponse).items)
} else if (apiChat.value.error) {
    apiProcessed.value = apiChat.value
}
```

That's in the structure of the TypeScript Schema's array of `Cart` LineItem's which are matched against the
products and available customizations from the App's database before being added to the user's cart in the 
[processChatItems(items)](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Pages/Index.cshtml#L507) function.

### Trying it Out

Now that all the pieces are connected together, we can finally try it out! Hit the **Record** Icon to start the 
microphone recording to capture CoffeeShop Orders via Voice Input. 

Let's try a Natural Language Customer Order whose free-text input would be notoriously difficult to parse with traditional 
programming methods:  

> two tall lattes, the first one with no foam, the second one with whole milk. actually, make the first one a grande.

After the configured `ISpeechToText` provider has finished transcribing the uploaded Voice Recording it will display
the transcribed text whilst the request is being processed by Chat GPT:  

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-natural-language-1.png)](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-natural-language-1.png)
:::

Then if all was successful we should see the Customer Order appear in the Customer's Cart, as if by magic!

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-natural-language-2.png)](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-natural-language-2.png)
:::

### Semantic Kernel Effectiveness

Most times you'll get the same desirable results when using Semantic Kernel to call Chat GPT from C# compared with
calling Chat GPT through node TypeChat as both are sending the same prompt to Chat GPT. 

Where they begin to differ is how they deal with invalid and undesirable responses from vague or problematic requests.

Lets take a look at an order from [TypeChat's input's](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/input.txt)
that Chat GPT had issues with: 

> i wanna latte macchiato with vanilla

That our Semantic Kernel Request translates into a reasonable Cart Item order:

```json
{
  "items": [
    {
      "type": "lineitem",
      "product": {
        "type": "LatteDrinks",
        "name": "latte macchiato",
        "options": [
          {
            "type": "Syrups",
            "name": "vanilla"
          }
        ]
      },
      "quantity": 1
    }
  ]
}
```

The problem being **"vanilla"** is close, but it's not an **exact match** for a **Syrup** on offer, so our order only gets partially filled:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-kernel-vanilla-latte-macchiato.png)](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-kernel-vanilla-latte-macchiato.png)
:::

### TypeChat's Invalid Response Handling

So how does TypeChat fair when handling the same problematic prompt?

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/typechat-failed-vanilla-latte-macchiato.png)](/img/posts/building-typechat-coffeeshop-modelling/typechat-failed-vanilla-latte-macchiato.png)
:::

It expectedly fails in the same way, but as it's able to validate JSON responses against the TypeScript's schema it 
knows and can report back where the failure occurs through the TypeScript's compiler schema validation errors.

### TypeChat's Auto Retry of Failed Requests

Interestingly before reporting back the schema validation error TypeChat had transparently sent a **retry** request back 
to Chat GPT with the original prompt with some additional context to help guide GPT into returning a valid response 
where it includes the invalid response it had received, an error message to say why it was invalid and the Schema
Validation Error:

```
//...
{
  "items": [
    {
      "type": "lineitem",
      "product": {
        "type": "LatteDrinks",
        "name": "latte macchiato",
        "options": [
          {
            "type": "Syrups",
            "name": "vanilla"
          }
        ]
      },
      "quantity": 1
    }
  ]
}
The JSON object is invalid for the following reason:
"""
Type '"vanilla"' is not assignable to type '"butter" | "strawberry jam" | "cream cheese" | "regular" | "warmed" | "cut in half" | "whole milk" | "two percent milk" | "nonfat milk" | "coconut milk" | "soy milk" | "almond milk" | ... 42 more ... | "heavy cream"'.
"""
The following is a revised JSON object:
```

Unfortunately in this case the schema validation error is incomplete and doesn't include the valid values for
the `Syrups` type, so ChatGPT 3.5 never gets it right.

### Custom Validation with C# Semantic Kernel

The benefit of TypeScript Schema validation errors are that they require little effort since they're automatically 
generated by TypeScript's compiler. Since TypeChat isn't able to resolve the issue in this case, can we do better?

As we already need to perform our own validation to match GPT's `Cart` response back to our database products, we
can construct our own auto-correcting prompt by specifying what was wrong and repeating the definition of the failed
`Syrups` type, e.g:

``` 
//...
JSON validation failed: 'vanilla' is not a valid name for the type: Syrups

export interface Syrups {
    type: 'Syrups';
    name: 'almond syrup' | 'buttered rum syrup' | 'caramel syrup' | 'cinnamon syrup' | 'hazelnut syrup' | 
        'orange syrup' | 'peppermint syrup' | 'raspberry syrup' | 'toffee syrup' | 'vanilla syrup';
    optionQuantity?: OptionQuantity;
}
```

Lo and behold we get what we're after, an expected valid response with the correct **"vanilla syrup"** value: 

```json
{
  "items": [
    {
      "type": "lineitem",
      "product": {
        "type": "LatteDrinks",
        "name": "latte macchiato",
        "options": [
          {
            "type": "Syrups",
            "name": "vanilla syrup"
          }
        ]
      },
      "quantity": 1
    }
  ]
}
```

This goes to show that interfacing with ChatGPT in C# with Semantic Kernel is a viable solution that whilst requires
more bespoke logic and manual validation, can produce a more effective response than what's possible with TypeChat's schema
validation errors.

If you need to maintain few interfaces with ChatGPT, building your solution all in .NET might be the best option, but if
you need to generate and maintain multiple schemas then using a generic library with automated retries like TypeChat utilizing 
TypeScript Schema validation errors is likely preferred.

### TypeChat with ChatGPT 4

As we're not able to achieve a successful response from this problematic request from GPT-3.5, would ChatGPT 4 fair
any better?

Lets try by setting the `OPENAI_MODEL` Environment Variable to **gpt-4** in your shell:

:::sh
set OPENAI_MODEL=gpt-4
:::

Then retry the same request:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/typechat-success-vanilla-latte-macchiato.png)](/img/posts/building-typechat-coffeeshop-modelling/typechat-success-vanilla-latte-macchiato.png)
:::

Success! GPT-4 didn't even need the retry and gets it right in one-shot on its first try.

So the easiest way to improve the success rate of your TypeChat GPT solutions is to switch to GPT-4, or for a more cost
effective approach you can start with GPT-3.5 than retry failed requests with GPT-4.

## Node TypeChat Provider

As TypeChat uses **typescript** we'll need to call out to the **node** executable in order to be able to use it from
our .NET App.

To configure our App to talk to ChatGPT API via TypeChat we need to specify to use `NodeTypeChat` provider in **appsettings.json**:

```json
{
  "TypeChatProvider": "NodeTypeChat"
}
```

Which will configure to use the `NodeTypeChat` in [Configure.Gpt.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.Gpt.cs)

```csharp
services.AddSingleton<ITypeChat>(c => new NodeTypeChat());
```

It works by executing an **external process** that invokes our custom
[typechat.mjs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/typechat.mjs) wrapper around TypeChat's
functionality to invoke it and return any error responses in a structured `ResponseStatus` format that our .NET App 
can understand, which you can also invoke manually from the command line with:

:::sh
node typechat.mjs json gpt\coffeeshop\schema.ts "i wanna latte macchiato with vanilla"
:::

Lets test this out this with an `OPENAI_MODEL=gpt-4` Environment Variable so we can see the above problematic prompt 
working in our App:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-nodegpt4-vanilla-latte-macchiato.png)](/img/posts/building-typechat-coffeeshop-modelling/coffeeshop-nodegpt4-vanilla-latte-macchiato.png)
:::

With everything connected and working together, try node's TypeChat out by ordering something from the CoffeeShop Menu or
try out some of TypeChat's [input.txt](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/input.txt) examples for inspiration.

## Try Alternative Providers

Feel free to try and evaluate different providers to measure how well each performs, if you prefer a managed cloud option 
switch to use GoogleCloud to store voice recordings and transcribe audio by configuring **appsettings.json** with:

```json
{
  "VfsProvider": "GoogleCloudVirtualFiles",
  "SpeechProvider": "GoogleCloudSpeechToText"
}
```

Using GoogleCloud Services requires your workstation to be configured with [GoogleCloud Credentials](https://cloud.google.com/speech-to-text/docs/before-you-begin).

A nice feature from using Cloud Services is the built-in tooling in IDEs like JetBrains 
[Big Data Tools](https://plugins.jetbrains.com/plugin/12494-big-data-tools) where you can inspect new Recordings and ChatGPT
JSON responses from within your IDE, instead of SSH'ing into remote servers to inspect local volumes: 

:::{.max-w-sm .mx-auto}
[![](/img/posts/building-typechat-coffeeshop-modelling/googlcloud-plugin.png)](/img/posts/building-typechat-coffeeshop-modelling/googlcloud-plugin.png)
:::

### Workaround for Safari

CoffeeShop worked great in all modern browsers we tested on in Windows including: Chrome, Edge, Firefox and Vivaldi but
unfortunately failed in Safari which seemed strange since we're using [WebM](https://en.wikipedia.org/wiki/WebM):

> the open, royalty-free, media file format designed for the web!

So much for "open formats", guess we'll have to switch to the format that all browsers support.
This information isn't readily available so I used this little script to detect which popular formats are supported in each browser:  

```js
const containers = ['webm', 'ogg', 'mp4', 'x-matroska', '3gpp', '3gpp2', 
                    '3gp2', 'quicktime', 'mpeg', 'aac', 'flac', 'wav']
const codecs = ['vp9', 'vp8', 'avc1', 'av1', 'h265', 'h.265', 'h264',             
                'h.264', 'opus', 'pcm', 'aac', 'mpeg', 'mp4a'];

const supportedAudios = containers.map(format => `audio/${format}`)
  .filter(mimeType => MediaRecorder.isTypeSupported(mimeType))
const supportedAudioCodecs = supportedAudios.flatMap(audio => 
  codecs.map(codec => `${audio};codecs=${codec}`))
      .filter(mimeType => MediaRecorder.isTypeSupported(mimeType))

console.log('Supported Audio formats:', supportedAudios)
console.log('Supported Audio codecs:', supportedAudioCodecs)
```

### Windows

All Chrome and Blink-based browsers inc. Edge and Vivaldi reported the same results:

```
Supported Audio formats: ['audio/webm']
Supported Audio codecs: ['audio/webm;codecs=opus', 'audio/webm;codecs=pcm']
```

Whilst Firefox reported:

```
Supported Audio formats: ["audio/webm", "audio/ogg"]
Supported Audio codecs: ["audio/webm;codecs=opus", "audio/ogg;codecs=opus"]
```

Things aren't looking good, we have exactly one shared Audio Format and Codec supported by all modern browsers on Windows.

### macOS

What does Safari say it supports?

```
Supported Audio formats: ["audio/mp4"]
Supported Audio codecs: ["audio/mp4;codecs=avc1", "audio/mp4;codecs=mp4a"]
```

That's unfortunate, The only Audio format Safari supports isn't supported by any other browser, even worse it's not an audio
encoding that [Google Speech-to-text supports](https://cloud.google.com/speech-to-text/docs/encoding). 

This means if we want to be able to serve Customers with shiny iPhone's we're going to need to convert it into a format
that the Speech-to-text APIs accepts, the logical choice being to use the same format that we're capturing from other
browsers natively, i.e. `audio/webm`.

### ffmpeg

Our first attempt to do this in .NET with [NAudio](https://github.com/naudio/NAudio) failed on macOS since it relied
on Windows APIs to perform the conversion.

Luckily this is pretty easy to do with the amazingly versatile [ffmpeg](https://ffmpeg.org) - that can be installed on macOS with
[Homebrew](https://brew.sh):

:::sh
brew install ffmpeg
:::

Whilst CoffeeShop's Ubuntu 22.04 [Dockerfile](https://github.com/NetCoreApps/CoffeeShop/blob/main/Dockerfile) it uses 
to deploy to Linux installs it with:

```dockerfile
RUN apt-get clean && apt-get update && apt-get upgrade -y \
    && apt-get install -y --no-install-recommends curl gnupg ffmpeg
```

### Converting Uploaded Files

We now need to integrate this conversion into our workflow. As we're using 
[Managed Files Uploads](https://docs.servicestack.net/locode/files-overview) to handle our uploads, the best place to
add support for it is in `transformFile` where you can intercept the file upload and transform it to a more suitable
upload for your App to work with, by configuring the `UploadLocation` in 
[Configure.AppHost.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.AppHost.cs):

```csharp
Plugins.Add(new FilesUploadFeature(
    new UploadLocation("recordings", VirtualFiles, allowExtensions:FileExt.WebAudios, writeAccessRole: RoleNames.AllowAnon,
        maxFileBytes: 1024 * 1024,
        transformFile: ctx => ConvertAudioToWebM(ctx.File),
        resolvePath: ctx => $"/recordings/{ctx.DateSegment}/{DateTime.UtcNow.TimeOfDay.TotalMilliseconds}.{ctx.FileExtension}")
));
```

Where the `ConvertAudioToWebM` handles converting Safari's **.mp4** Audio recordings into **.webm** format by executing 
the external **ffmpeg** process, the converted file is then returned in a new `HttpFile` referencing the new **.webm** Stream contents:

```csharp
public async Task<IHttpFile?> ConvertAudioToWebM(IHttpFile file)
{
    if (!file.FileName.EndsWith("mp4")) 
        return file;
    
    var ffmpegPath = ProcessUtils.FindExePath("ffmpeg") 
        ?? throw new Exception("Could not resolve path to ffmpeg");
    
    var now = DateTime.UtcNow;
    var time = $"{now:yyyy-M-d_s.fff}";
    var tmpDir = Environment.CurrentDirectory.CombineWith("App_Data/tmp").AssertDir();
    var tmpMp4 = tmpDir.CombineWith($"{time}.mp4");
    await using (File.Create(tmpMp4)) {}
    var tmpWebm = tmpDir.CombineWith($"{time}.webm");
    
    var msMp4 = await file.InputStream.CopyToNewMemoryStreamAsync();
    await using (var fsMp4 = File.OpenWrite(tmpMp4))
    {
        await msMp4.WriteToAsync(fsMp4);
    }
    await ProcessUtils.RunShellAsync($"{ffmpegPath} -i {tmpMp4} {tmpWebm}");
    File.Delete(tmpMp4);
    
    HttpFile? to = null;
    await using (var fsWebm = File.OpenRead(tmpWebm))
    {
        to = new HttpFile(file) {
            FileName = file.FileName.WithoutExtension() + ".webm",
            InputStream = await fsWebm.CopyToNewMemoryStreamAsync()
        };
    }
    File.Delete(tmpWebm);

    return to;
}
```

In the terminal this simply done by specifying the **file.mp4** input file and the output file extension and format
you want it converted to:

:::sh
ffmpeg -i file.mp4 file.webm
:::

With this finishing touch CoffeeShop is now accepting Customer Orders from all modern browsers in all major Operating Systems at:

<h3 class="not-prose text-center pb-8">
    <a class="text-4xl text-blue-600 hover:underline" href="https://coffeeshop.netcore.io">https://coffeeshop.netcore.io</a>
</h3>

## Local OpenAI Whisper

CoffeeShop also serves as nice real-world example we can use to evaluate the effectiveness of different GPT models,
which has been surprisingly [effortless on Apple Silicon](https://servicestack.net/posts/postgres-mysql-sqlserver-on-apple-silicon) 
whose great specs and popularity amongst developers means a lot of the new GPT projects are well supported on my new M2 Macbook Air.

As we already have **ffmpeg** installed, installing [OpenAI's Whisper](https://github.com/openai/whisper) can be done with:

:::sh
pip install -U openai-whisper
:::

Where you'll then be able to transcribe Audio recordings that's as easy as specifying the recording you want a transcription of:

:::sh
whisper recording.webm
:::

#### Usage Notes

 - The `--language` flag helps speed up transcriptions by avoiding needing to run auto language detection
 - By default whisper will generate its transcription results in all supported `.txt`, `.json`, `.tsv`, `.srt` and `.vtt` formats
   - you can limit to just the format you want it in with `--output_format`, e.g. use `txt` if you're just interested in the transcribed text
 - The default install also had [FP16](https://github.com/openai/whisper/discussions/301) and 
[Numba Deprecation](https://github.com/openai/whisper/discussions/1344) warnings

All these issues were resolved by using the modified prompt: 

```bash
export PYTHONWARNINGS="ignore"
whisper --language=en --fp16 False --output_format txt recording.webm
```

Which should now generate a clean output containing the recordings transcribed text, that's also written to `recording.txt`:

```
[00:00.000 --> 00:02.000]  A latte, please.
```

Which took **9 seconds** to transcribe on my M2 Macbook Air, a fair bit longer than the **1-2 seconds** it takes to upload
and transcribe recordings to Google Cloud, but still within acceptable response times for real-time transcriptions.  

### Switch to local OpenAI Whisper

You can evaluate how well local `whisper` performs for transcribing CoffeeShop recordings in **appsettings.json**: 

```json
{
  "SpeechProvider": "WhisperLocalSpeechToText"
}
```

Which configures [Configure.Speech.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.Speech.cs)
to use `WhisperLocalSpeechToText`:

```csharp
services.AddSingleton<ISpeechToText>(c => new WhisperLocalSpeechToText {
   WhisperPath = c.Resolve<AppConfig>().WhisperPath ?? ProcessUtils.FindExePath("whisper"),
   TimeoutMs = c.Resolve<AppConfig>().NodeProcessTimeoutMs,
});
```

## Running LLMs Locally

Being able to run [OpenAI's Whisper](https://openai.com/research/whisper) locally gives our App the option for removing  
an external API and infrastructure dependency, will it also be possible to run LLM locally to remove our ChatGPT dependency?

Thanks to tools like [llama.cpp](https://github.com/ggerganov/llama.cpp) we're able to run popular Open Source LLMs 
from our laptops. An easy way to easily download and evaluate different LLMs is using the [llm](https://llm.datasette.io) 
Python utility that can be installed with:

:::sh
pip install llm
:::

That you can use to install the **llama.cpp** plugin with:

:::sh
llm install llm-llama-cpp
:::

Then use that it to download the popular Llama2 7B model with: 

```bash
llm llama-cpp download-model \
https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin \
--alias llama2-chat --alias l2c --llama2-chat
```

Which lets us ask our local LLama2 model what we want, a great list used to measure the effectiveness of 
different LLMs can be found at [benchmarks.llmonitor.com](https://benchmarks.llmonitor.com):

:::sh
llm -m l2c 'Argue for and against the use of kubernetes in the style of a haiku.' 
:::

Which our Llama2 took **64.51s** to responded with:

```
Sure, I can provide a haiku argument for and against the use of Kubernetes:

**For Kubernetes:**

Containerized world
Efficiently orchestrated bliss
Apps swim with ease

Kubernetes reigns
Managing pods with grace
Smooth sailing ahead

**Against Kubernetes:**

Overcomplicated sea
Cluttered with APIs and tools
Tainted by complexity

Kubernetes, alas
Forcing us to conform
To its rigid ways
```

The response is impressive, unfortunately taking over 1 minute to execute wasn't. 

Whilst we're here lets see how viable the Open Source models are for evaluating our natural language CoffeeShop orders,
which we can do with: 

```bash
curl --get --data-urlencode "userMessage=i'd like a latte that's it" \
https://coffeeshop.netcore.io/coffeeshop/prompt | llm -m l2c
```

:::info tip
Or use `https://localhost:5001/coffeeshop/prompt` to use a local App prompt
:::

Which we're happy to report returns a valid JSON prompt our App can process:

```json
{
    "items": [
        {
            "type": "lineitem",
            "product": {
                "type": "LatteDrinks",
                "name": "latte",
                "temperature": "hot",
                "size": "grande"
            },
            "quantity": 1
        }
    ]
}
```

Whilst Llama2 7B isn't nearly as capable or accurate as ChatGPT, it demonstrates potential for Open Source LLMs to
eventually become a viable option for being able to add support for natural language requests in our Apps.

Unfortunately the **105s** time it took to execute makes it unsuitable for handling any real-time tasks, when running on 
laptop hardware at least with only a **10-core GPU** and **24GB RAM** with **100GB/s** memory bandwidth. 

But the top-line specs of PC's equipped with high-end Nvidia GPUs or Apple Silicon's [Mac Studio](https://www.apple.com/au/mac-studio/) 
Ultra's impressive **76-core GPU** with **192GB RAM** at **800GB/s** memory bandwidth should mean we'll be able to run our 
AI workloads on consumer hardware in the not too distant future.

### Local AI Workflows

Whilst running all our App's AI requirements from a laptop may not be feasible just yet, we're still able to execute
some cool AI workflows that's only been possible to do from our laptop's until very recently.

For example, this shell command:

 - uses **ffmpeg** to capture 5 seconds of our voice recording 
 - uses **whisper** to transcribe our recording to text
 - uses CoffeeShop Prompt API to create a TypeChat Request from our text order
 - uses **llm** to execute our App's TypeChat prompt with our local Llama2 7B LLM

```bash
ffmpeg -f avfoundation -i ":1" -t 5 order.mp3 \
&& whisper --language=en --fp16 False --output_format txt order.mp3 \
&& curl --get --data-urlencode "userMessage=$(cat order.txt)" \
https://coffeeshop.netcore.io/coffeeshop/prompt | llm -m l2c
```

If all stars have aligned you should get a machine readable JSON response the CoffeeShop can process to turn your 
voice recording into a Cart order:

```json
{
    "items": [
        {
            "type": "lineitem",
            "product": {
                "type": "LatteDrinks",
                "name": "latte",
                "temperature": "hot",
                "size": "grande"
            },
            "quantity": 1
        }
    ]
}
```

## New .NET TypeChat Examples Soon

Watch this space or [join our Newsletter](/posts/building-typechat-coffeeshop-gpt#top) (max 1 email / 2-3 months) 
to find out when new .NET implementations for TypeChat's different GPT examples become available.


# Modelling TypeChat's CoffeeShop in .NET
Source: https://servicestack.net/posts/building-typechat-coffeeshop-modelling

## Building a TypeChat CoffeeShop .NET App 

Since the release of [Open AI's Chat GPT](https://chat.openai.com) we've been exploring its potential
in unlocking new AI powered capabilities in our Apps that were previously limited to Large companies
with dedicated AI development teams, now their capabilities is within everyone's reach with just 1 API call away!

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="MjNqPAXLH5w" style="background-image: url('https://img.youtube.com/vi/MjNqPAXLH5w/maxresdefault.jpg')"></lite-youtube>
</div>

### Chain of thought Prompt engineering

Our initial approach of leveraging LLMs was to [create ChatGPT Agents to call APIs](/posts/chat-gpt-agents) 
by adopting the [Chain-of-Thought](https://arxiv.org/abs/2201.11903) technique popularized by 
[Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) where along with the goal we also ask it for its
**plan**, **reasoning**, and **criticism** which influences its future decisions to help break down its tasks into 
smaller, more achievable steps.

We adopt this approach within a scoped context with just the APIs that we want Chat GPT to know about in order to
accomplish purpose specific tasks we assign it. We showcase the utility of this approach in 
[GPTMeetingAgent(github)](https://github.com/NetCoreApps/GPTMeetingAgent) in which we use the GPT Meeting Agent 
to use available APIs to search for Users and book Meetings:

<div class="not-prose my-16 px-4 sm:px-6">
    <div class="text-center">
        <h3 class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
            <a class="text-indigo-600 hover:text-indigo-600" href="https://gptmeetings.netcore.io">gptmeetings.netcore.io</a>
        </h3>
    </div>
    <p class="mx-auto pt-5 text-xl text-gray-500"> 
        Use Natural Language to get GPT Agents to book meetings with your APIs
    </p>
    <div class="my-8">
        <div class="flex justify-center">
            <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="7vChIGHWPuI" style="background-image: url('https://img.youtube.com/vi/7vChIGHWPuI/maxresdefault.jpg')"></lite-youtube>
        </div>
    </div>
</div>

This approach relies on a fairly sophisticated prompt to get the desired outcome whose probability of a successful response
is dependent on the ambiguity and complexity of a command and the surface complexity of the APIs they need to call.

The difficulty then becomes how best to construct the interface of our System APIs to LLMs and how best to detect a valid
response from an invalid one. In our experience the key to best guiding LLMs to produce a valid response is to front load
the prompt with descriptive information of available functionality it should utilize and constrain its output to a restricted
surface area. 

### TypeChat

Microsoft's new [TypeChat](https://github.com/microsoft/TypeChat) library takes another interesting approach to interfacing with LLMs where instead of 
using chain of thought to continually refine LLM outputs towards a valid successful response, it relies on using 
TypeScript schemas to define and restrict what valid responses Chat GPT should return, which both validates LLM responses
to verify if they're valid and if not replies back with Schema Validation errors to guide GPT into returning a successful response.

Whilst this approach is less ambitious and open ended from harnessing the reasoning capabilities of LLMs than Chain of Thought, 
it's easier to develop from a pragmatic view where instead of tweaking and refining prompt templates to get more desirable
outcomes you're defining TypeScript schemas of what you want the Natural Language free text to convert into.

[TypeChat's CoffeeShop](https://github.com/microsoft/TypeChat/tree/main/examples/coffeeShop) 
is a good example of what this looks like in real world application which uses LLMs to
implement a natural language ordering system by capturing all the different ways a Customer can order at a Cafe, 
as defined in:
[coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts)

```ts
// The following is a schema definition for ordering lattes.

export interface Cart {
    items: (LineItem | UnknownText)[];
}

// Use this type for order items that match nothing else
export interface UnknownText {
    type: 'unknown',
    text: string; // The text that wasn't understood
}

export interface LineItem {
    type: 'lineitem',
    product: Product;
    quantity: number;
}

export type Product = BakeryProducts | LatteDrinks | EspressoDrinks | CoffeeDrinks;

export interface BakeryProducts {
    type: 'BakeryProducts';
    name: 'apple bran muffin' | 'blueberry muffin' | 'lemon poppyseed muffin' |'bagel'
    options: (BakeryOptions | BakeryPreparations)[];
}

export interface BakeryOptions {
    type: 'BakeryOptions';
    name: 'butter' | 'strawberry jam' | 'cream cheese';
    optionQuantity?: OptionQuantity;
}

export interface BakeryPreparations {
    type: 'BakeryPreparations';
    name: 'warmed' | 'cut in half';
}

export interface LatteDrinks {
    type: 'LatteDrinks';
    name: 'cappuccino' | 'flat white' | 'latte' | 'macchiato' | 'mocha' |'chai latte'
    temperature?: CoffeeTemperature;
    size?: CoffeeSize;  // The default is 'grande'
    options?:(Milks | Sweeteners | Syrups | Toppings | Caffeines|LattePreparations)[]
}

// more categories and products...
export type CoffeeTemperature = 'hot' | 'extra hot' | 'warm' | 'iced';

export type CoffeeSize = 'short' | 'tall' | 'grande' | 'venti';

export type EspressoSize = 'solo' | 'doppio' | 'triple' | 'quad';

export type OptionQuantity = 'no' | 'light' | 'regular' | 'extra' | number;
```

We can see TypeScript's expressive Type System really shines here which is easily able to succinctly express all
available products and options with minimal syntax. It's also worth noting the schema is solely concerned with the orders
customers are able to make and not about how the data is modelled in a datastore which is a good approach when interfacing
with LLMs to increase the probability of a successful result.

But to be useful App's still need to model their data model which as a goal needs to:
- Capture all categories and products Customers can order
- Be able to dynamically generate the resulting TypeScript Schema
- Persist in a Data Store
- Enable management through a User Friendly UI
 
Which will be the initial goal of our .NET App. FortunatelyTypeScript schema also serves as a great requirements documentation,
clearly and precisely defining all the categories, products, relationships and variants our Data Model needs to support.

### Code-First Data Modelling

This can be easily done in code-first ORMs like [OrmLite](https://docs.servicestack.net/ormlite/) which lets you design
RDBMS Tables with simple POCO classes. Since we'll also be using the Data Model to generate our online store we'll also add
an `ImageUrl` on **Category** and **Product** Models. 

OrmLite also supports persisting complex types on Data Models which are serialized with the RDBMS configured 
[Complex Type Serializer](https://docs.servicestack.net/ormlite/complex-type-serializers) which saves from requiring 
a number of unnecessary code tables and inefficient table joins for table data that doesn't need to be queried server-side. 

By utilizing complex type collections we can get this down to **5 tables** to define a data model that supports capturing the
categories, products, options and relationships in the [coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts):

```csharp
public class Category
{
    [AutoIncrement]
    public int Id { get; set; }
    public string Name { get; set; }
    public string Description { get; set; }
    public List<string>? Temperatures { get; set; }
    public string? DefaultTemperature { get; set; }
    public List<string>? Sizes { get; set; }
    public string? DefaultSize { get; set; }
    public string? ImageUrl { get; set; }
    
    [Reference]
    public List<Product> Products { get; set; }

    [Reference]
    public List<CategoryOption> CategoryOptions { get; set; }
}

public class Product
{
    [AutoIncrement]
    public int Id { get; set; }
    [References(typeof(Category))]
    public int CategoryId { get; set; }
    public string Name { get; set; }
    public decimal Cost { get; set; }
    public string? ImageUrl { get; set; }

    [Reference]
    public Category Category { get; set; }
}

public class Option
{
    [AutoIncrement]
    public int Id { get; set; }
    public string Type { get; set; }
    public List<string> Names { get; set; }
    public bool? AllowQuantity { get; set; }
    public string? QuantityLabel { get; set; }
}

public class OptionQuantity
{
    [AutoIncrement]
    public int Id { get; set; }
    public string Name { get; set; }
    public decimal Value { get; set; }
}

public class CategoryOption
{
    [AutoIncrement]
    public int Id { get; set; } 
    [References(typeof(Category))]
    public int CategoryId { get; set; }
    [References(typeof(Option))]
    public int OptionId { get; set; }
}
```

:::info
The `[Reference]` attributes defines [POCO References](https://docs.servicestack.net/ormlite/reference-support) for pulling 
in data from related tables in OrmLite's `Load*` APIs
:::

## Creating the CoffeeShop Database

Next step is to create the RDBMS tables, which we recommend doing from within a 
[Code-First DB Migration](https://docs.servicestack.net/ormlite/db-migrations) class so they can be easily run, re-run
and extend over time. 

For this we can just copy all data models into a 
[Migration1000.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Migrations/Migration1000.cs) 
class which represents the initial state of the Application database, where all RDBMS tables are created and populated 
in the `Up()` method and tables deleted and drop in the `Down()` method:

```csharp
public class Migration1000 : MigrationBase
{
    // Embedded copy of Data Models...
    
    public override void Up()
    {
        Db.CreateTable<Category>();
        Db.CreateTable<Option>();
        Db.CreateTable<OptionQuantity>();
        Db.CreateTable<CategoryOption>();
        Db.CreateTable<Product>();
        
        Db.SaveAll(new OptionQuantity[]
        {
            new() { Name = "no", Value = 0 },
            new() { Name = "light", Value = 0.5m },
            new() { Name = "regular", Value = 1 },
            new() { Name = "extra", Value = 2 },
        });
        
        void AddOptions(string type, string[] names, bool? allowQuantity = false, string? quantityLabel = null)
        {
            var item = new Option
            {
                Type = type,
                Names = new(names),
                AllowQuantity = allowQuantity,
                QuantityLabel = quantityLabel,
            };
            Db.Save(item);
        }
        
        AddOptions("BakeryOptions", new[] {
            "Butter", 
            "Strawberry Jam", 
            "Cream Cheese",
        });
        AddOptions("BakeryPreparations", new[] {
            "Warmed", 
            "Cut in Half", 
        });
        AddOptions("Milks", new[] {
            "Whole Milk", 
            "Two Percent Milk", 
            "NonFat Milk", 
            "Coconut Milk", 
            "Soy Milk", 
            "Almond Milk", 
            "Oat Milk",
        });
        //....
        
        void AddCategoryProducts(string category, 
            (string name, decimal cost)[] productInfos, 
            string[]? optionTypes = null,
            string[]? temperatures = null,
            string? defaultTemperature = null,
            string[]? sizes = null,
            string? defaultSize = null)
        {
            var cat = new Category
            {
                Name = category,
                Description = category.SplitCamelCase(),
                Temperatures = temperatures != null ? new(temperatures) : null,
                DefaultTemperature = defaultTemperature,
                Sizes = sizes != null ? new(sizes) : null,
                DefaultSize = defaultSize,
                ImageUrl = $"/products/{category.SplitCamelCase().GenerateSlug()}.jpg",
            };
            Db.Save(cat);

            foreach (var optionType in optionTypes.Safe())
            {
                var option = options.First(x => x.Type == optionType);
                var categoryOption = new CategoryOption
                {
                    CategoryId = cat.Id,
                    OptionId = option.Id,
                };
                Db.Save(categoryOption);
            }
            
            foreach (var productInfo in productInfos)
            {
                var product = new Product
                {
                    CategoryId = cat.Id,
                    Name = productInfo.name,
                    Cost = productInfo.cost,
                    ImageUrl = $"/products/{productInfo.name.GenerateSlug()}.jpg",
                };
                Db.Save(product);
            }
        }

        AddCategoryProducts("BakeryProducts", new[] {
            ("Apple Bran Muffin", 4m),
            ("Blueberry Muffin", 4),
            ("Lemon Poppy seed Muffin", 4),
            ("Bagel", 4),
        }, new[]{ "BakeryOptions", "BakeryPreparations" });
        
        AddCategoryProducts("LatteDrinks", new[] {
            ("Cappuccino", 5.5m),
            ("Flat White", 5),
            ("Latte", 5),
            ("Latte Macchiato", 5),
            ("Mocha", 4.5m),
            ("Chai Latte", 4),
        }, new[] { "Milks", "Sweeteners", "Syrups", "Toppings", "Caffeines", "LattePreparations" },
           new[] { "Iced", "Warm", "Hot", "Extra Hot" }, defaultTemperature:"Hot",
           new[] { "Short", "Tall", "Grande", "Venti" }, defaultSize:"Grande");
        //...
    }
    
    public override void Down()
    {
        // Delete referential foreign key data first
        Db.DeleteAll<Product>();
        Db.DeleteAll<CategoryOption>();
        Db.DeleteAll<OptionQuantity>();
        Db.DeleteAll<Option>();
        Db.DeleteAll<Category>();

        Db.DropTable<Product>();
        Db.DropTable<CategoryOption>();
        Db.DropTable<OptionQuantity>();
        Db.DropTable<Option>();
        Db.DropTable<Category>();
    }
}
```

Within the `Up()` method we can utilize everything C# offers including Tuples and local functions to populate the 
database with minimal boilerplate. The `Down()` method just needs to undo everything the `Up()` method does, which
typically means dropping any new tables with `Db.DropTable<T>` in the reverse order they were created, the data 
only needs to be deleted first if tables have foreign keys and foreign key enforcement is enabled.

### Running Migrations

After creating the initial migration all that's left is to run it, which you can do from the command-line with:

:::sh
npm run migrate
:::

Which is an alias for [dotnet Migration App Tasks](https://docs.servicestack.net/ormlite/db-migrations#dotnet-migration-tasks)
to run your App in the context of a Migration App Tasks. 

Alternatively Migration Tasks can also be run from within your IDE by running the `Migrate()` Unit Test in 
[CoffeeShop's MigrationTasks.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.Tests/MigrationTasks.cs)

### Rerunning the last Migration

Often you'll need a few iterations before you're happy with the your current set of Database changes, a productive workflow
for making iterative changes to your current migration you're actively working on (but haven't committed) is to make changes
to your current migration then revert and re-run it, which you and do with the `Rerun_Last_Migration()` Unit Test or
on the command-line with:

:::sh
npm run rerun:last
:::

Or for your first DB Migration you could also delete the entire `App_Data\db.sqlite` database and re-run `npm run migrate`
to rerun migrations and recreate the database.

### Viewing the App database

By default CoffeeShop is configured to use an embedded SQLite database at `App_Data\db.sqlite` which you can inspect
with your favorite RDBMS viewer. My favorite all-purpose database tool is [JetBrains DataGrip](https://www.jetbrains.com/datagrip/)
but if you're using Rider you can simply drag **db.sqlite** to Rider's **Database** panel to view it within the same IDE:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/db.png)](/img/posts/building-typechat-coffeeshop-modelling/db.png)
:::

## Building the UI and API

With the Data Models and Database created we can get to the fun stuff in building our App's APIs and UI. Luckily this
is effortless with AutoQuery where most of it can be done without any implementation - i.e. by just using code-first
DTOs to describe the API we want, then ServiceStack's [AutoQuery](/autoquery) and [Vue Components](/vue/) does the rest.

For this we just create the [AutoQuery CRUD](https://docs.servicestack.net/autoquery/crud) APIs for the functionality
we want to enable. E.g. 
 - Defining `QueryDb<T>` for a Data Model allows it to be queried
 - Defining `ICreateDb<T>` enables new rows to be created
 - Defining `IPatchDb<T>` enables rows to be updated
 - Defining `IDeleteDb<T>` enables rows to be deleted

So that CoffeeShop owners can manage their own database we'll implement AutoQuery CRUD APIs for the `Category`, `Product`,
`Option` and `OptionQuantity` tables. We'll handle the **Many to Many** `CategoryOption` table later as it's not something
users will want to manage directly. 

We can accomplish the bulk of the functionality we need by with the CRUD APIs below after copying all the properties
from the DataModels we want users to be able to Create / Edit:

```csharp
public class QueryCategories : QueryDb<Category> {}
public class CreateCategory : ICreateDb<Category>, IReturn<Category>
{
    public string Name { get; set; }
    public string Description { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Sizes { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Temperatures { get; set; }
    public string? DefaultSize { get; set; }
    public string? DefaultTemperature { get; set; }
    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }
}
public class UpdateCategory : IPatchDb<Category>, IReturn<Category>
{
    public int Id { get; set; }
    public string? Name { get; set; }
    public string? Description { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Sizes { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Temperatures { get; set; }
    public string? DefaultSize { get; set; }
    public string? DefaultTemperature { get; set; }
    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }
}
public class DeleteCategory : IDeleteDb<Category>, IReturnVoid
{
    public int Id { get; set; }
}

public class QueryProducts : QueryDb<Product> {}
public class CreateProduct : ICreateDb<Product>, IReturn<Product>
{
    public int CategoryId { get; set; }
    public string Name { get; set; }
    public decimal Cost { get; set; }
    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }
}
public class UpdateProduct : IPatchDb<Product>, IReturn<Product>
{
    public int Id { get; set; }
    public int? CategoryId { get; set; }
    public string? Name { get; set; }
    public decimal? Cost { get; set; }
    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }
}
public class DeleteProduct : IDeleteDb<Product>, IReturnVoid
{
    public int Id { get; set; }
}

public class QueryOptions : QueryDb<Option> {}
public class CreateOption : ICreateDb<Option>, IReturn<Option>
{
    public string Type { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string> Names { get; set; }
    public bool? AllowQuantity { get; set; }
    public string? QuantityLabel { get; set; }
}
public class UpdateOption : IPatchDb<Option>, IReturn<Option>
{
    public int Id { get; set; }
    public string? Type { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Names { get; set; }
    public bool? AllowQuantity { get; set; }
    public string? QuantityLabel { get; set; }
}
public class DeleteOption : IDeleteDb<Option>, IReturnVoid
{
    public int Id { get; set; }
}

public class QueryOptionQuantities : QueryDb<OptionQuantity> {}
public class CreateOptionQuantity : ICreateDb<OptionQuantity>, IReturn<OptionQuantity>
{
    public string Name { get; set; }
}
public class UpdateOptionQuantity : IPatchDb<OptionQuantity>, IReturn<OptionQuantity>
{
    public int Id { get; set; }
    public string? Name { get; set; }
}
public class DeleteOptionQuantity : IDeleteDb<OptionQuantity>, IReturnVoid
{
    public int Id { get; set; }
}
```

### Partial Updates

The one thing we have to look out for if we implement our Update APIs with `IPatchDb<T>` Partial Updates is that all properties
other than the Primary Key should be nullable. This isn't required when implementing Update APIs with `IUpdateDb<T>`
which sends full updates of every property per request.

### Custom Fields

Most of the functionality available to APIs can be enabled using [declarative attributes](https://docs.servicestack.net/locode/declarative),
In this case we're making usage of the `[Input]` [UI Attribute](https://docs.servicestack.net/locode/declarative#ui-metadata-attributes)
to customize which UI Control is used to render the property in ServiceStack [Auto UIs](/auto-ui).

The [Auto Form Components](https://docs.servicestack.net/vue/autoform) only render Input UI Components for .NET Types 
where their exists a HTML Input Element for it. Since there's no HTML Input element that edits collections natively
we'll need to specify which Custom Vue Component to use, which in that case will use the [TagInput Component](https://docs.servicestack.net/vue/taginput)
to edit `List<string>` properties:

```csharp
public class UpdateCategory : IPatchDb<Category>, IReturn<Category>
{
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Sizes { get; set; }

    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Temperatures { get; set; }

    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }
    //...
}
```

Whilst `[Input(Type="file")]` specifies to use the HTML File Input to populate the `ImageUrl` property with the path to 
the uploaded file that's handled by the `[UploadTo("products")]` [Managed File Upload Location](https://docs.servicestack.net/locode/files-overview)
defined in [Configure.AppHost.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/Configure.AppHost.cs):

```csharp
var wwwrootVfs = GetVirtualFileSource<FileSystemVirtualFiles>();
Plugins.Add(new FilesUploadFeature(
    new UploadLocation("products", wwwrootVfs, allowExtensions:FileExt.WebImages,
        resolvePath: ctx => $"/products/{ctx.FileName}"))
));
```

## Locode Auto UI

A nice UX touch we can add to our UIs is to give each Table visual icons to better describe what they're for which we
can do by annotating them with SVGs in [Icons.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceModel/Icons.cs):

```csharp
[Icon(Svg = Icons.Category)]
public class Category { ... }

[Icon(Svg = Icons.Option)]
public class Option { ... }

[Icon(Svg = Icons.OptionQuantity)]
public class OptionQuantity { ... }

[Icon(Svg = Icons.Product)]
public class Product { ... }
```

Where it will now show up next to each Table in [Locode's CRUD UI](https://docs.servicestack.net/locode/) accessible at:

<h3 class="not-prose text-center pb-8">
    <a class="text-4xl text-blue-600 hover:underline" href="https://localhost:5001/locode/">https://localhost:5001/locode/</a>
</h3>

Which we can login with the `admin@email.com` user and default `p@55wOrd` created in [Configure.AuthRepository.cs](https://github.com/NetCoreApps/CoffeeShop/blob/f00d4e5d3edd3dd23e325d1899e0078db025204a/CoffeeShop/Configure.AuthRepository.cs#L44).

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/query-products.png)](/img/posts/building-typechat-coffeeshop-modelling/query-products.png)
:::

So without having needing to write any implementation, Locode provides us with a full CRUD UI generated from your APIs typed
Request DTOs that's used to render all UI Forms using Vue [Auto Form Components](https://docs.servicestack.net/vue/autoform),
which we can see makes use of our customizations with `Sizes` and `Temperatures` properties managed by **full-width** `Tag` Input components
and `ImageUrl` managed by the [FileInput Component](https://docs.servicestack.net/vue/fileinput): 

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/update-category.png)](/img/posts/building-typechat-coffeeshop-modelling/update-category.png)
:::

And because we have [Crud Events registered](https://github.com/NetCoreApps/CoffeeShop/blob/f00d4e5d3edd3dd23e325d1899e0078db025204a/CoffeeShop/Configure.AutoQuery.cs#L13)
our UI Forms also shows the [AutoQuery CRUD Executable Audit Log](https://docs.servicestack.net/autoquery/audit-log) to be able to track and view 
all edits made to each record through our AutoQuery APIs.

## Custom Admin UI

Locode is a great way to enable an instant Management UI for backend developers or employees to browse and manage your Apps
data, and whilst it offers a [lot of customizability](https://docs.servicestack.net/locode/custom-overview) it's not as 
customizable as a Bespoke UI.

Fortunately as Locode is built on ServiceStack's [Vue Components](https://servicestack.net/vue/) we can reuse the same
components to quickly build Custom Admin UIs:

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="wlRA4_owEsc" style="background-image: url('https://img.youtube.com/vi/wlRA4_owEsc/maxresdefault.jpg')"></lite-youtube>
</div>

To get started quickly the [blazor-vue](https://blazor-vue.web-templates.io) template includes a Client Admin example at:

- [/admin/](https://blazor-vue.web-templates.io/admin/) - [/portal/index.html](https://github.com/NetCoreTemplates/blazor-vue/blob/main/MyApp/wwwroot/admin/index.html)

Whose sections of Vue Components can be replaced with our Models to create the CoffeeShop Admin UI:  

```js
const sections = {
    //...
    Categories: {
        type: 'Category',
        component: {
            template:`
            <AutoQueryGrid :type="type" selectedColumns="imageUrl,id,name,defaultSize,products"
                    :headerTitles="{ imageUrl: ' ' }" :canFilter="x => x != 'ImageUrl'">
                <template #imageUrl="{ imageUrl }">
                    <Icon :src="imageUrl" class="w-8 h-8 rounded-full" />
                </template>
                <template #id="{ id }">{{id}}</template>
                <template #name="{ name }">{{name}}</template>
                <template #description="{ description }">{{description}}</template>
                <template #defaultSize="{ defaultSize }">{{defaultSize}}</template>
                <template #products="{ products }">{{ products.map(x => x.name).join(', ') }}</template>
            </AutoQueryGrid>`,
        },
    },
    Products: {
        type: 'Product',
        component: {
            template:`
            <AutoQueryGrid :type="type" selectedColumns="imageUrl,category,id,name,cost"
                    :headerTitles="{ imageUrl: ' ' }" :canFilter="x => x != 'ImageUrl'">
                <template #imageUrl="{ imageUrl }">
                    <Icon :src="imageUrl" class="w-8 h-8 rounded-full" />
                </template>
                <template #category="{ category }">
                    <a :href="'#Categories?edit=' + category.id" class="flex text-indigo-600 hover:underline">
                        <icon :svg="getIcon('Categories')" class="w-5 h-5 mr-1 shrink-0 text-indigo-600"></icon>
                        {{category.name}}
                    </a>
                </template>
                <template #id="{ id }">{{id}}</template>
                <template #name="{ name }">{{name}}</template>
                <template #cost="{ cost }">
                    <preview-format :value="cost" :format="Formats.currency"></preview-format>
                </template>
            </AutoQueryGrid>`,
            setup() {
                return { getIcon, Formats }
            }
        },
    },
    Options: {
        type: 'Option',
        component: {
            template:`<AutoQueryGrid :type="type" />`,
        },
    },
    OptionQuantities: {
        type: 'OptionQuantity',
        component: {
            template:`<AutoQueryGrid :type="type" />`,
        },
    },
}
```

Which gives us a similar Admin UI with the advantage of a completely customizable UI, which we make use of to implement
custom [AutoQueryGrid components](https://docs.servicestack.net/vue/autoquerygrid) for **Categories** and **Products** 
showing a customized view with just the columns we want, the order and format we want it in whilst **Options** and 
**OptionQuantities** continue to use the default AutoQueryGrid components:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/portal-products.png)](/img/posts/building-typechat-coffeeshop-modelling/portal-products.png)
:::

## Custom UI Forms

We're pretty close to a complete Admin UI with AutoQuery and AutoForm Components being able to implement most of the 
standard UI Forms we need except it doesn't have auto support for managing Many-to-Many relationships like `CategoryOption` 
table which we'll need to implement ourselves to be able to specify which Options a category of Products can have.

### Implementing Many to Many CategoryOption Admin UI

The easier way to implement this functionality would be to have the UI call an API each time an `Option` was added or removed
to a `Category`. The problem with this approach is that it doesn't match the existing behavior where if a User **cancels**
a form they'd expect for none of their changes to be applied.

To implement the desired functionality we'll instead create a custom `UpdateCategory` implementation that also
handles any changes to `CategoryOption` using new `AddOptionIds` and `RemoveOptionIds` properties that we'll want 
rendered as **hidden** inputs in our HTML Form with:

```csharp
public class UpdateCategory : IPatchDb<Category>, IReturn<Category>
{
    public int Id { get; set; }
    public string? Name { get; set; }
    public string? Description { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Sizes { get; set; }
    [Input(Type = "tag"), FieldCss(Field = "col-span-12")]
    public List<string>? Temperatures { get; set; }
    public string? DefaultSize { get; set; }
    public string? DefaultTemperature { get; set; }
    [Input(Type = "file"), UploadTo("products")]
    public string? ImageUrl { get; set; }

    [Input(Type = "hidden")]
    public List<int>? AddOptionIds { get; set; }
 
    [Input(Type = "hidden")]
    public List<int>? RemoveOptionIds { get; set; }
}
```

We can then provide a [Custom AutoQuery Implementation](https://docs.servicestack.net/autoquery/rdbms#custom-autoquery-implementations)
in [CoffeeShopServices.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceInterface/CoffeeShopServices.cs) 
which AutoQuery will use instead of its own.

Where our custom implementation continues to utilize AutoQuery's Partial Update functionality if there's any changes to update, 
as well as removing or adding any Options the user makes to the `Category`: 

```csharp
public class CoffeeShopServices : Service
{
    public IAutoQueryDb AutoQuery { get; set; }
    
    public async Task<object> Any(UpdateCategory request)
    {
        // Perform all RDBMS Updates within the same Transaction
        using var trans = Db.OpenTransaction();

        Category? response = null;
        var ignore = new[] { nameof(request.Id), nameof(request.AddOptionIds), nameof(request.RemoveOptionIds) };
        // Only call AutoQuery Update if there's something to update
        if (request.ToObjectDictionary().HasNonDefaultValues(ignoreKeys:ignore))
        {
            response = (Category) await AutoQuery.PartialUpdateAsync<Category>(request, Request, Db);
        }
        if (request.RemoveOptionIds?.Count > 0)
        {
            await Db.DeleteAsync<CategoryOption>(x => x.CategoryId == request.Id && request.RemoveOptionIds.Contains(x.OptionId));
        }
        if (request.AddOptionIds?.Count > 0)
        {
            await Db.InsertAllAsync(request.AddOptionIds.Map(id => new CategoryOption { CategoryId = request.Id, OptionId = id }));
        }
        trans.Commit();

        response ??= request.ConvertTo<Category>();
        return response;
    }
}
```

We now need to implement the Custom UI that Adds/Removes Options from a Category which we'll do in a custom `CategoryOptions`
Vue Component that displays all the Category Options with a button to remove existing ones and a Select Input to 
add non existing options.

The purpose of the component is to populate the `addOptionIds` field with Option Ids that should be added and `removeOptionIds`
with Ids to be removed, which updates the Request DTO of the parent Form Model with the `update:modelValue` event:

```js
const CategoryOptions = {
    template:`
         <div>
            <ul v-for="optionType in currentOptionTypes">
                <li class="py-1 flex justify-between">
                    <span>
                        {{optionType}}
                    </span>
                    <span>
                        <svg class="w-6 h-6 text-red-600 hover:text-red-800 cursor-pointer" @click="removeOption(optionType)" xmlns='http://www.w3.org/2000/svg' width='1024' height='1024' viewBox='0 0 1024 1024'>
                            <title>Remove Option</title>
                            <path fill='currentColor' d='M512 64a448 448 0 1 1 0 896a448 448 0 0 1 0-896zM288 512a38.4 38.4 0 0 0 38.4 38.4h371.2a38.4 38.4 0 0 0 0-76.8H326.4A38.4 38.4 0 0 0 288 512z'/>
                        </svg>
                    </span>
                </li> 
            </ul>
            <div class="flex justify-between items-center">
                <select-input class="flex-grow" @change="addOption" :values="['',...options.filter(x => !currentOptionTypes.includes(x.type)).map(x => x.type)]"></select-input>
                <svg class="ml-2 w-6 h-6 text-green-600" xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24'>
                    <title>Add Option</title>
                    <path fill='currentColor' d='M11 17h2v-4h4v-2h-4V7h-2v4H7v2h4v4Zm1 5q-2.075 0-3.9-.788t-3.175-2.137q-1.35-1.35-2.137-3.175T2 12q0-2.075.788-3.9t2.137-3.175q1.35-1.35 3.175-2.137T12 2q2.075 0 3.9.788t3.175 2.137q1.35 1.35 2.138 3.175T22 12q0 2.075-.788 3.9t-2.137 3.175q-1.35 1.35-3.175 2.138T12 22Z'/>
                </svg>            
            </div>
         </div>
    `,
    props:['type','id','modelValue'],
    emits:['update:modelValue'],
    setup(props, { emit }) {
        const client = useClient()
        const options = ref([])
        const model = props.modelValue

        model.addOptionIds ??= []
        model.removeOptionIds ??= []
        const origOptionIds = model.categoryOptions?.map(x => x.optionId) || []

        const currentOptionIds = computed(() => [...origOptionIds, ...model.addOptionIds]
            .filter(x => !model.removeOptionIds.includes(x)))
        const currentOptionTypes = computed(() =>
            currentOptionIds.value.map(id => options.value.find(x => x.id === id)?.type).filter(x => !!x))

        function addOption(e) {
            const optionType = e.target.value
            if (!optionType) return
            const option = options.value.find(x => x.type === optionType)
            if (model.removeOptionIds.includes(option.id))
                model.removeOptionIds = model.removeOptionIds.filter(id => id !== option.id)
            else if (!model.addOptionIds.includes(option.id))
                model.addOptionIds.push(option.id)
            emit('update:modelValue', model)
        }
        function removeOption(optionType) {
            const option = options.value.find(x => x.type === optionType)
            if (model.addOptionIds.includes(option.id))
                model.addOptionIds = model.addOptionIds.filter(id => id !== option.id)
            else if (!model.removeOptionIds.includes(option.id))
                model.removeOptionIds.push(option.id)
        }

        onMounted(async () => {
            const api = await client.api(new QueryOptions({ orderBy:'id' }))
            options.value = api.response.results || []
            emit('update:modelValue', model)
        })

        return { options, addOption, removeOption, currentOptionTypes }
    }
}
```

Which is then attached to the AutoQueryGrid Form Components using its `<template #formfooter>` to include it in the
bottom of the Create and Edit Form Components:

```js
const sections = {
    Categories: {
        type: 'Category',
        component: {
            components: { CategoryOptions },
            template:`
            <AutoQueryGrid :type="type" selectedColumns="imageUrl,id,name,defaultSize,products"
                    :headerTitles="{ imageUrl: ' ' }" :canFilter="x => x != 'ImageUrl'">
                <template #imageUrl="{ imageUrl }">
                    <Icon :src="imageUrl" class="w-8 h-8 rounded-full" />
                </template>
                <template #id="{ id }">{{id}}</template>
                <template #name="{ name }">{{name}}</template>
                <template #description="{ description }">{{description}}</template>
                <template #defaultSize="{ defaultSize }">{{defaultSize}}</template>
                <template #products="{ products }">{{ products.map(x => x.name).join(', ') }}</template>
                <template #formfooter="{ form, type, apis, model, id, updateModel }">
                    <div class="w-1/2 mt-4 px-4 sm:px-6">
                        <h3 class="text-lg font-semibold">Options</h3>
                        <CategoryOptions v-if="form === 'edit'" :key="id" :type="type" :id="id" v-model="model" @update:modelValue="updateModel(model)" />
                    </div>
                </template>                
            </AutoQueryGrid>`,
        },
    },
    //...
}
```

With those finishing touches our back-end Admin UI is now complete which CoffeeShop owners can use to manage their entire database:

:::{.shadow .rounded-sm}
[![](/img/posts/building-typechat-coffeeshop-modelling/portal-update-category.png)](/img/posts/building-typechat-coffeeshop-modelling/portal-update-category.png)
:::

### Generate TypeScript Schema

One of the goals for our App's Database is to dynamically generating the TypeScript Schema for TypeChat to use
to translate Customers Orders into valid Cart Orders that our Application can process. ServiceStack does include functionality
to [convert C# DTOs into TypeScript Types](https://docs.servicestack.net/typescript-add-servicestack-reference) except that's not
useful here as the [coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts)
is primarily populated from data not C# Types.

Instead the most flexible option to be able to generate the schema is to use a templating language, luckily we have a
great option for this in [#Script](https://sharpscript.net) which combines [Handlebars](https://handlebarsjs.com) and
[JavaScript Expression](https://sharpscript.net/docs/expression-viewer) syntax with tight 
[integration with .NET](https://sharpscript.net/docs/script-net) that we can develop quickly thanks to its 
[Live Reloading](https://sharpscript.net/#exploratory-programming) dev UX.

To utilize #Script we need to create a `ScriptContext` in C# with all the Data and functionality the template needs to 
generate the prompt by adding script arguments for all the data in our App's database which is done in
[CoffeeShopPromptProvider.cs](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop.ServiceInterface/CoffeeShopPromptProvider.cs):

```csharp
public class CoffeeShopPromptProvider : IPromptProvider
{
    public IDbConnectionFactory DbFactory { get; set; }
    public AppConfig Config { get; set; }

    public async Task<string> CreateSchemaAsync(TypeChatRequest request, CancellationToken token)
    {
        var file = new FileInfo(Config.CoffeeShop.GptPath.CombineWith("schema.ss"));
        if (file == null)
            throw HttpError.NotFound($"{Config.CoffeeShop.GptPath}/schema.ss not found");

        using var db = await DbFactory.OpenDbConnectionAsync(token: token);
        var categories = await db.LoadSelectAsync(db.From<Category>(), token:token);
        var options = await db.SelectAsync<Option>(token: token);
        var optionsMap = options.ToDictionary(x => x.Id);
        var optionQuantities = await db.SelectAsync<OptionQuantity>(token: token);

        var tpl = await file.ReadAllTextAsync(token: token);
        var context = new ScriptContext {
            Plugins = { new TypeScriptPlugin() }
        }.Init();

        var output = await new PageResult(context.OneTimePage(tpl))
        {
            Args =
            {
                [nameof(categories)] = categories,
                [nameof(options)] = options,
                [nameof(optionsMap)] = optionsMap,
                [nameof(optionQuantities)] = optionQuantities,
            },
        }.RenderScriptAsync(token: token);
        return output;
    }
    //...
}
```

With the only added functionality our schema makes use of are the simple helpers in
[TypeScriptMethods.cs](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack/src/ServiceStack.Common/Script/Methods/TypeScriptMethods.cs)

```csharp
public class TypeScriptPlugin : IScriptPlugin
{
    public void Register(ScriptContext context) => 
        context.ScriptMethods.Add(new TypeScriptMethods());
}

public class TypeScriptMethods : ScriptMethods
{
    public RawString tsUnionStrings(IEnumerable<string> strings) => new(
        StringUtils.Join(" | ", strings.Map(x => $"'{x}'"), lineBreak:108));

    public RawString tsUnionTypes(IEnumerable<string> strings) => new(
        StringUtils.Join(" | ", strings, lineBreak:108));
}
```

The populated `ScriptContext` is then used to execute the
[gpt/coffeeshop/schema.ss](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ss) **#Script** template:

```js
// The following is a schema definition for ordering lattes.

export interface Cart {
    items: (LineItem | UnknownText)[];
}

// Use this type for order items that match nothing else
export interface UnknownText {
    type: 'unknown',
    text: string; // The text that wasn't understood
}

export interface LineItem {
    type: 'lineitem',
    product: Product;
    quantity: number;
}

export type Product = {{categories.map(x => x.name) |> tsUnionTypes}};

{{#each category in categories}}
export interface {{category.name}} {
    type: '{{category.name}}';
    name: {{ category.products.map(x => x.name.lower()) |> tsUnionStrings }};
{{#if !category.temperatures.isEmpty() }}
    temperature?: {{category.temperatures.map(x => x.lower()) |> tsUnionStrings}};{{category.defaultTemperature ? ` // The default is '${category.defaultTemperature.lower()}'`.raw() : ''}}
{{/if}}
{{#if !category.sizes.isEmpty() }}
    size?: {{category.sizes.map(x => x.lower()) |> tsUnionStrings}};{{category.defaultSize ? ` // The default is '${category.defaultSize.lower()}'`.raw() : ''}}
{{/if}}
{{#if options.count > 0}}
    options?: ({{ options.map(x => x.type) |> tsUnionTypes }})[];
{{/if}}
}

{{/each}}

{{#each option in options}}
export interface {{option.type}} {
    type: '{{option.type}}';
    name: {{ option.names.map(x => x.lower()) |> tsUnionStrings }};
{{#if option.allowQuantity}}
    optionQuantity?: OptionQuantity;
{{/if}}
}
{{/each}}

export type OptionQuantity = {{optionQuantities.map(x => x.name.lower()) |> tsUnionStrings}} | number;
```

To generate our [gpt/coffeeshop/schema.ts](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ts)
which is functionality equivalent to TypeChat's hand crafted [coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts): 

```ts
// The following is a schema definition for ordering lattes.

export interface Cart {
    items: (LineItem | UnknownText)[];
}

// Use this type for order items that match nothing else
export interface UnknownText {
    type: 'unknown',
    text: string; // The text that wasn't understood
}

export interface LineItem {
    type: 'lineitem',
    product: Product;
    quantity: number;
}

export type Product = BakeryProducts | LatteDrinks | EspressoDrinks | CoffeeDrinks;

export interface BakeryProducts {
    type: 'BakeryProducts';
    name: 'apple bran muffin' | 'blueberry muffin' | 'lemon poppy seed muffin' | 'bagel';
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations | 
        Creamers)[];
}

export interface LatteDrinks {
    type: 'LatteDrinks';
    name: 'cappuccino' | 'flat white' | 'latte' | 'latte macchiato' | 'mocha' | 'chai latte';
    temperature?: 'iced' | 'warm' | 'hot' | 'extra hot'; // The default is 'hot'
    size?: 'short' | 'tall' | 'grande' | 'venti'; // The default is 'grande'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations | 
        Creamers)[];
}

export interface EspressoDrinks {
    type: 'EspressoDrinks';
    name: 'espresso' | 'lungo' | 'ristretto' | 'macchiato';
    temperature?: 'iced' | 'warm' | 'hot' | 'extra hot'; // The default is 'hot'
    size?: 'solo' | 'doppio' | 'triple' | 'quad'; // The default is 'doppio'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations | 
        Creamers)[];
}

export interface CoffeeDrinks {
    type: 'CoffeeDrinks';
    name: 'americano' | 'coffee';
    temperature?: 'iced' | 'warm' | 'hot' | 'extra hot'; // The default is 'hot'
    size?: 'short' | 'tall' | 'grande' | 'venti'; // The default is 'grande'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations | 
        Creamers)[];
}


export interface BakeryOptions {
    type: 'BakeryOptions';
    name: 'butter' | 'strawberry jam' | 'cream cheese';
    optionQuantity?: OptionQuantity;
}
export interface BakeryPreparations {
    type: 'BakeryPreparations';
    name: 'warmed' | 'cut in half';
}
export interface Milks {
    type: 'Milks';
    name: 'whole milk' | 'two percent milk' | 'nonfat milk' | 'coconut milk' | 'soy milk' | 'almond milk' | 'oat milk';
}
export interface Sweeteners {
    type: 'Sweeteners';
    name: 'equal' | 'honey' | 'splenda' | 'sugar' | 'sugar in the raw' | 'sweet n low';
    optionQuantity?: OptionQuantity;
}
export interface Syrups {
    type: 'Syrups';
    name: 'almond syrup' | 'buttered rum syrup' | 'caramel syrup' | 'cinnamon syrup' | 'hazelnut syrup' | 
        'orange syrup' | 'peppermint syrup' | 'raspberry syrup' | 'toffee syrup' | 'vanilla syrup';
    optionQuantity?: OptionQuantity;
}
export interface Toppings {
    type: 'Toppings';
    name: 'cinnamon' | 'foam' | 'ice' | 'nutmeg' | 'whipped cream' | 'water';
    optionQuantity?: OptionQuantity;
}
export interface Caffeines {
    type: 'Caffeines';
    name: 'regular' | 'two thirds caf' | 'half caf' | 'one third caf' | 'decaf';
    optionQuantity?: OptionQuantity;
}
export interface LattePreparations {
    type: 'LattePreparations';
    name: 'for here cup' | 'lid' | 'with room' | 'to go' | 'dry' | 'wet';
}
export interface Creamers {
    type: 'Creamers';
    name: 'whole milk creamer' | 'two percent milk creamer' | 'one percent milk creamer' | 'nonfat milk creamer' | 
        'coconut milk creamer' | 'soy milk creamer' | 'almond milk creamer' | 'oat milk creamer' | 
        'half and half' | 'heavy cream';
}

export type OptionQuantity = 'no' | 'light' | 'regular' | 'extra' | number;
```

### Optimizing TypeScript Schema Generation

Whilst our schema is functionally equivalent to TypeChat's [coffeeShopSchema.ts](https://github.com/microsoft/TypeChat/blob/main/examples/coffeeShop/src/coffeeShopSchema.ts), we noticed that some 
of the prompts were not returning the same desirable results. After several iterations of experimentation we learned
that several factors about the structure of the schema contributed to the effectiveness of the results.

For example the `CoffeeTemperature`, `CoffeeSize`, `EspressoSize` Types aren't just inert aliases and adds descriptive
context to the prompt. Likewise the proximity and order of the Types affects the strength of their relationships and
we were able to get more effective results by defining Options next to the Categories they apply to. 

We applied both changes to our [schema.ss](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ss) 
which now looks like:

```js
{{* Type Aliases *}}
{{ var coffeeTemperatures = ['Iced','Warm','Hot','Extra Hot'] }}
{{ var coffeeSizes        = ['Short','Tall','Grande','Venti'] }}
{{ var espressoSizes      = ['Solo','Doppio','Triple','Quad'] }}

// The following is a schema definition for ordering lattes.

export interface Cart {
    items: (LineItem | UnknownText)[];
}

// Use this type for order items that match nothing else
export interface UnknownText {
    type: 'unknown',
    text: string; // The text that wasn't understood
}

export interface LineItem {
    type: 'lineitem',
    product: Product;
    quantity: number;
}

export type Product = {{categories.map(x => x.name) |> tsUnionTypes}};

{{ var generatedOptionTypes = [] }}
{{#each category in categories}}
export interface {{category.name}} {
    type: '{{category.name}}';
    name: {{ category.products.map(x => x.name.lower()) |> tsUnionStrings }};
{{#if coffeeTemperatures.equivalentTo(category.temperatures) }}
    temperature?: CoffeeTemperature;{{category.defaultTemperature ? `  // The default is '${category.defaultTemperature.lower()}'`.raw() : ''}}
{{else if !category.temperatures.isEmpty() }}
    temperature?: {{category.temperatures.map(x => x.lower()) |> tsUnionStrings}};{{category.defaultTemperature ? `  // The default is '${category.defaultTemperature.lower()}'`.raw() : ''}}
{{/if}}
{{#if coffeeSizes.equivalentTo(category.sizes) }}
    size?: CoffeeSize;{{category.defaultSize ? `  // The default is '${category.defaultSize.lower()}'`.raw() : ''}}
{{else if espressoSizes.equivalentTo(category.sizes) }}
    size?: EspressoSize;{{category.defaultSize ? `  // The default is '${category.defaultSize.lower()}'`.raw() : ''}}
{{else if !category.sizes.isEmpty() }}
    size?: {{category.sizes.map(x => x.lower()) |> tsUnionStrings}};{{category.defaultSize ? `  // The default is '${category.defaultSize.lower()}'`.raw() : ''}}
{{/if}}
{{#if options.count > 0}}
    options?: ({{ options.map(x => x.type) |> tsUnionTypes }})[];
{{/if}}
}

{{ var options = category.categoryOptions.map(x => optionsMap[x.optionId]) }}
{{#each option in options.where(x => !generatedOptionTypes.contains(x.type)) }}
{{ generatedOptionTypes.push(option.type) |> ignore }}
export interface {{option.type}} {
    type: '{{option.type}}';
    name: {{ option.names.map(x => x.lower()) |> tsUnionStrings }};
{{#if option.allowQuantity}}
    optionQuantity?: OptionQuantity;
{{/if}}
}
{{/each}}

{{/each}}

export type CoffeeTemperature = {{coffeeTemperatures.map(x => x.lower()) |> tsUnionStrings}};

export type CoffeeSize = {{coffeeSizes.map(x => x.lower()) |> tsUnionStrings}};

export type EspressoSize = {{espressoSizes.map(x => x.lower()) |> tsUnionStrings}};

export type OptionQuantity = {{optionQuantities.map(x => x.name.lower()) |> tsUnionStrings}} | number;
```

To generate our restructured and more effective resulting Schema:

```typescript
// The following is a schema definition for ordering lattes.

export interface Cart {
    items: (LineItem | UnknownText)[];
}

// Use this type for order items that match nothing else
export interface UnknownText {
    type: 'unknown',
    text: string; // The text that wasn't understood
}

export interface LineItem {
    type: 'lineitem',
    product: Product;
    quantity: number;
}

export type Product = BakeryProducts | LatteDrinks | EspressoDrinks | CoffeeDrinks;

export interface BakeryProducts {
    type: 'BakeryProducts';
    name: 'apple bran muffin' | 'blueberry muffin' | 'lemon poppy seed muffin' | 'bagel';
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations |
        Creamers)[];
}


export interface BakeryOptions {
    type: 'BakeryOptions';
    name: 'butter' | 'strawberry jam' | 'cream cheese';
    optionQuantity?: OptionQuantity;
}

export interface BakeryPreparations {
    type: 'BakeryPreparations';
    name: 'warmed' | 'cut in half';
}

export interface LatteDrinks {
    type: 'LatteDrinks';
    name: 'cappuccino' | 'flat white' | 'latte' | 'latte macchiato' | 'mocha' | 'chai latte';
    temperature?: CoffeeTemperature;  // The default is 'hot'
    size?: CoffeeSize;  // The default is 'grande'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations |
        Creamers)[];
}


export interface Milks {
    type: 'Milks';
    name: 'whole milk' | 'two percent milk' | 'nonfat milk' | 'coconut milk' | 'soy milk' | 'almond milk' | 'oat milk';
}

export interface Sweeteners {
    type: 'Sweeteners';
    name: 'equal' | 'honey' | 'splenda' | 'sugar' | 'sugar in the raw' | 'sweet n low';
    optionQuantity?: OptionQuantity;
}

export interface Syrups {
    type: 'Syrups';
    name: 'almond syrup' | 'buttered rum syrup' | 'caramel syrup' | 'cinnamon syrup' | 'hazelnut syrup' |
        'orange syrup' | 'peppermint syrup' | 'raspberry syrup' | 'toffee syrup' | 'vanilla syrup';
    optionQuantity?: OptionQuantity;
}

export interface Toppings {
    type: 'Toppings';
    name: 'cinnamon' | 'foam' | 'ice' | 'nutmeg' | 'whipped cream' | 'water';
    optionQuantity?: OptionQuantity;
}

export interface Caffeines {
    type: 'Caffeines';
    name: 'regular' | 'two thirds caf' | 'half caf' | 'one third caf' | 'decaf';
    optionQuantity?: OptionQuantity;
}

export interface LattePreparations {
    type: 'LattePreparations';
    name: 'for here cup' | 'lid' | 'with room' | 'to go' | 'dry' | 'wet';
}

export interface EspressoDrinks {
    type: 'EspressoDrinks';
    name: 'espresso' | 'lungo' | 'ristretto' | 'macchiato';
    temperature?: CoffeeTemperature;  // The default is 'hot'
    size?: EspressoSize;  // The default is 'doppio'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations |
        Creamers)[];
}


export interface Creamers {
    type: 'Creamers';
    name: 'whole milk creamer' | 'two percent milk creamer' | 'one percent milk creamer' | 'nonfat milk creamer' |
        'coconut milk creamer' | 'soy milk creamer' | 'almond milk creamer' | 'oat milk creamer' |
        'half and half' | 'heavy cream';
}

export interface CoffeeDrinks {
    type: 'CoffeeDrinks';
    name: 'americano' | 'coffee';
    temperature?: CoffeeTemperature;  // The default is 'hot'
    size?: CoffeeSize;  // The default is 'grande'
    options?: (BakeryOptions | BakeryPreparations | Milks | Sweeteners | Syrups | Toppings | Caffeines | LattePreparations |
        Creamers)[];
}


export type CoffeeTemperature = 'iced' | 'warm' | 'hot' | 'extra hot';

export type CoffeeSize = 'short' | 'tall' | 'grande' | 'venti';

export type EspressoSize = 'solo' | 'doppio' | 'triple' | 'quad';

export type OptionQuantity = 'no' | 'light' | 'regular' | 'extra' | number;
```

As prompt engineering is more an Art than a Science it took several iterations of experimentation across multiple prompts 
to be able to measure the effectiveness of different changes, as such it was important to be able to modify and test 
prompts quickly which generating prompts with **#Script** lets us do as we could make changes to the
[schema.ss](https://github.com/NetCoreApps/CoffeeShop/blob/main/CoffeeShop/gpt/coffeeshop/schema.ss) template and 
get immediate feedback of their efficacy whilst the App was running.  

## Part 2

Check out [Part 2](/posts/building-typechat-coffeeshop-gpt) which covers the different options and challenges for using this 
schema to create a functional Voice Activated CoffeeShop in .NET that utilizes different Speech-to-Text and GPT Providers.


# Should .NET Apps switch to ARM?
Source: https://servicestack.net/posts/cloud-value-between-architectures

In the ever-competitive landscape of cloud computing, choices abound for software developers seeking to harness the power of various providers and architectures. This blog post presents an insightful analysis comparing three major providers: Hetzner, AWS, and Azure. Focusing on cost efficiency and performance, I've explored different instance types utilizing both ARM and x86 architectures. The benchmarks used for this comparison include the Postgres benchmark (`pgbench`), a .NET HTTP load test, and Geekbench scores.

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="yeGEI3n5pnQ" style="background-image: url('https://img.youtube.com/vi/yeGEI3n5pnQ/maxresdefault.jpg')"></lite-youtube>
</div>

## Benchmarks and Tools
- **Postgres Benchmark:** Utilizing `pgbench`, I ran the same Docker setup on each provider's instances to ensure consistency.
- **HTTP RPS Benchmark:** A simple .NET load test was executed, performing approximately 100,000 requests as quickly as possible to measure the Requests Per Second (RPS).
- **Geekbench Scores:** Geekbench 6.0 for x86 and 6.1-Preview for ARM were used to generate scores. Some scores were gathered from public community results, while others were manually retested.

:::info
All tests were conducted on Ubuntu 22.04, providing a uniform testing environment.
:::

## Upselling and Value

In a world where numerous large cloud providers are competing, the focus on value often takes a backseat. While giants like AWS and Azure offer an extensive array of managed services, there are scenarios where developers simply need raw compute power at a reasonable price. This is where providers like Hetzner, focusing more on value, come into play.

The emergence of ARM as a cost-effective option for production systems is also noteworthy, especially for those who are 'locked in' to one of the large vendors. With improving support and compatibility, ARM presents a viable way to save money without switching providers, especially if you're already on AWS or Azure.

## Methodology

The methodology for this analysis was designed to capture differences in performance and value between architectures (ARM and x86) and providers (Hetzner, AWS, Azure). Here's a brief overview of the approach:

### Testing Environment
- **Docker Containers:** Both the .NET HTTP load test and `pgbench` were run inside Docker containers to ensure consistency across all instances.
- **Test Repetition:** Each test was conducted three times to mitigate the impact of outliers, providing a more accurate representation of performance.

### Benchmarks Used
- **.NET HTTP**: Load test Focusing on web service performance, this test runs a ServiceStack service with SQLite. This tests concurrent workload of a HTTP server to show the change in performance between architectures.
- **pgbench**: Specifically targeting database performance, the Postgres benchmark was conducted using `pgbench`. This tests SQL transaction throughput across cores, calculating average transactions per second.
- **Geekbench**: As a rounded synthetic benchmark, Geekbench was run using the standard binary distributed by the creators. A well rounded benchmark that tests CPU performance across different tasks.

### Objective
While you should always do your own performance testing for your use case, the combination of these three distinct benchmarks offers a multifaceted view of the computing capabilities of different instance types across providers and architectures. By analyzing web service performance, database performance, and synthetic benchmarks, this approach aims to hopefully provide insights for developers and businesses seeking cost-efficient compute power.

## Results Overview

Lets now have a look at the results for cost efficiency across providers (Hetzner, AWS, Azure) and architectures (ARM and x86) for three distinct benchmarks: TPS (Transactions Per Second), HTTP RPS (Requests Per Second), and Geekbench.

### Provider Comparison

Hetzer, AWS, and Azure were chosen for this analysis due to their popularity and the availability of ARM instances. While AWS and Azure are well-known, Hetzner is a lesser-known provider that offers great value for money. Here we can see an overview of the cost efficiency of x86 and ARM across the three providers.

### pgbench Database Workload

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/pgbench-provider-cost-efficiency.png)](./img/posts/arm-vs-x86/pgbench-provider-cost-efficiency.png)


| Provider | Architecture | Cost Efficiency |
|----------|--------------|-----------------|
| Azure    | x86          | 1.00x           |
| Azure    | ARM          | 2.29x           |
| AWS      | x86          | 6.13x           |
| AWS      | ARM          | 7.45x           |
| Hetzner  | x86          | 39.57x          |
| Hetzner  | ARM          | 35.70x          |


### .NET HTTP Web Service Workload

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/http-provider-cost-efficiency.png)](./img/posts/arm-vs-x86/http-provider-cost-efficiency.png)

| Provider | Architecture | Cost Efficiency |
|----------|--------------|-----------------|
| Azure    | x86          | 1.00x           |
| Azure    | ARM          | 1.49x           |
| AWS      | x86          | 1.87x           |
| AWS      | ARM          | 2.04x           |
| Hetzner  | x86          | 10.26x          |
| Hetzner  | ARM          | 10.23x          |

### Geekbench Scores

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/geekbench-provider-cost-efficiency.png)](./img/posts/arm-vs-x86/geekbench-provider-cost-efficiency.png)

| Provider | Architecture | Cost Efficiency |
|----------|--------------|-----------------|
| Azure    | x86          | 1.00x           |
| Azure    | ARM          | 2.45x           |
| AWS      | x86          | 1.97x           |
| AWS      | ARM          | 3.48x           |
| Hetzner  | x86          | 15.41x          |
| Hetzner  | ARM          | 19.06x          |

Hetzer offers the best value by a large margin across these 3 providers, but AWS and Azure offer a much wider range of managed services and features.

So while the savings for switching to ARM for Hetzner servers are relatively small, for Azure and AWS, ARM offers a significant cost saving, lets have a look at the specific benchmarks to see where ARM shines.

## pgbench TPS Across Instance Types

### AWS pgbench Performance

Here we have a performance comparison of the TPS (Transactions Per Second) for AWS x86 vs ARM, comparing matching instance types within AWS T series.

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-pgbench-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/aws-pgbench-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| AWS      | t3.medium     | x86          | 2    | 4      | $17.23       | 1.01x                |
| AWS      | t4g.medium    | ARM          | 2    | 4      | $15.40       | 1.0x                 |
| AWS      | t3.large      | x86          | 2    | 8      | $38.11       | 1.03x                |
| AWS      | t4g.large     | ARM          | 2    | 8      | $30.73       | 1.02x                |
| AWS      | t3.xlarge     | x86          | 4    | 16     | $76.14       | 1.21x                |
| AWS      | t4g.xlarge    | ARM          | 4    | 16     | $61.54       | 1.55x                |

### AWS pgbench Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-x86-vs-arm-pgbench.png)](./img/posts/arm-vs-x86/aws-x86-vs-arm-pgbench.png)

We can see as the ARM cores scale up, the cost savings can increase and so can performance. This is a great example of how ARM can be a cost-effective option for workloads that can scale across cores.
For example if you had an existing workload running on **3x t3.xlarge** instances you could drop your monthly costs from **$228.42 to $184.62** by switching to 3x t4g.xlarge instances, a saving of **$43.8 per month** or **$525.6 per year**.

### Azure pgbench Performance

For Azure, we tested the D series, comparing the D4sv3 x86 instance type with the D4psv5 ARM instance type, and same with the D2.
The D2 instances have two cores, while the D4 instances have four cores, so again we can see how well ARM scales up.

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-pgbench-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/azure-pgbench-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| Azure    | D4sv3         | x86          | 4    | 16     | $121.40      | 1.11x                |
| Azure    | D2psv5        | ARM          | 2    | 8      | $38.62       | 1.32x                |
| Azure    | D2sv3         | x86          | 2    | 8      | $60.74       | 1.0x                 |
| Azure    | D4psv5        | ARM          | 4    | 16     | $77.23       | 1.9x                 |

### Azure pgbench Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-x86-vs-arm-pgbench.png)](./img/posts/arm-vs-x86/azure-x86-vs-arm-pgbench.png)

Azure costs are significantly higher than AWS, and the cost savings opportunity by switching to ARM is even larger. For example, if you had an existing workload running on **3x D4sv3** instances you could drop your monthly costs from **$364.20 to $231.70** by switching to **3x D4psv5** instances, a saving of **$132.50** per month or a whopping **$1,589.94** per year, and you would get a performance **boost of around 30%** for these kinds of workloads.

If you are running on Azure, ARM could be a great way to save money.

### Hetzner pgbench Performance

For Hetzner we tested the AMD CPX series against the ARM CAX series, comparing matching instance types.

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/hetzner-pgbench-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/hetzner-pgbench-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| Hetzner  | CPX11         | x86          | 2    | 2      | $4.74        | 1.47x                |
| Hetzner  | CAX11         | ARM          | 2    | 4      | $4.02        | 1.0x                 |
| Hetzner  | CPX21         | x86          | 3    | 4      | $8.76        | 1.69x                |
| Hetzner  | CAX21         | ARM          | 4    | 8      | $8.03        | 1.53x                |
| Hetzner  | CPX31         | x86          | 4    | 8      | $16.79       | 2.5x                 |
| Hetzner  | CAX31         | ARM          | 8    | 16     | $15.33       | 2.12x                |
| Hetzner  | CPX41         | x86          | 8    | 16     | $32.85       | 3.36x                |
| Hetzner  | CAX41         | ARM          | 16   | 32     | $29.93       | 3.06x                |


Core counts don't match up exactly between instances, but we can see how well ARM scales up as the core count increases, but it doesn't quite close the gap in this benchmark.

### Hetzner pgbench Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/hetzner-x86-vs-arm-pgbench.png)](./img/posts/arm-vs-x86/hetzner-x86-vs-arm-pgbench.png)

Hetzner offers the best value across all providers, but for cost vs performance, x86 still comes out on top for this benchmark. So for Hetzner, there isn't a large saving by switching to ARM in this workload.

## .NET HTTP Across Instance Types

Switching gears to web service workloads, we can see how ARM performs in this scenario where we have a .NET web service running on ServiceStack with SQLite, and .NET client that scales with available cores to perform 100,000 requests as quickly as possible.

### AWS .NET HTTP Performance

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-http-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/aws-http-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| AWS      | t3.medium     | x86          | 2    | 4      | $17.23       | 1.1x                 |
| AWS      | t4g.medium    | ARM          | 2    | 4      | $15.40       | 1.04x                |
| AWS      | t3.large      | x86          | 2    | 8      | $38.11       | 1.08x                |
| AWS      | t4g.large     | ARM          | 2    | 8      | $30.73       | 1.0x                 |
| AWS      | t3.xlarge     | x86          | 4    | 16     | $76.14       | 2.24x                |
| AWS      | t4g.xlarge    | ARM          | 4    | 16     | $61.54       | 2.01x                |


While x86 still comes out on top for this benchmark in regards to performance, the cost savings are even better than the previous test with the same instances.

### AWS .NET HTTP Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-x86-vs-arm-http.png)](./img/posts/arm-vs-x86/aws-x86-vs-arm-http.png)

A lot smaller than we saw with the pgbench test, but still a saving which can add up over time.

### Azure .NET HTTP Performance

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-http-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/azure-http-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| Azure    | D2psv5        | ARM          | 2    | 8      | $38.62       | 1.0x                 |
| Azure    | D2sv3         | x86          | 2    | 8      | $60.74       | 1.01x                |
| Azure    | D4psv5        | ARM          | 4    | 16     | $77.23       | 1.65x                |
| Azure    | D4sv3         | x86          | 4    | 16     | $121.40      | 1.85x                |

Again, we see x86 win in raw performance, but for Azure the cost savings are a lot better than AWS.

### Azure .NET HTTP Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-x86-vs-arm-http.png)](./img/posts/arm-vs-x86/azure-x86-vs-arm-http.png)

We see nearly a **50% saving for the same performance**, which is a great result for ARM.

### Hetzner .NET HTTP Performance

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/hetzner-http-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/hetzner-http-x86-arm-performance-comparison.png)

The performance is a lot closer for this benchmark, but x86 still comes out on top on a per core basis since the instances like the CAX41 have 16 cores, but the CPX41 only has 8 cores.

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| Hetzner  | CPX11         | x86          | 2    | 2      | $4.74        | 1.53x                |
| Hetzner  | CAX11         | ARM          | 2    | 4      | $4.02        | 1.0x                 |
| Hetzner  | CPX21         | x86          | 3    | 4      | $8.76        | 1.39x                |
| Hetzner  | CAX21         | ARM          | 4    | 8      | $8.03        | 2.04x                |
| Hetzner  | CPX31         | x86          | 4    | 8      | $16.79       | 3.75x                |
| Hetzner  | CAX31         | ARM          | 8    | 16     | $15.33       | 2.41x                |
| Hetzner  | CPX41         | x86          | 8    | 16     | $32.85       | 7.82x                |
| Hetzner  | CAX41         | ARM          | 16   | 32     | $29.93       | 8.33x                |

### Hetzner .NET HTTP Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/hetzner-x86-vs-arm-http.png)](./img/posts/arm-vs-x86/hetzner-x86-vs-arm-http.png)

Hetzner offers the best value over all compared to the other providers, but for this benchmark we get a mixed result when it comes to performance.
This is partly due again to the difference in core count for the instance types, as they are a lot more matched in value proposition.
So we essentially get the same performance for the same cost.

## Geekbench Scores Across Instance Types

Geekbench is a synthetic benchmark, but it can be useful for comparing the relative performance of different architectures and instance types.

### AWS Geekbench Performance

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-geekbench-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/aws-geekbench-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| AWS      | t3.medium     | x86          | 2    | 4      | $17.23       | 1.0x                 |
| AWS      | t4g.medium    | ARM          | 2    | 4      | $15.40       | 1.55x                |
| AWS      | t3.large      | x86          | 2    | 8      | $38.11       | 1.12x                |
| AWS      | t4g.large     | ARM          | 2    | 8      | $30.73       | 1.55x                |
| AWS      | t3.xlarge     | x86          | 4    | 16     | $76.14       | 1.91x                |
| AWS      | t4g.xlarge    | ARM          | 4    | 16     | $61.54       | 2.94x                |

Here we can see even though the ARM instances are cheaper, they still offer better performance for this benchmark once we scale up to 4 cores.

### AWS Geekbench Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/aws-x86-vs-arm-geekbench.png)](./img/posts/arm-vs-x86/aws-x86-vs-arm-geekbench.png)

If we use Geekbench as an overall we can see ARM offers about a ~70% saving for the same performance.
These are synthetic benchmarks, so your mileage may vary, but it's a good indicator of the possible savings.

### Azure Geekbench Performance

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-geekbench-x86-arm-performance-comparison.png)](./img/posts/arm-vs-x86/azure-geekbench-x86-arm-performance-comparison.png)

| Provider | Instance Type | Architecture | CPUs | Memory | Monthly Cost | Relative Performance |
|----------|---------------|--------------|------|--------|--------------|----------------------|
| Azure    | D2psv5        | ARM          | 2    | 8      | $38.62       | 1.56x                |
| Azure    | D2sv3         | x86          | 2    | 8      | $60.74       | 1.0x                 |
| Azure    | D4psv5        | ARM          | 4    | 16     | $77.23       | 2.89x                |
| Azure    | D4sv3         | x86          | 4    | 16     | $121.4       | 1.86x                |

### Azure Geekbench Cost Efficiency

<div style="color: dimgrey; text-align: center; width: 100%;margin-bottom: -2em">
    Higher is Better
</div>

[![](./img/posts/arm-vs-x86/azure-x86-vs-arm-geekbench.png)](./img/posts/arm-vs-x86/azure-x86-vs-arm-geekbench.png)

Due to the higher costs of Azure, we see a much better saving for ARM at ~2.5-3x the performance for the same cost.
This is higher than the other benchmarks, so it is likely you can consider something like Geekbench as a theoretical maximum saving.

**For Azure, the message is clear, if you are running on x86, you should consider switching to ARM where possible as it will likely result in significant savings.**

## Summary
- **Hetzner's Focus on Value:** Hetzner consistently offers strong performance at a reasonable price, particularly with x86 architectures in database performance.
- **ARM's Rising Potential:** ARM emerges as a cost-effective option, especially for web services and scalable workloads. On AWS and Azure, the savings could be substantial.
- **Workload Consideration:** The choice between ARM and x86 may depend on the specific workload, with ARM showing particular strength in web services and scalability.

The analysis of cost efficiency across Hetzner, AWS, and Azure, considering both ARM and x86 architectures, unveils significant insights that can guide software developers and businesses in their decision-making process. Here are the key implications and takeaways:

### Choosing the Right Provider and Architecture
- **Value vs. Features:** Hetzner stands out as a provider focusing on value, offering great performance at a reasonable price. AWS and Azure, with their multitude of managed services, come with complex billing structures that might lead to unexpected expenses.
- **ARM's Emergence:** ARM emerges as an attractive, cost-effective option, especially for scalable workloads and web services. Its value is notable within AWS and Azure, potentially leading to substantial savings.

### Considerations for ARM on Hetzner
- **Regional Availability:** ARM on Hetzner is currently limited to one region and offered as a shared resource. This limitation could lead to occasional performance inconsistencies.

### Egress Cost Considerations
- **Bandwidth/Traffic Costs:** AWS and Azure's high egress costs contrast sharply with Hetzner's generous free bandwidth. For large compute tasks involving extensive data transfer, Hetzner could present significant cost savings.

## Final Thoughts
- **Balancing Features and Costs:** Developers must weigh the trade-offs between feature-rich offerings and straightforward value. Hetzner's focus on value may appeal to those seeking robust performance without complexity, while AWS and Azure may be suitable for those requiring a wide array of managed services.
- **The Role of Workload:** The choice between ARM and x86, or between providers, may hinge on specific workload requirements, scalability needs, and budget constraints.

### Performance Testing Code

The code for each performance test is linked below, along with a gist with scripts for running each test.
These were only used on Ubuntu 22.04, so they may not work on other distributions, and may require some tweaking depending on your environment.

- [pgbench](https://github.com/Layoric/pgbench-docker)
- [.NET HTTP Load Test](https://github.com/Layoric/ArmBenchmarks)
- [Scripts for Run](https://gist.github.com/Layoric/ee75e380210f29355aeebb746b641bcb)

## Feedback

If you have any feedback or questions, please feel free to reach out. I'd love to hear your thoughts on this analysis and any suggestions for future posts.


# New Blogging features in Razor SSG
Source: https://servicestack.net/posts/razor-ssg-new-blog-features

## New Blogging features in Razor SSG

[Razor SSG](https://razor-ssg.web-templates.io) is our Free Project Template for creating fast, statically generated Websites and Blogs with
Markdown & C# Razor Pages. A benefit of using Razor SSG to maintain this 
[servicestack.net(github)](https://github.com/ServiceStack/servicestack.net) website is that any improvements added
to our website end up being rolled into the Razor SSG Project Template for everyone else to enjoy.

This latest release brings a number of features and enhancements to improve Razor SSG usage as a Blogging Platform -
a primary use-case we're focused on as we pen our [22nd Blog Post for the year](/posts/year/2023) with improvements
in both discoverability and capability of blog posts:~~~~

### RSS Feed

Razor SSG websites now generates a valid RSS Feed for its blog to support their readers who'd prefer to read blog posts
and notify them as they're published with their favorite RSS reader:

<div class="not-prose my-8">
   <a href="https://razor-ssg.web-templates.io/feed.xml">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img class="max-w-3xl py-8" src="/img/posts/razor-ssg/valid-rss.png">
      </div>
   </a>
    <div class="mt-4 flex justify-center">
        <a class="text-indigo-600 hover:text-indigo-800" href="https://razor-ssg.web-templates.io/feed.xml">https://razor-ssg.web-templates.io/feed.xml</a>
        <a class="ml-4 text-indigo-600 hover:text-indigo-800" href="https://servicestack.net/feed.xml">https://servicestack.net/feed.xml</a>
    </div>
</div>

### Meta Headers support for Twitter cards and SEO

Blog Posts and Pages now include additional `<meta>` HTML Headers to enable support for 
[Twitter Cards](https://developer.twitter.com/en/docs/twitter-for-websites/cards/overview/abouts-cards) in both
Twitter and Meta's new [threads.net](https://threads.net), e.g:

<div class="not-prose my-8 flex justify-center">
   <a class="block max-w-2xl" href="https://www.threads.net/@servicestack/post/CvIFobPBs5h">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img class="py-8" src="/img/posts/razor-ssg/twitter-cards.png">
      </div>
   </a>
</div>

### Improved Discoverability

To improve discoverability and increase site engagement, bottom of blog posts now include links to other posts by
the same Blog Author, including links to connect to their preferred social networks and contact preferences:

![](https://servicestack.net/img/posts/razor-ssg/other-author-posts.png)

### Posts can include Vue Components

Blog Posts can now embed any global Vue Components directly in its Markdown, e.g: 

```html
<getting-started template="razor-ssg"></getting-started>
```

#### [/mjs/components/GettingStarted.mjs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/wwwroot/mjs/components/GettingStarted.mjs)

<div class="not-prose my-20 flex justify-center">
    <getting-started template="razor-ssg"></getting-started>
</div>

#### Individual Blog Post dependencies

Just like Pages and Docs they can also include specific JavaScript **.mjs** or **.css** in the `/wwwroot/posts` folder
which will only be loaded for that post:

<file-layout :files="{
    wwwroot: { 
        posts: { _: ['<slug>.mjs','<slug>.css'] },
    }
}"></file-layout>

Now posts that need it can dynamically load large libraries like [Chart.js](https://www.chartjs.org) and use it 
inside a custom Vue component by creating a custom `/posts/<slug>.mjs` that exports what components and features
your blog post needs, e.g:

#### [/posts/new-blog-features.mjs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/wwwroot/posts/new-blog-features.mjs)

```js
import ChartJs from './components/ChartJs.mjs'

export default {
    components: { ChartJs }
}
```

In this case it enables support for [Chart.js](https://www.chartjs.org) by including a custom Vue component that makes it
easy to create charts from Vue Components embedded in markdown:

#### [/posts/components/ChartJs.mjs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/wwwroot/posts/components/ChartJs.mjs)

```js
import { ref, onMounted } from "vue"
import { addScript } from "@servicestack/client"

let loadJs = addScript('https://cdn.jsdelivr.net/npm/chart.js/dist/chart.umd.min.js')

export default {
    template:`<div><canvas ref="chart"></canvas></div>`,
    props:['type','data','options'],
    setup(props) {
        const chart = ref()
        onMounted(async () => {
            await loadJs

            const options = props.options || {
                responsive: true,
                legend: {
                    position: "top"
                }
            }
            new Chart(chart.value, {
                type: props.type || "bar",
                data: props.data,
                options,
            })

        })
        return { chart }
    }
}
```

Which allows this post to embed Chart.js charts using the above custom `<chart-js>` Vue component and a JS Object literal, e.g: 

```html
<chart-js :data="{
    labels: [
        //...
    ],
    datasets: [
        //...
    ]
}"></chart-js>
```

Which the [Bulk Insert Performance](/posts/bulk-insert-performance) Blog Post uses extensively to embeds its
Chart.js Bar charts:

<chart-js :data="{
    labels: [
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [17.066, 166.747]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [20.224, 199.697]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [14.389, 115.645]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [64.389, 310.966]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [64.427, 308.574]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [89.821, 835.181]
        }
    ]
}"></chart-js>

### New Markdown Containers

[Custom Containers](https://github.com/xoofx/markdig/blob/master/src/Markdig.Tests/Specs/CustomContainerSpecs.md) 
are a popular method for implementing Markdown Extensions for enabling rich, wrist-friendly consistent 
content in your Markdown documents.

Most of [VitePress Markdown Containers](https://vitepress.dev/guide/markdown#custom-containers)
are also available in Razor SSG websites for enabling rich, wrist-friendly consistent markup in your Markdown pages, e.g:

```md
:::info
This is an info box.
:::

:::tip
This is a tip.
:::

:::warning
This is a warning.
:::

:::danger
This is a dangerous warning.
:::

:::copy
Copy Me!
:::
```

:::info
This is an info box.
:::

:::tip
This is a tip.
:::

:::warning
This is a warning.
:::

:::danger
This is a dangerous warning.
:::

:::copy
Copy Me!
:::

See Razor Press's [Markdown Containers docs](https://razor-press.web-templates.io/containers) for the complete list of available containers and examples on how to 
implement your own [Custom Markdown containers](https://razor-press.web-templates.io/containers#implementing-block-containers).

### Support for Includes

Markdown fragments can be added to `_pages/_include` - a special folder rendered with
[Pages/Includes.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Includes.cshtml) using
an [Empty Layout](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Shared/_LayoutEmpty.cshtml)
which can be included in other Markdown and Razor Pages or fetched on demand with Ajax.

Markdown Fragments can be then included inside other markdown documents with the `::include` inline container, e.g:

:::pre
::include vue/formatters.md::
:::

Where it will be replaced with the HTML rendered markdown contents of fragments maintained in `_pages/_include`.

### Include Markdown in Razor Pages

Markdown Fragments can also be included in Razor Pages using the custom `MarkdownTagHelper.cs` `<markdown/>` tag:

```html
<markdown include="vue/formatters.md"></markdown>
```

### Inline Markdown in Razor Pages

Alternatively markdown can be rendered inline with:

```html
<markdown>
## Using Formatters

Your App and custom templates can also utilize @servicestack/vue's
[built-in formatting functions](href="/vue/use-formatters).
</markdown>
```

### Light and Dark Mode Query Params

You can link to Dark and Light modes of your Razor SSG website with the `?light` and `?dark` query string params:

- [https://razor-ssg.web-templates.io/?dark](https://razor-ssg.web-templates.io/?dark)
- [https://razor-ssg.web-templates.io/?light](https://razor-ssg.web-templates.io/?light)

### Blog Post Authors threads.net and Mastodon links

The social links for Blog Post Authors can now include [threads.net](https://threads.net) and [mastodon.social](https://mastodon.social) links, e.g:

```json
{
  "AppConfig": {
    "BlogImageUrl": "https://servicestack.net/img/logo.png",
    "Authors": [
      {
        "Name": "Lucy Bates",
        "Email": "lucy@email.org",
        "ProfileUrl": "img/authors/author1.svg",
        "TwitterUrl": "https://twitter.com/lucy",
        "ThreadsUrl": "https://threads.net/@lucy",
        "GitHubUrl": "https://github.com/lucy",
        "MastodonUrl": "https://mastodon.social/@lucy"
      }
    ]
  }
}
```

## Feature Requests Welcome

Most of Razor SSG's features are currently being driven by requirements from the new 
[Websites built with Razor SSG](https://razor-ssg.web-templates.io/#showcase) and features we want available in our Blogs, 
we welcome any requests for missing features in other popular Blogging Platforms you'd like to see in Razor SSG to help 
make it a high quality blogging solution built with our preferred C#/.NET Technology Stack, by submitting them to:

:::{.text-indigo-600 .text-3xl .text-center}
[https://servicestack.net/ideas](https://servicestack.net/ideas)
:::

### SSG or Dynamic Features

Whilst statically generated websites and blogs are generally limited to features that can be generated at build time, we're able to
add any dynamic features we need in [CreatorKit](/creatorkit/) - a Free companion self-host .NET App Mailchimp and Disqus 
alternative which powers any dynamic functionality in Razor SSG Apps like the blogs comments and Mailing List features 
in this Blog Post.


# Exploring the new streamable JSON Lines Format
Source: https://servicestack.net/posts/jsonl-format

ServiceStack already supports a range of serializers out of the box like CSV,JSON,JSV and XML, but today we will explore the new support for the 
[JSON Lines](https://jsonlines.org) (JSONL) format. JSON Lines is an efficient JSON data format that is parseable by streaming parsers and 
text processing tools. It is a popular data format for maintaining large datasets, such as the AI datasets maintained on [Huggingface](https://huggingface.co).

## Introducing the JSON Lines Format


<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="m0tAfjvJaZg" style="background-image: url('https://img.youtube.com/vi/m0tAfjvJaZg/maxresdefault.jpg')"></lite-youtube>
</div>

Before we dive into the details of using the new feature, let's take a moment to understand what JSON Lines is and why it is beneficial. JSON Lines is similar to the CSV format in the way it behaves, where each line in the file represents a separate JSON object. This makes it easy to process large datasets incrementally, without having to load the entire file into memory.

```json lines
{"id": 1, "name": "John Doe"}
{"id": 2, "name": "Jane Doe"}
{"id": 3, "name": "John Smith"}
```

The JSON Lines format has become popular because of its streamable properties. It can be processed by streaming parsers and integrated with even Unix shell pipelines, making it an ideal format for handling large datasets efficiently, or hacking together a data pipeline quickly for large scale experimentation.

For example, you can pull out the `title` property from a JSON Lines file using the following command line using just `curl` and `jq`:

```shell
curl https://blazor-gallery.servicestack.net/albums.jsonl -s | jq '.title'
```

## Setting up an Endpoint to Support JSONL in ServiceStack

To enable the JSON Lines (JSONL) format for your ServiceStack endpoint, there are several ways to configure your Request DTO and Response DTO. JSONL behaves similarly to the CSV format, where the first `IEnumerable` property is automatically serialized in the streamable JSONL format. In this section, we will explore different ways to configure your DTOs for JSONL serialization.

### Automatic JSONL Serialization with AutoQuery

If you're using AutoQuery services in ServiceStack, JSONL serialization is already supported by default. The first `IEnumerable` property in your Request DTO will be automatically serialized in the JSONL format. This means that all AutoQuery APIs will return their `IEnumerable` datasets in the streamable JSONL format without any additional configuration.

### Configuring the Request DTO and Response DTO

To configure your own Request DTO and Response DTO for JSONL serialization, you can use either the `[DataContract]` and `[DataMember]` annotations or the `[Csv(CsvBehavior.FirstEnumerable)]` attribute.

### Using the `[DataContract]` and `[DataMember]` Annotations

By applying the `[DataContract]` annotation to your Response DTO and the `[DataMember]` annotation to the `IEnumerable` property, you can specify that the first `IEnumerable` property should be serialized in the JSONL format. Here's an example:

```csharp
[Route("/albums")]
public class QueryAlbums : IReturn<QueryAlbumsResponse>
{
}

[DataContract]
public class QueryAlbumsResponse
{
    [DataMember]
    public List<Album> Albums { get; set; }
}
```

In this example, the `QueryAlbumsResponse` class is annotated with `[DataContract]`, and the `Albums` property is annotated with `[DataMember]`. This signifies that the `Albums` property should be serialized in the JSONL format when returned as a response.

### Using the `[Csv(CsvBehavior.FirstEnumerable)]` Attribute

Alternatively, you can use the `[Csv(CsvBehavior.FirstEnumerable)]` attribute on your Response DTO to indicate that the first `IEnumerable` property should be serialized in the JSONL format. Here's an example:

```csharp
[Route("/users")]
public class SearchUsers : IReturn<SearchUsersResponse>
{
    public string Name { get; set; }
}

[Csv(CsvBehavior.FirstEnumerable)]
public class SearchUsersResponse
{
    public List<User> Users { get; set; }
}
```

## Async Streaming Parsing Example

To process each row of a JSON Lines file one at a time and avoid large allocations, ServiceStack provides convenient extension methods in the [HTTP Utils](https://docs.servicestack.net/http-utils) package. These methods allow you to implement async streaming parsing effortlessly.

Let's take a look at an example that demonstrates how to use async streaming parsing to process each row of a JSON Lines file:

```csharp
const string BaseUrl = "https://api.example.com";
var url = BaseUrl.CombineWith("data.jsonl");
await using var stream = await url.GetStreamFromUrlAsync();
await foreach (var line in stream.ReadLinesAsync())
{
    var row = line.FromJson<DataType>();
    // Process each row as needed
}
```

In this example, we start by obtaining a stream from the URL of the JSON Lines file using the `url.GetStreamFromUrlAsync()` method provided by the ServiceStack HTTP Utils package. Then, we iterate over each line of the stream using the `stream.ReadLinesAsync()` method. Finally, we deserialize each line into the desired data type and process it accordingly.

Using async streaming parsing ensures that you can process large JSON Lines files efficiently, without consuming excessive memory.

You can access JSON Lines data using the following methods:

- A `.jsonl` suffix on the URL
- An `Accepts: text/jsonl` header
- A `?format=jsonl` query string parameter

## Using the JsonlSerializer

If streaming the results is not necessary, ServiceStack also provides the `JsonlSerializer` class to directly serialize and deserialize JSON Lines data. The `JsonlSerializer` can be used in scenarios where you want to work with the JSON Lines format as a whole, rather than streaming it line by line.

Here's an example of how to use the `JsonlSerializer` to deserialize a JSON Lines string into a list of objects:

```csharp
var jsonl = await url.GetStringFromUrlAsync();
var objects = JsonlSerializer.DeserializeFromString<List<DataType>>(jsonl);
```

In this example, we start by fetching the JSON Lines data as a string using the `url.GetStringFromUrlAsync()` method. Then, we use the `JsonlSerializer` to deserialize the string into a list of objects of the desired data type.

The `JsonlSerializer` can also be used to serialize objects to the JSON Lines format:

```csharp
var jsonl = JsonlSerializer.SerializeToString(objects);
JsonlSerializer.SerializeToStream(objects, stream);
JsonlSerializer.SerializeToWriter(objects, textWriter);
```

## Indexing AI-Generated Art Albums with JSONL

In this part, we will walk you through a practical demonstration of using JSON Lines to index AI-generated art albums. We will fetch data from the Blazor Diffusion API, which provides "Creatives" generated based on user-provided text prompts. Our goal is to index the text and image URLs of these Creatives into a Typesense search server.

Here is the code snippet with a practical implementation of interacting with `creatives.jsonl` endpoint:

```csharp
const string ApiUrl = "https://localhost:5001/creatives.jsonl";

var provider = TypesenseUtils.InitProvider();
var typesenseClient = provider.GetRequiredService<ITypesenseClient>();
await typesenseClient.InitCollection();
var sw = new Stopwatch();
sw.Start();

var lastIndexedCreative = 0;

while (true)
{
    await using var stream = await ApiUrl.AddQueryParams(new() {
            ["IdGreaterThan"] = lastIndexedCreative,
            ["OrderBy"] = "Id",
        })
        .GetStreamFromUrlAsync();
    await foreach (var line in stream.ReadLinesAsync())
    {
        var creative = line.FromJson<Creative>();
        lastIndexedCreative = creative.Id;
        // processing
        if (creative.Artifacts.Count == 0)
            return;
        var imageId = creative.PrimaryArtifactId 
            ?? creative.CuratedArtifactId 
            ?? creative.Artifacts.First().Id;
        var artifact = creative.Artifacts.First(x => x.Id == imageId);
        var indexedCreative = new IndexedCreative
        {
            Text = creative.Prompt,
            Url = $"https://cdn.diffusion.works{artifact.FilePath}"
        };
        // Process and index the creative data
        await typesenseClient.CreateDocument("Creatives", indexedCreative);
    } 
    
    // sleep if no new data ..
}
```

Here we have the service client initialization with the `TypesenseUtils.InitProvider()`. Then, inside an infinite loop, we fetch data using `GetStreamFromUrlAsync()` and stream it line by line using the `ReadLinesAsync()` method. After processing each line, we index the creative item into typesense.

## Indexing Creative Data into Typesense

With Typesense, implementation of full-text search for our data can be achieved efficiently. Once we have fetched the Creative data from Blazor Diffusion's API, we can proceed to index it into Typesense for easy searching and analysis of the indexed documents. To index the Creative data, we're using [a C# client library built by the community](https://github.com/DAXGRID/typesense-dotnet).

Processed data are saved into IndexedCreative instances and are added into the Typesense server. Under the hood, the C# client library interacts with Typesense's API and handles all HTTP requests and responses for us.

### Memory Foot Print

Our application logs the memory usage over time, and we see it’s constant relative to the size of our process. Streamed JSONL parsing and async indexing allows us processing infinite datasets size without hitting the memory bounds.

## Skipping Already Indexed Data Example

For increased efficiency, instead of re-indexing all the data each time our program runs, we will only fetch and index data that hasn't been indexed already. We use `lastIndexedCreative` integer variable to keep track of the last creative indexed. On subsequent runs of our program, we fetch only new data by modifying the API URL to fetch Creative objects starting from the ID greater than `lastIndexedCreative`.

This is done by appending the `IdGreaterThan` query parameter to our AutoQuery endpoint.

```csharp
await using var stream = await ApiUrl.AddQueryParams(new() {
        ["IdGreaterThan"] = lastIndexedCreative,
        ["OrderBy"] = "Id",
    }).GetStreamFromUrlAsync();
```

## Visualizing Indexed Documents with Typesense Dashboard

Another great community feature is the [Typesense dashboard project](https://github.com/bfritscher/typesense-dashboard), providing a user-friendly interface to manage and browse collections in a Typesense search server. The dashboard, which can be accessed via the browser or as a downloadable desktop app, provides real-time statistics and overview over the indexed collections.

The indexed creative data can now be explored and analyzed using the dashboard's intuitive interface. You can apply filters, perform searches, and manipulate the stored data through the user interface.

## Conclusion

The JSON lines format is a versatile and efficient standard for working with large datasets. Its streamable properties make it particularly useful for handling big data and its ease of use makes it an ideal choice for developers working in any client language.

Streaming data directly to and from HTTP APIs — a major power of the JSON lines format — can dramatically improve the performance of data-intensive applications. Furthermore, ServiceStack's new JSON Lines support makes it straight forward serve and consume these streams.


# Using GitHub for Auto Deployments - Kubernetes Not Required
Source: https://servicestack.net/posts/kubernetes_not_required

## TL;DR

This post outlines a cost-effective and straightforward approach to deploying your web applications directly from GitHub to a Linux server, without needing Kubernetes. At its core, the process leverages GitHub infrastructure, Docker Compose, SSH, and a Linux server setup with an NGINX reverse proxy and LetsEncrypt for routing, TLS, and certificate management.

The process triggers automatic deployment upon a GitHub event such as a push to the main or master branch, or a new release. GitHub Actions automatically build and push Docker images to the GitHub Container Registry, then securely transfers the Docker Compose file to the remote server. The server pulls the new Docker image and starts the application using Docker Compose.

This system is not exclusive to any specific type of application; it is flexible enough to be adapted to any type of web application that can be containerized using Docker.

Important dependencies for this deployment process include:

- `DEPLOY_HOST`: IP address or domain name of the deployment server.
- `DEPLOY_USERNAME`: SSH username for the deployment server.
- `DEPLOY_KEY`: Private SSH key for the deployment server.
- `LETSENCRYPT_EMAIL`: Registration email for Let's Encrypt certificate.

These secrets are stored in GitHub Action secrets for secure use throughout the end-to-end deployment process. The deployment strategy also needs Docker-enabled Linux server, NGINX for managing routing and a Docker Compose file.

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="7dardvqBFbE" style="background-image: url('https://img.youtube.com/vi/7dardvqBFbE/maxresdefault.jpg')"></lite-youtube>
</div>

## Introduction

Every developer knows the struggle: you've written a fantastic application, it works flawlessly on your local machine, but now you need to get it out there in the world. The traditional cloud-native way often involves complex orchestration platforms like Kubernetes, which, while extremely powerful, can be overwhelming and overkill for smaller projects or applications with low traffic.

This is why we're going to explore an alternative and cost-effective approach in this blog post: deploying your web applications directly from GitHub to a Linux server, without the need for Kubernetes. This approach not only allows for affordable deployment of your applications but also promotes a quick and straightforward process, making it an excellent option for proof of concept or low-traffic applications.

This pattern we'll discuss consists of some core components: GitHub and its various functionalities, Docker Compose files, SSH for remote deployments, and a Linux server setup with an NGINX reverse proxy and LetsEncrypt companion containers for automation of routing, TLS, and certificate management.

By the end of this post, you will understand how to set up this deployment pattern. We will walk you through each step in detail, focusing on actionable, practical advice that you can apply right away.

Whether you're a .NET developer or working with other technologies, this pattern is flexible and can be adapted to your unique needs. As we progress, we will highlight how cost-effective and efficient this deployment strategy is for hosting your proof-of-concept, lower environment applications, or lower traffic systems, where horizontal scaling is not a necessity.

## Understanding the Core Components

Before we get our hands dirty with deployment, let's take a moment to understand the essential components involved in this process. We'll also explain why we've chosen each one and how they contribute to our this cost-effective deployment pipeline.

### GitHub and Its Functions

The backbone of our deployment pipeline is GitHub, a platform that provides a host of features to support our goals. Here are the key GitHub features we'll leverage:

- **GitHub Actions**: These are automated workflows that will handle the process of Continuous Integration (CI) for our application. It checks out code, sets up .NET Core, builds, tests, and creates releases. We'll go into detail about this in the practical section.
- **GitHub Action Secrets**: Sensitive information like login credentials, tokens, or keys shouldn't be exposed in your codebase. GitHub Action Secrets are encrypted and allow us to store such information securely, which is essential for building and pushing Docker images.
- **GitHub Container Registry (GHCR)**: This is where our Docker images will be stored and versioned. GitHub Actions will build and push Docker images to GHCR, from where our Linux server will pull them during deployment.
- **GitHub Repository**: The central location where our application's codebase is stored. This is also where we'll configure GitHub Actions and store our Docker Compose files.

### Docker Compose

Docker Compose is a tool that helps with the management of multi-container Docker applications. It allows us to define our application's environment in a YAML file (`docker-compose.yml`), enabling consistent setups across different environments. This will be installed on your target Linux server, where it will be used to pull Docker images and run our application.

### Secure Shell (SSH)

SSH is a protocol used for securely connecting to a remote server. In our deployment process, we'll use SSH for copying Docker Compose files and executing commands on the Linux server, such as pulling Docker images and running Docker Compose.
This is done via GitHub Actions where we will use the `ssh-action` to securely transfer the Docker Compose file to the Linux server, and execute commands remotely.

### Linux Server, NGINX, and LetsEncrypt

Our Linux server is where everything comes together. It's where our application will be hosted, and where the Docker Compose file will be executed. But that's not all; we'll set up NGINX reverse proxy on this server to manage HTTP requests and responses between clients and our application.

We're also adding LetsEncrypt into the mix. LetsEncrypt will work in conjunction with NGINX to automate routing, provide Transport Layer Security (TLS), and manage SSL certificates. This combination will ensure our application is secure and readily accessible.

By understanding these core components, we can now piece together our deployment pipeline. But before we proceed, it's essential to ensure you have access to the required tools. Make sure you have a GitHub account, Docker installed on your local machine, access to a Linux server, and the necessary knowledge to work with these tools. Don't worry if you're unsure about any steps; we'll guide you through each stage as we move forward.

## Setting Up Your Linux Server

The heart of your web application deployment, the Linux server, requires proper setup and configuration to ensure the deployment and operation of your applications. Here, we'll be using Ubuntu 22.04 as our target server, although you can adapt these instructions for other distributions as well.

Firstly, you need to ensure you have a Linux server at your disposal. This could be a virtual private server (VPS) from any cloud provider, a dedicated server, or even a Linux machine in your local network (provided it can be accessed via GitHub Actions).

Once you have access to your server, follow these steps:

### Installing Docker and Docker Compose

Docker is a crucial component that enables us to build, ship, and run applications inside containers. Docker Compose, on the other hand, streamlines the process of managing multi-container Docker applications. Here's how to install them:

1. **Update your server:** Before we start, let's make sure our server has the latest updates. Run the following commands:
    ```
    sudo apt update
    sudo apt upgrade -y
    ```
2. **Install Docker:** Docker provides an official guide to install Docker Engine on Ubuntu, which you can follow [here](https://docs.docker.com/engine/install/ubuntu/).

By default, this should come with the latest version of Docker Compose. However, if you need to install Docker Compose separately, you can follow the official guide [here](https://docs.docker.com/compose/install/).

After the installation, confirm that Docker and Docker Compose have been correctly installed by running `docker --version` and `docker compose`. The terminal should print the version of Docker and show command options for Docker Compose, respectively.

### Setting up NGINX and LetsEncrypt

Next, we will set up NGINX and LetsEncrypt in Docker containers. These are responsible for handling client requests, routing, SSL encryption, and certificate management.

- **Create the following YAML file** on your Linux server:

```yaml
version: "3.9"

services:
  nginx-proxy:
    image: nginxproxy/nginx-proxy
    container_name: nginx-proxy
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - conf:/etc/nginx/conf.d
      - vhost:/etc/nginx/vhost.d
      - html:/usr/share/nginx/html
      - dhparam:/etc/nginx/dhparam
      - certs:/etc/nginx/certs:ro
      - /var/run/docker.sock:/tmp/docker.sock:ro
    labels:
      - "com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy"

  letsencrypt:
    image: nginxproxy/acme-companion:2.2
    container_name: nginx-proxy-le
    restart: always
    depends_on:
      - "nginx-proxy"
    environment:
      - DEFAULT_EMAIL=you@example.com
    volumes:
      - certs:/etc/nginx/certs:rw
      - acme:/etc/acme.sh
      - vhost:/etc/nginx/vhost.d
      - html:/usr/share/nginx/html
      - /var/run/docker.sock:/var/run/docker.sock:ro

networks:
  default:
    name: nginx

volumes:
  conf:
  vhost:
  html:
  dhparam:
  certs:
  acme:
```

- **Start the containers:** Use the command `docker compose up -d` in the directory containing your `docker-compose.yml` file to start the NGINX reverse proxy and LetsEncrypt containers.

With Docker, Docker Compose, NGINX, and LetsEncrypt setup, your Linux server is ready for deployments. Note that these are one-off tasks. Once set up, you won't need to repeat them for deploying additional applications.

For subsequent deployments, GitHub Actions will take care of creating necessary directories and SCP copying the `docker-compose.yml` file from the repository to the target Linux server. This allows your deployment pipeline to be flexible, repeatable, and reliable.

![](./img/posts/docker-compose/linux-server.png)

## GitHub: The Backbone of Your Deployment Pipeline

In this section, we'll explore how to set up a GitHub-based deployment pipeline, using GitHub Actions for automation, GitHub Secrets for security, and the GitHub Container Registry for Docker image storage. You'll see how these tools can provide a powerful, end-to-end solution for building and deploying your applications.

### Understanding GitHub Actions

GitHub Actions allow you to create custom software development life cycle (SDLC) workflows directly in your GitHub repository. These workflows are described in YAML files and can be triggered by GitHub events (such as pushing code, creating releases, etc.), on a schedule, or manually.

In the provided YAML, the deployment workflow is triggered on three events:

1. A new GitHub release is published.
2. The build workflow has completed on `main` or `master` branches.
3. Manual trigger for rolling back to a specific release or redeploying the latest version.

The jobs are divided into two parts: `push_to_registry` and `deploy_via_ssh`. The first job builds a Docker image from the repository's code and pushes it to GitHub's container registry. The second job deploys the image to a remote server via SSH.

### Configuring GitHub Action Secrets

GitHub Secrets are encrypted environment variables created in your repository settings. They're a secure way to store and use sensitive information in GitHub Actions, like credentials, SSH keys, tokens, etc. Without them, this sensitive information would be exposed in your public repository.

The YAML script uses the following secrets:

- `DEPLOY_HOST`: The IP address or domain name of the server where the application is deployed.
- `DEPLOY_USERNAME`: The username used for SSHing into the deployment server.
- `DEPLOY_KEY`: The private SSH key for the deployment server.
- `LETSENCRYPT_EMAIL`: The email used for Let's Encrypt certificate registration.

These secrets can be set up in your repository settings under the `Secrets` section.

![](./img/posts/docker-compose/secrets-separation.PNG)

```markdown
Go to `Settings` -> `Secrets` -> `New repository secret`
```

### Leveraging GitHub Container Registry

The GitHub Container Registry is a place to store Docker images within GitHub, which can then be used in your GitHub Actions workflows or pulled directly to any server with Docker installed.

The following `release.yml` can be used with any dockerized application with a `docker-compose.yml`,`docker-compose.prod.yml` with an `app` and `app-migration` services, and the secrets above.

```yaml
name: Release
permissions:
  packages: write
  contents: write
on:
  # Triggered on new GitHub Release
  release:
    types: [published]
  # Triggered on every successful Build action
  workflow_run:
    workflows: ["Build"]
    branches: [main,master]
    types:
      - completed
  # Manual trigger for rollback to specific release or redeploy latest
  workflow_dispatch:
    inputs:
      version:
        default: latest
        description: Tag you want to release.
        required: true

jobs:
  push_to_registry:
    runs-on: ubuntu-22.04
    if: ${{ github.event.workflow_run.conclusion != 'failure' }}
    steps:
      # Checkout latest or specific tag
      - name: checkout
        if: ${{ github.event.inputs.version == '' || github.event.inputs.version == 'latest' }}
        uses: actions/checkout@v3
      - name: checkout tag
        if: ${{ github.event.inputs.version != '' && github.event.inputs.version != 'latest' }}
        uses: actions/checkout@v3
        with:
          ref: refs/tags/${{ github.event.inputs.version }}
          
      # Assign environment variables used in subsequent steps
      - name: Env variable assignment
        run: echo "image_repository_name=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
      # TAG_NAME defaults to 'latest' if not a release or manual deployment
      - name: Assign version
        run: |
          echo "TAG_NAME=latest" >> $GITHUB_ENV
          if [ "${{ github.event.release.tag_name }}" != "" ]; then
            echo "TAG_NAME=${{ github.event.release.tag_name }}" >> $GITHUB_ENV
          fi;
          if [ "${{ github.event.inputs.version }}" != "" ]; then
            echo "TAG_NAME=${{ github.event.inputs.version }}" >> $GITHUB_ENV
          fi;
      
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      # Build and push new docker image, skip for manual redeploy other than 'latest'
      - name: Build and push Docker images
        uses: docker/build-push-action@v3
        if: ${{ github.event.inputs.version == '' || github.event.inputs.version == 'latest' }}
        with:
          file: Dockerfile
          context: .
          push: true
          tags: ghcr.io/${{ env.image_repository_name }}:${{ env.TAG_NAME }}
  
  deploy_via_ssh:
    needs: push_to_registry
    runs-on: ubuntu-22.04
    if: ${{ github.event.workflow_run.conclusion != 'failure' }}
    steps:
      # Checkout latest or specific tag
      - name: checkout
        if: ${{ github.event.inputs.version == '' || github.event.inputs.version == 'latest' }}
        uses: actions/checkout@v3
      - name: checkout tag
        if: ${{ github.event.inputs.version != '' && github.event.inputs.version != 'latest' }}
        uses: actions/checkout@v3
        with:
          ref: refs/tags/${{ github.event.inputs.version }}

      - name: repository name fix and env
        run: |
          echo "image_repository_name=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
          echo "domain=${{ secrets.DEPLOY_HOST }}" >> $GITHUB_ENV
          echo "letsencrypt_email=${{ secrets.LETSENCRYPT_EMAIL }}" >> $GITHUB_ENV
          echo "TAG_NAME=latest" >> $GITHUB_ENV
          if [ "${{ github.event.release.tag_name }}" != "" ]; then
            echo "TAG_NAME=${{ github.event.release.tag_name }}" >> $GITHUB_ENV
          fi;
          if [ "${{ github.event.inputs.version }}" != "" ]; then
            echo "TAG_NAME=${{ github.event.inputs.version }}" >> $GITHUB_ENV
          fi;

      - name: Create .env file
        run: |
          echo "Generating .env file"

          echo "# Autogenerated .env file" > .env
          echo "HOST_DOMAIN=${{ secrets.DEPLOY_HOST }}" >> .env
          echo "LETSENCRYPT_EMAIL=${{ secrets.LETSENCRYPT_EMAIL }}" >> .env

          echo "IMAGE_REPO=${{ env.image_repository_name }}" >> .env
          echo "RELEASE_VERSION=${{ env.TAG_NAME }}" >> .env
      
      # Copy docker-compose and .env files to target server
      - name: copy files to target server via scp
        uses: appleboy/scp-action@v0.1.3
        with:
          host: ${{ secrets.DEPLOY_HOST }}
          username: ${{ secrets.DEPLOY_USERNAME }}
          port: 22
          key: ${{ secrets.DEPLOY_KEY }}
          source: "./docker-compose.yml,./docker-compose.prod.yml,./.env"
          target: "~/.deploy/${{ github.event.repository.name }}/"
      
      - name: Run remote db migrations
        uses: appleboy/ssh-action@v0.1.5
        env:
          APPTOKEN: ${{ secrets.GITHUB_TOKEN }}
          USERNAME: ${{ secrets.DEPLOY_USERNAME }}
        with:
          host: ${{ secrets.DEPLOY_HOST }}
          username: ${{ secrets.DEPLOY_USERNAME }}
          key: ${{ secrets.DEPLOY_KEY }}
          port: 22
          envs: APPTOKEN,USERNAME
          script: |
            echo $APPTOKEN | docker login ghcr.io -u $USERNAME --password-stdin
            cd ~/.deploy/${{ github.event.repository.name }}
            docker compose -f ./docker-compose.yml -f ./docker-compose.prod.yml pull
            docker compose -f ./docker-compose.yml -f ./docker-compose.prod.yml up app-migration

      # Deploy Docker image with your application using `docker compose up app` remotely
      - name: remote docker-compose up via ssh
        uses: appleboy/ssh-action@v0.1.5
        env:
          APPTOKEN: ${{ secrets.GITHUB_TOKEN }}
          USERNAME: ${{ secrets.DEPLOY_USERNAME }}
        with:
          host: ${{ secrets.DEPLOY_HOST }}
          username: ${{ secrets.DEPLOY_USERNAME }}
          key: ${{ secrets.DEPLOY_KEY }}
          port: 22
          envs: APPTOKEN,USERNAME
          script: |
            echo $APPTOKEN | docker login ghcr.io -u $USERNAME --password-stdin
            cd ~/.deploy/${{ github.event.repository.name }}
            docker compose -f ./docker-compose.yml -f ./docker-compose.prod.yml pull
            docker compose -f ./docker-compose.yml -f ./docker-compose.prod.yml up app -d
```

In the provided YAML, the image is built and pushed to the registry in the `push_to_registry` job with the `docker/build-push-action@v3` action.

```yml
- name: Build and push Docker images
  uses: docker/build-push-action@v3
  if: ${{ github.event.inputs.version == '' || github.event.inputs.version == 'latest' }}
  with:
    file: Dockerfile
    context: .
    push: true
    tags: ghcr.io/${{ env.image_repository_name }}:${{ env.TAG_NAME }}
```

After building the Docker image, it is tagged and pushed to the GitHub Container Registry.

## Using GitHub Action Secrets for Multiple Applications

In the YAML provided, the secrets are used in multiple places:

- `GITHUB_TOKEN`: This is a system-generated token used by GitHub Actions to authorize interactions with the GitHub API and other services. Within the context of this workflow, `GITHUB_TOKEN` is used to authenticate Docker, allowing it to push images to and pull images from the GitHub Container Registry. This is generated for you based on the permissions you give the GitHub Actions workflow.
- `DEPLOY_HOST`: This secret represents the hostname to which SSH connections will be made during the deployment phase. This could be an IP address or a subdomain, as long as it's correctly set with an A record pointing to your server.
- `DEPLOY_USERNAME`: This secret corresponds to the username required for logging into the deployment server via SSH. The value for this secret can vary depending on your server setup. For example, it could be 'ubuntu', 'ec2-user', 'root', etc., depending on the operating system and configuration of the deployment server.
- `DEPLOY_KEY`: This is the private SSH key used for remote access to your deployment server or application host. This secret is crucial for securing your SSH connections. It's important to generate this key securely and to store it safely within your GitHub secrets to ensure your server remains secure.
- `LETSENCRYPT_EMAIL`: This secret represents the email address used for Let's Encrypt certificate registration. Let's Encrypt provides free automated TLS (Transport Layer Security) certificates, which help secure the communication between your server and its clients. The email is used to receive important notices.

These secrets are utilized in both jobs, and they provide a secure way to use sensitive data across multiple steps.

```yml
# Copy only the docker-compose.yml to remote server home folder
- name: copy files to target server via scp
  uses: appleboy/scp-action@v0.1.3
  with:
     host: ${{ secrets.DEPLOY_HOST }}
     username: ${{ secrets.DEPLOY_USERNAME }}
     port: 22
     key: ${{ secrets.DEPLOY_KEY }}
     source: "./docker-compose.yml,./docker-compose.prod.yml,./.env"
     target: "~/.deploy/${{ github.event.repository.name }}/"
```

In the example above, secrets are used to provide the `scp-action` with necessary information to copy our `docker-compose.yml`,`docker-compose.prod.yml`, and generated `.env` file to the remote server.

## How They All Work Together

Having a clear grasp of the components involved in our deployment pipeline, it's now time to delve into how they work together end to end to deploy your dockerized application. This section will help you visualize the bigger picture, breaking down the deployment process into a series of understandable steps.

### Setting the Stage: The Deployment Process

The deployment process is a series of operations that involve our main actors: GitHub, Docker, and your Linux server. Here is how each step unfolds:

1. The process begins when the user triggers an event such as a push to the `main` or `master` branch, creates a release, or manually triggers a dispatch.
2. The GitHub repository responds to this by initiating the workflow defined in our GitHub Action.
3. The GitHub Action then retrieves the necessary secrets stored in GitHub Action Secrets. These are essential for secure operations such as logging into the GitHub Container Registry (GHCR).
4. After logging in, the GitHub Action builds the Docker image from your application's source code and pushes the image to GHCR.
5. Once the Docker image is securely stored in GHCR, the GitHub Action uses Secure Copy Protocol (SCP) to transfer the `docker-compose.yml`,`docker-compose.prod.yml`, and `.env` files to your remote Linux server.
6. Using SSH, the GitHub Action then logs into the remote server and runs the database migrations service defined in your `docker-compose.yml` file called the `app-migrate` service.
7. Finally, the GitHub Action instructs your server to pull the new Docker image from GHCR and start your application using Docker Compose.

![](./img/posts/docker-compose/sequence-diagram.PNG)

Remember, this series of steps repeats each time a change triggers the GitHub Action, ensuring that your application is continuously integrated and deployed.

### Flexibility

It's important to note that this process is not exclusive to a specific kind of application. The versatility of Docker allows us to adapt this process to deploy any web application that can be containerized using Docker, giving you a reliable and repeatable deployment process for a broad range of web applications.

## Monitoring with LazyDocker

LazyDocker is a terminal UI for both Docker and Docker Compose. It allows you to monitor your Docker containers and services in real-time, giving you a visual representation of your application's health.

Part of what makes LazyDocker such an awesome tool is that you can use it anywhere from a terminal.

![](./img/posts/docker-compose/lazydocker.png)

To run it as a docker container itself, you can use the following command.

:::sh
docker run --name lazydocker --rm -it -v /var/run/docker.sock:/var/run/docker.sock -v ~/.lazydocker:/.config/jessedufffield/lazydocker lazyteam/lazydocker
:::

Or better yet, use it as an alias in your `.bashrc` or `.zshrc` file.

:::sh
alias lazydocker="docker run --name lazydocker --rm -it -v /var/run/docker.sock:/var/run/docker.sock -v ~/.lazydocker:/.config/jessedufffield/lazydocker lazyteam/lazydocker"
:::

So you can run `lazydocker` easily from your terminal.

## Concrete Example: Deploying a .NET Application

Let's see how you can use it to deploy a specific application. In this section, we will look at how to deploy a .NET application using the `release.yml` workflow.
It needs 3 files to run, as well as some GitHub Actions secrets like `DEPLOY_HOST`, `DEPLOY_USERNAME`, `DEPLOY_KEY`, and `LETSENCRYPT_EMAIL`.

### docker-compose.yml

This file is used by Docker Compose to define the services that make up your application, so they can be run together in an isolated environment. 
The `release.yml` expects a web application service called `app` and a database migration service called `app-migration` to be declared.

```yaml
version: "3.9"
services:
   app:
      container_name: my_app
```

This file is the common denominator between your development and production environments. It is used by Docker Compose to build your application locally and by the `release.yml` workflow to deploy your application to your server.
We can control the behavior of Docker Compose for [local development by using the `docker-compose.override.yml` file](https://docs.docker.com/compose/multiple-compose-files/merge/), which is not used by the `release.yml` workflow.

### docker-compose.override.yml

The override file is used by default when calling the command `docker compose up` without any arguments.


```yaml
version: "3.9"
services:
  app:
    build: .
    ports:
      - "5000:80"
```

Here we are telling Docker Compose to build the application from the Dockerfile in the current directory and expose port 80 on the container to port 5000 on the host machine. This allows us to access the application locally at `http://localhost:5000`.

### docker-compose.prod.yml

This file is similar to `docker-compose.yml`, but it is specifically tailored for your production environment. Instead of building from the Dockerfile like in the development version, it pulls a specific image from a repository (`ghcr.io/${IMAGE_REPO}:${RELEASE_VERSION}`) which come from a `.env` file. 
It also connects to an external network (`nginx`) which is our reverse proxy on our target Linux server.

```yaml
version: "3.9"
services:
   app:
      image: ghcr.io/${IMAGE_REPO}:${RELEASE_VERSION}
      restart: always
      ports:
         - "80"
      container_name: ${APP_NAME}_app
      environment:
         VIRTUAL_HOST: ${HOST_DOMAIN}
         LETSENCRYPT_HOST: ${HOST_DOMAIN}
         LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL}

networks:
   default:
      external: true
      name: nginx

```

### Dockerfile

The Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In this example, it first creates a build environment based on `dotnet/sdk:6.0-focal`, copies your application code into the container, and restores any necessary .NET dependencies. It then switches to your application's directory and publishes your application in release mode. Finally, it creates the runtime environment based on `dotnet/aspnet:6.0-focal`, copies the built application from the previous stage, and sets the Docker entrypoint to run your application.

```dockerfile
# Example dotnet app built from Dockerfile
FROM mcr.microsoft.com/dotnet/sdk:6.0-focal AS build
WORKDIR /app

COPY . .
RUN dotnet restore

WORKDIR /app/MyApp
RUN dotnet publish -c release -o /out --no-restore

FROM mcr.microsoft.com/dotnet/aspnet:6.0-focal AS runtime
WORKDIR /app
COPY --from=build /out .
ENTRYPOINT ["dotnet", "MyApp.dll"]
```

### .env (generated)

This file, `.env`, is a plain text file that stores environment variables for your Docker containers. Docker Compose automatically looks for this file in the same deployment directory as your `docker-compose.yml` and `docker-compose.prod.yml` files, and variables defined in `.env` can be read into the Compose file. The variables in this file could be secrets, URLs, or other configuration values. The `.env` file itself is typically not included in source control and instead, you might include a `.env.example` file to highlight the required environment variables.
This means when using `docker-compose.yml` locally, you can have a `.env` file with your local settings, and when using `docker-compose.prod.yml` on your server, you can have a `.env` file with your production settings, for example.

```.env
HOST_DOMAIN=razor-pages.web-templates.io
LETSENCRYPT_EMAIL=team@servicestack.com
APP_NAME=razor-pages
IMAGE_REPO=netcoretemplates/razor-pages
RELEASE_VERSION=latest
```

## Applying the Deployment Pattern: A Practical Guide

The following step-by-step guide will show you how to create, prepare, and deploy your .NET application using best practices.

### 1. Building Blocks: Creating Your .NET Application

Kick-off your development process by creating your .NET application. In this example, we use the ServiceStack's `x` tool to create our web app from a template with the command `x new web MyApp`.

Your application will have a structure that may look something like this:

```
MyApp/
|-- MyApp/
|   |-- wwwroot/
|   |-- Configure.AppHost.cs
|   |-- Program.cs
|-- MyApp.ServiceModel/
|-- MyApp.ServiceInterface/
|-- MyApp.Tests/
|-- MyApp.sln
|-- Dockerfile
|-- docker-compose.yml
|-- docker-compose.prod.yml
|-- .gitignore
```

### 2. Join the Hub: Pushing Your Application to GitHub

Once your application is set up, you can push it to GitHub. JetBrains Rider IDE has a handy "Share On GitHub" functionality that streamlines this process. Here's how you use it:

- Right-click on your solution.
- Select `Git -> GitHub -> Share Project on GitHub`.
- Enter your repository name and description.
- Click `Share`.

Your .NET application is now on GitHub, ready for collaboration and continuous integration.

### 3. Implementing GitHub Actions for CI

To automate the build and test process every time you push to your GitHub repository, you can use GitHub Actions. A minimal .NET workflow file, `build.yml`, can be created and placed in the `.github/workflows` directory. Here's a minimal example:

```yaml
name: Build
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2

    - name: Setup .NET
      uses: actions/setup-dotnet@v1
      with:
        dotnet-version: '6.0.x'

    - name: Restore dependencies
      run: dotnet restore

    - name: Build
      run: dotnet build --no-restore
```

You can also add a `test` step to run your tests here.

This will run alongside any other workflows, like the `release.yml` outlined above, to ensure your application is always in a deployable state.
When the `Build` step completes, it will trigger the `release.yml` workflow.

### 4. Setting Up and Using GitHub Action Secrets

Next, you'll need to create secrets for sensitive data. In GitHub, you can store them as "Secrets". Follow these steps:

- Go to your GitHub repository and click on `Settings`.
- Click on `Actions`->`Secrets` in the left sidebar.
- Click on `New repository secret`.
- Enter the `Name` and `Value` of the secret.
- Click on `Add secret`.

Create the following secrets:

- `DEPLOY_HOST`: This secret represents the hostname to which SSH connections will be made during the deployment phase. This could be an IP address or a subdomain, as long as it's correctly set with an A record pointing to your server.
- `DEPLOY_USERNAME`: This secret corresponds to the username required for logging into the deployment server via SSH. 
- `DEPLOY_KEY`: This is the private SSH key used for remote access to your deployment server or application host. 
- `LETSENCRYPT_EMAIL`: This secret represents the email address used for Let's Encrypt certificate registration.

### 5. Finding Your Home: Create an A Record DNS Entry

To link your domain to your server, you need to create an A Record DNS entry. Here's a generalized step-by-step:

- Go to your DNS provider's website and sign in.
- Find the domain you want to update and go to its DNS records.
- Create a new record. For the type, select `A`.
- Enter your server's IP address in the `Value` or `Points to` field.
- Save the changes.

Now, you can populate the `DEPLOY_HOST` secret in GitHub with your DNS.
This will be used by the `release.yml` workflow to deploy your application to your server via SSH.

### 6. Configuring Linux Server with Docker

By now, you should have a Linux server with Docker installed, and the NGINX reverse proxy with LetsEncrypt running, as we did in the previous section.

Your server will also need to be accessible via SSH on port 22 so that GitHub Actions can deploy your application.

### 7. Commit Change and Deploy Application

Now, commit any changes and push to your GitHub repository. If you've set up your GitHub Actions correctly, the CI will kick off. After a successful build, your application will be deployed and accessible from your specified domain.

## Deploying Multiple Applications on the Same Server

### Same Pattern, Multiple Applications

One of the core strengths of Docker is the ability to isolate and manage applications in their individual containers. This means you can deploy numerous applications, each from its separate GitHub repository, onto the same Linux server. All these applications can coexist and operate independently, provided each one is correctly dockerized. The release pattern remains unchanged and consistent to apply.

```markdown
1. MyApp1/ -> GitHub Repo 1 -> Docker Image 1 -> Server (Container 1)
2. MyApp2/ -> GitHub Repo 2 -> Docker Image 2 -> Server (Container 2)
3. MyApp3/ -> GitHub Repo 3 -> Docker Image 3 -> Server (Container 3)
```

![](./img/posts/docker-compose/graph-diagram.PNG)

### Organization Secrets

To maintain a streamlined deployment process when dealing with multiple applications, you can utilize the GitHub Actions' Organization Secrets. This feature allows secrets to be shared across multiple repositories in an organization, which saves time and reduces redundancy. The only secret that needs to be set up individually for each repository would be the `DEPLOY_HOST` and corresponding DNS for each application.

To set up organization secrets:

- Go to your GitHub organization's settings.
- Click on `Secrets` in the left sidebar.
- Click on `New organization secret`.
- Enter the `Name` and `Value` of the secret.
- Click on `Add secret`.

Each deployed application now shares the organization-level secrets, streamline the setup process for new applications to a new DNS entry and `DEPLOY_HOST` environment variable.

And since every GitHub Repository in the organization is unique, there is no conflicting files deployed to the same server.

### Cost Optimization: Using a Single Server

In our previous post,[In pursuit of the best value US cloud provider](/posts/hetzner-cloud), we looked for the most cost-effective cloud provider. We found that Hetzner Cloud offers the best value for money, with a powerful server costing around $10 USD per month. This means you can deploy multiple applications on a single server, saving you money and resources.

To host lots of our demo applications, we are doing exactly this, meaning we have a per container of less than $0.50 USD per month.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/docker-compose/cloud-cost-comparison.PNG" alt="">
    </div>
</div>

## Conclusion

By avoiding Kubernetes, you can have less complex control your deployment process, and have more control over your applications. You can also save money by using a single server to host multiple applications. The release pattern is a flexible, yet effective way to deploy your applications, and it can be applied to any programming language or framework. 

## Feedback

If you have any feedback on this article, please let us know by commenting below, joining our [Discord](https://servicestack.net/discord), [GitHub Discussions](https://servicestack.net/ask), or [Customer Forums](https://forums.servicestack.net).


# Which RDBMS has the fastest .NET Bulk Insert implementation?
Source: https://servicestack.net/posts/bulk-insert-performance

The Bulk Insert support in the latest release of [OrmLite](https://docs.servicestack.net/ormlite/) enables accessing 
different efficient ways made available for [each RDBMS](https://docs.servicestack.net/ormlite/installation) for 
inserting large amounts of data from a .NET App, encapsulated behind OrmLite's new `BulkInsert` API:

```csharp
db.BulkInsert(rows);
```

## Bulk Insert Implementations

Where the optimal implementation for each RDBMS utilize the different available RDBMS-specific implementations below:

- **PostgreSQL** - Uses PostgreSQL's [COPY](https://www.postgresql.org/docs/current/sql-copy.html) 
command via Npgsql's [Binary Copy](https://www.npgsql.org/doc/copy.html) import
- **MySql** - Uses [MySqlBulkLoader](https://dev.mysql.com/doc/connector-net/en/connector-net-programming-bulk-loader.html)
feature where data is written to a temporary **CSV** file that's imported directly by `MySqlBulkLoader` 
- **MySqlConnector** - Uses [MySqlConnector's MySqlBulkLoader](https://mysqlconnector.net/api/mysqlconnector/mysqlbulkloadertype/)
implementation which makes use of its `SourceStream` feature to avoid writing to a temporary file
- **SQL Server** - Uses SQL Server's `SqlBulkCopy` feature which imports data written to an in-memory `DataTable` 
- **SQLite** - SQLite doesn't have a specific import feature, instead Bulk Inserts are performed using batches of [Multiple Rows Inserts](https://www.tutorialscampus.com/sql/insert-multiple-rows.htm)
to reduce I/O calls down to a configurable batch size
- **Firebird** - Is also implemented using **Multiple Rows Inserts** within an [EXECUTE BLOCK](https://firebirdsql.org/refdocs/langrefupd20-execblock.html)
configurable up to Firebird's maximum of **256** statements

### SQL Multiple Row Inserts

An efficient and compact alternative to inserting large amounts of data that enjoys broad support from all RDBMS's is
to use SQL's Multiple Insert Rows feature to insert multiple rows within a single INSERT statement:

```sql
INSERT INTO Contact (Id, FirstName, LastName, Age) VALUES 
(1, 'John', 'Doe', 21),
(2, 'Jane', 'Doe', 27),
(3, 'Jack', 'Doe', 42);
```

Normally OrmLite APIs uses parameterized statements however for SQL Multiple Row Inserts it uses inline rasterized values in order
to construct and send large SQL INSERT statements that avoids RDBMS's max parameter limitations, which if preferred can 
be configured to be used instead of its default optimal Bulk Insert implementation:

```csharp
db.BulkInsert(rows, new BulkInsertConfig {
    Mode = BulkInsertMode.Sql
});
```

### Batch Size

**Multiple Row Inserts** are sent in batches of **1000** (Maximum for SQL Server), Firebird uses a maximum of **256** 
whilst other RDBMS's can be configured to use larger batch sizes:

```csharp
db.BulkInsert(rows, new BulkInsertConfig {
    BatchSize = 1000
});
```

## Benchmarks

[All benchmarks](https://github.com/ServiceStack/ServiceStack/blob/main/ServiceStack.OrmLite/tests/ServiceStack.OrmLite.Benchmarks/Program.cs)
were run with [BenchmarkDotNet](https://benchmarkdotnet.org) running on an 
**macOS M2 Apple Macbook**, an **Ubuntu 22.04 Linux VM** and an **Windows 10 Intel iMac 5K**.   

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="3gO_OEWIyPo" style="background-image: url('https://img.youtube.com/vi/3gO_OEWIyPo/maxresdefault.jpg')"></lite-youtube>
</div>

Hopefully these benchmarks are informative in showing the performance you can expect from Bulk Inserts
for each popular RDBMS's across on different operating systems. For context, these benchmarks were run on the configurations below:

#### Apple M2 / 24GB RAM
 - macOS / ARM
 - .NET 6.0 / ARM
 - RDBMS run from local Docker containers

#### Linux VM / 7GB RAM
 - Ubuntu 22.04 LTS / x64
 - .NET 6.0 / x64
 - RDBMS run from local Docker containers

#### Intel iMac 5K / 24GB RAM
 - Windows 10 / x64
 - .NET 6.0 / x64
 - RDBMS run from local Windows installations

Performance is also affected by the quality and efficiency of the ADO .NET library implementations specific to each RDBMS,
which can result in non .NET languages yielding different results based on the quality of their data access implementations.

## Apple M2 Benchmarks

Apple M2 Benchmarks were run on an Apple M2 Macbook Air 15" / 24GB RAM, specifications reported by BenchmarkDotNet:

```txt
BenchmarkDotNet v0.13.6, macOS Ventura 13.4.1 (22F2083) [Darwin 22.5.0]
Apple M2, 1 CPU, 8 logical and 8 physical cores
.NET SDK 6.0.411
  [Host]     : .NET 6.0.19 (6.0.1923.31806), Arm64 RyuJIT AdvSIMD
  Job-VQWEED : .NET 6.0.19 (6.0.1923.31806), Arm64 RyuJIT AdvSIMD
```

All benchmarks were run against local databases with most RDBMS's installed and
[running from a Docker container](https://servicestack.net/posts/postgres-mysql-sqlserver-on-apple-silicon).

All Docker containers runs native ARM Builds whilst SQL Server's **AMD64** Docker image runs emulated on Rosetta using 
[Apple's Virtualization Framework](https://developer.apple.com/documentation/virtualization).

## Optimized Bulk Insert Performance

These benchmarks below uses the default optimal Bulk Insert implementation for each RDBMS:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [1.812, 17.066, 166.747]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [2.330, 20.224, 199.697]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [6.123, 14.389, 115.645]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [9.676, 64.389, 310.966]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [8.778, 64.427, 308.574]
        },
    ]
}"></chart-js>

:::{.text-xs .text-gray-500 .text-center}
_SQL Server results removed due to poor outlier performance_
:::

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |     1.812 ms |  0.0355 ms |  0.0520 ms |     1.813 ms |
| SQLite Disk    |    1.29x |     2.330 ms |  0.0463 ms |  0.0664 ms |     2.331 ms |
| PostgreSQL     |    3.38x |     6.123 ms |  0.2952 ms |  0.8705 ms |     6.280 ms |
| MySqlConnector |    4.84x |     8.778 ms |  0.2387 ms |  0.6962 ms |     8.821 ms |
| MySql          |    5.34x |     9.676 ms |  0.2860 ms |  0.8159 ms |     9.575 ms |
| SqlServer      |    7.27x |    13.182 ms |  0.3376 ms |  0.9795 ms |    12.987 ms |

#### Inserting 10,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| PostgreSQL     |       1x |    14.389 ms |  0.4020 ms |  1.1533 ms |    14.181 ms |
| SQLite Memory  |    1.19x |    17.066 ms |  0.3119 ms |  0.4269 ms |    16.899 ms |
| SQLite Disk    |    1.41x |    20.224 ms |  0.3869 ms |  0.4139 ms |    20.212 ms |
| MySql          |    4.47x |    64.389 ms |  1.3736 ms |  3.9411 ms |    65.195 ms |
| MySqlConnector |    4.48x |    64.427 ms |  1.4085 ms |  3.9496 ms |    64.854 ms |
| SqlServer      |    6.24x |    89.821 ms |  2.9396 ms |  8.6213 ms |    91.203 ms |

#### Inserting 100,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| PostgreSQL     |       1x |   115.645 ms |  3.6250 ms | 10.5744 ms |   111.401 ms |
| SQLite Memory  |    1.44x |   166.747 ms |  0.4975 ms |  0.4154 ms |   166.742 ms |
| SQLite Disk    |    1.73x |   199.697 ms |  3.8056 ms |  5.9249 ms |   201.890 ms |
| MySqlConnector |    2.67x |   308.574 ms |  6.0372 ms |  7.8501 ms |   307.322 ms |
| MySql          |    2.69x |   310.966 ms |  6.2043 ms | 10.3660 ms |   308.421 ms |
| SqlServer      |    7.22x |   835.181 ms | 16.5847 ms | 15.5133 ms |   836.200 ms |

#### Inserting 1,000,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| PostgreSQL     |       1x |   980.558 ms | 18.6512 ms | 39.3418 ms |   973.302 ms |
| SQLite Memory  |    1.73x | 1,697.041 ms | 12.3617 ms | 11.5632 ms | 1,692.348 ms |
| SQLite Disk    |    2.01x | 1,975.092 ms | 15.7094 ms | 13.1181 ms | 1,971.127 ms |
| MySqlConnector |    2.71x | 2,654.076 ms | 37.9302 ms | 33.6241 ms | 2,662.150 ms |
| MySql          |    2.71x | 2,661.294 ms | 29.3865 ms | 26.0504 ms | 2,661.826 ms |
| SqlServer      |    8.67x | 8,517.809 ms | 55.3398 ms | 49.0573 ms | 8,514.901 ms |
:::

Effectively showing PostgreSQL binary COPY is the fastest Bulk Insert implementation of all RDBMS providers,
we're also seeing the overhead of running SQL Server's **AMD64** images on ARM having a significant
impact on its performance.

## Multiple Inserts Rows Performance

These benchmarks show the performance of executing **Multiple Row Inserts** in batches of **1000**
which is a good example to show the comparative performance of each RDBMS for executing large SQL Insert statements:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [1.812, 17.066, 166.747]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [2.306, 20.149, 201.074]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [8.073, 48.502, 425.789]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [11.210, 98.179, 718.902]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [9.154, 79.905, 630.839]
        },
    ]
}"></chart-js>

:::{.text-xs .text-gray-500 .text-center}
_SQL Server results removed due to poor outlier performance_
:::

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |          Mean |       Error |      StdDev |
|----------------|---------:|--------------:|------------:|------------:|
| SQLite Memory  |       1x |      1.774 ms |   0.0331 ms |   0.0554 ms |
| SQLite Disk    |    1.30x |      2.306 ms |   0.0398 ms |   0.0531 ms |
| PostgreSQL     |    4.55x |      8.073 ms |   0.5297 ms |   1.5452 ms |
| MySqlConnector |    5.16x |      9.154 ms |   0.3355 ms |   0.9681 ms |
| MySql          |    6.32x |     11.210 ms |   0.3573 ms |   1.0252 ms |
| SqlServer      |   49.11x |     87.128 ms |   0.8415 ms |   0.7460 ms |

#### Inserting 10,000 Rows

| Database       | Relative |          Mean |       Error |      StdDev |
|----------------|---------:|--------------:|------------:|------------:|
| SQLite Memory  |       1x |     16.724 ms |   0.3183 ms |   0.3406 ms |
| SQLite Disk    |    1.20x |     20.149 ms |   0.2649 ms |   0.2212 ms |
| PostgreSQL     |    2.90x |     48.502 ms |   2.3704 ms |   6.9893 ms |
| MySqlConnector |    4.78x |     79.905 ms |   2.5416 ms |   7.4540 ms |
| MySql          |    5.97x |     98.179 ms |   2.5259 ms |   7.4080 ms |
| SqlServer      |   50.28x |    840.832 ms |   3.6211 ms |   2.8271 ms |

#### Inserting 100,000 Rows

| Database       | Relative |          Mean |       Error |      StdDev |
|----------------|---------:|--------------:|------------:|------------:|
| SQLite Memory  |       1x |    168.207 ms |   1.8704 ms |   1.4603 ms |
| SQLite Disk    |    1.20x |    201.074 ms |   3.7597 ms |   5.3920 ms |
| PostgreSQL     |    2.53x |    425.789 ms |   8.4799 ms |  12.6922 ms |
| MySqlConnector |    3.75x |    630.839 ms |  11.4516 ms |  19.1330 ms |
| MySql          |    4.27x |    718.902 ms |  14.0201 ms |  15.5833 ms |
| SqlServer      |   50.35x |  8,469.901 ms |  48.8046 ms |  43.2640 ms |

#### Inserting 1,000,000 Rows

| Database       | Relative |          Mean |       Error |      StdDev |
|----------------|---------:|--------------:|------------:|------------:|
| SQLite Memory  |       1x |  1,710.710 ms |   5.0494 ms |   4.2165 ms |
| SQLite Disk    |    1.34x |  1,949.376 ms |   7.3221 ms |   6.4909 ms |
| PostgreSQL     |    2.93x |  5,009.395 ms |  99.4490 ms |  93.0246 ms |
| MySqlConnector |    3.82x |  6,541.659 ms |  61.4450 ms |  57.4757 ms |
| MySql          |    4.04x |  6,905.332 ms |  72.1600 ms |  63.9680 ms |
| SqlServer      |   52.31x | 89,485.334 ms | 713.9731 ms | 596.1999 ms |

:::

Takeaways from these results show PostgreSQL continuing to be a star performer, consistently out-performing 
other distributed RDBMS's whilst SQL Server yields even worse comparative performances.

## Single Insert Performance

This benchmark measures the performance of multiple single inserts, i.e. using the traditional Single Insert ORM APIs 
when Bulk Insert is not available: 

<chart-js :data="{
    labels: [
        '1,000 Rows',
    ],
    datasets: [
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [224.2379]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [349.2328]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [1272.0975]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [1209.4689]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [978.0029]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |           Mean |        Error |       StdDev |         Median |
|----------------|---------:|---------------:|-------------:|-------------:|---------------:|
| SQLite Memory  |       1x |    15,417.3 μs |  1,820.82 μs |  5,311.41 μs |    18,153.2 μs |
| SQLite Disk    |   14.54x |   224,237.9 μs |  1,777.94 μs |  1,388.10 μs |   224,184.8 μs |
| PostgreSQL     |   22.65x |   349,232.8 μs |  3,255.67 μs |  2,541.81 μs |   349,404.4 μs |
| SqlServer      |   63.44x |   978,002.9 μs | 19,327.88 μs | 19,848.30 μs |   982,696.9 μs |
| MySqlConnector |   78.45x | 1,209,468.9 μs | 24,150.34 μs | 38,305.00 μs | 1,201,763.9 μs |
| MySql          |   82.51x | 1,272,097.5 μs | 24,790.10 μs | 38,595.20 μs | 1,287,374.8 μs |

Where we can see that PostgreSQL/Npgsql holds the performance crown for Multiple Single Inserts, in addition to
Multiple Rows Inserts and Bulk Inserts implementations.

We can the use this compare the relative performance benefits of Bulk Inserts vs Multiple Rows Inserts 
vs Single Inserts of each database for **1,000** records:

| Database       | Bulk Inserts | Multiple Rows Inserts | Single Row Inserts |
|----------------|-------------:|----------------------:|-------------------:|
| SQLite Disk    |           1x |                    1x |             97.24x |
| PostgreSQL     |           1x |                 1.32x |             57.04x |
| MySqlConnector |           1x |                 1.04x |            137.78x |
| MySql          |           1x |                 1.16x |            131.47x |
| SqlServer      |           1x |                 6.61x |             74.19x |

Effectively showing the cost of Single Inserts multiple I/O calls vs Bulk or SQL Batch Inserts - confirming that
Bulk Inserts offer much better performance when needing to insert a significant number of rows.

Relative performance for Inserting **10,000** records:

| Database       | Bulk Inserts | Multiple Rows Inserts |
|----------------|-------------:|----------------------:|
| SQLite Disk    |           1x |                    1x |
| PostgreSQL     |           1x |                 3.37x |
| MySqlConnector |           1x |                 1.24x |
| MySql          |           1x |                 1.52x |
| SqlServer      |           1x |                 9.36x |

Relative performance for Inserting **100,000** records:

| Database       | Bulk Inserts | Multiple Rows Inserts |
|----------------|-------------:|----------------------:|
| SQLite Disk    |           1x |                 1.01x |
| PostgreSQL     |           1x |                 3.68x |
| MySqlConnector |           1x |                 2.04x |
| MySql          |           1x |                 2.31x |
| SqlServer      |           1x |                10.14x |

Showing large batched SQL Insert statements staying within 2-3.7x performance range of their efficient Bulk Insert 
implementations, except for SQL Server which is an order of magnitude slower than `SqlBulkCopy`

:::

## Ubuntu Linux VM Benchmarks

The Ubuntu Linux VM Benchmarks were run on a GitHub Actions VM which are reported to run on a 2 Core CPU with 
[7 GB of RAM](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources),
specifications reported by BenchmarkDotnet:

```txt
BenchmarkDotNet v0.13.6, Ubuntu 22.04.2 LTS (Jammy Jellyfish)
Intel Xeon Platinum 8370C CPU 2.80GHz, 1 CPU, 2 logical and 2 physical cores
.NET SDK 6.0.412
  [Host]     : .NET 6.0.20 (6.0.2023.32017), X64 RyuJIT AVX2
  Job-HXEVFT : .NET 6.0.20 (6.0.2023.32017), X64 RyuJIT AVX2
```

## Optimized Bulk Insert Performance

These benchmarks below uses the default optimal Bulk Insert implementation for each RDBMS:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [3.025, 27.653, 303.348]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [4.520, 40.234, 417.378]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [5.646, 26.436, 211.951]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [10.597, 80.429, 670.859]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [53.698, 80.535, 693.690]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [12.201, 94.899, 986.676]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |      Mean |     Error |    StdDev |    Median |
|----------------|---------:|----------:|----------:|----------:|----------:|
| SQLite Memory  |       1x |  3.025 ms | 0.0527 ms | 0.1089 ms |  3.053 ms |
| SQLite Disk    |    1.49x |  4.520 ms | 0.0898 ms | 0.1794 ms |  4.576 ms |
| PostgreSQL     |    1.87x |  5.646 ms | 0.2275 ms | 0.6415 ms |  5.630 ms |
| MySqlConnector |    3.50x | 10.597 ms | 0.2045 ms | 0.1813 ms | 10.608 ms |
| SqlServer      |    4.03x | 12.201 ms | 0.2420 ms | 0.5845 ms | 12.104 ms |
| MySql          |   17.75x | 53.698 ms | 1.0640 ms | 2.0753 ms | 53.425 ms |

As the poor MySql performance is more prevalent the smaller number of rows are Bulk Inserted suggests it's the result of a 
1-time cost like writing to the temporary file, which is less of a overhead when more rows are inserted.

#### Inserting 10,000 Rows

| Database       | Relative |      Mean |     Error |    StdDev |    Median |
|----------------|---------:|----------:|----------:|----------:|----------:|
| PostgreSQL     |       1x | 26.436 ms | 1.8383 ms | 5.1848 ms | 23.624 ms |
| SQLite Memory  |    1.05x | 27.653 ms | 0.5439 ms | 1.0478 ms | 27.100 ms |
| SQLite Disk    |    1.52x | 40.234 ms | 0.7801 ms | 1.2817 ms | 40.189 ms |
| MySqlConnector |    3.04x | 80.429 ms | 1.6022 ms | 2.0833 ms | 80.585 ms |
| MySql          |    3.05x | 80.535 ms | 1.5778 ms | 2.5478 ms | 80.400 ms |
| SqlServer      |    3.59x | 94.899 ms | 1.8399 ms | 2.6388 ms | 95.128 ms |

PostgreSQL continues to shine on its native Linux platform which somehow has a magically efficient implementation that 
bests bulk inserts to an In Memory SQLite database?

#### Inserting 100,000 Rows

| Database       | Relative |       Mean |      Error |     StdDev |     Median |
|----------------|---------:|-----------:|-----------:|-----------:|-----------:|
| PostgreSQL     |       1x | 211.951 ms |  4.2186 ms | 11.1134 ms | 210.844 ms |
| SQLite Memory  |    1.43x | 303.348 ms |  5.9261 ms |  8.6864 ms | 305.627 ms |
| SQLite Disk    |    1.97x | 417.378 ms |  8.0392 ms |  8.6019 ms | 418.351 ms |
| MySqlConnector |    3.17x | 670.859 ms | 11.1014 ms |  9.8411 ms | 668.926 ms |
| MySql          |    3.28x | 693.690 ms |  8.6160 ms |  8.0594 ms | 691.702 ms |
| SqlServer      |    4.66x | 986.676 ms | 19.0059 ms | 18.6663 ms | 983.779 ms |
:::

Here we see that SQL Server AMD64 image is not just slow under Rosetta/ARM, it's just consistently slow on Linux -
surprising how much slower a multi-billion dollar commercial database is vs the Free and Open Source PostgreSQL.

## Multiple Inserts Rows Performance

These benchmarks show the performance of executing **Multiple Row Inserts** in batches of **1000** rows:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [3.117, 28.106, 302.656]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [4.543, 39.774, 424.378]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [7.481, 65.437, 649.366]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [12.025, 100.473, 917.979]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [15.204, 142.609, 1282.074]
        },
    ]
}"></chart-js>

:::{.text-xs .text-gray-500 .text-center}
_SQL Server results removed due to poor outlier performance_
:::

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |       Mean |     Error |    StdDev |     Median |
|----------------|---------:|-----------:|----------:|----------:|-----------:|
| SQLite Memory  |       1x |   3.117 ms | 0.0293 ms | 0.0274 ms |   3.116 ms |
| SQLite Disk    |    1.46x |   4.543 ms | 0.0898 ms | 0.2235 ms |   4.519 ms |
| PostgreSQL     |    2.40x |   7.481 ms | 0.1495 ms | 0.1325 ms |   7.469 ms |
| MySqlConnector |    3.86x |  12.025 ms | 0.2265 ms | 0.2946 ms |  11.966 ms |
| MySql          |    4.88x |  15.204 ms | 0.3003 ms | 0.7254 ms |  15.106 ms |
| SqlServer      |   39.61x | 123.473 ms | 2.2482 ms | 1.9930 ms | 123.270 ms |

#### Inserting 10,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |    28.106 ms |  0.5567 ms |  1.1621 ms |    27.552 ms |
| SQLite Disk    |    1.42x |    39.774 ms |  0.5604 ms |  0.4968 ms |    39.701 ms |
| PostgreSQL     |    2.33x |    65.437 ms |  1.2455 ms |  1.4343 ms |    65.431 ms |
| MySqlConnector |    3.57x |   100.473 ms |  1.9342 ms |  1.7146 ms |   100.253 ms |
| MySql          |    5.07x |   142.609 ms |  9.0717 ms | 26.3187 ms |   130.328 ms |
| SqlServer      |   41.47x | 1,165.550 ms | 11.0161 ms |  9.7655 ms | 1,167.982 ms |

#### Inserting 100,000 Rows

| Database       | Relative |          Mean |       Error |      StdDev |        Median |
|----------------|---------:|--------------:|------------:|------------:|--------------:|
| SQLite Memory  |       1x |    302.656 ms |   5.9832 ms |   6.4019 ms |    301.417 ms |
| SQLite Disk    |    1.40x |    424.378 ms |   6.9570 ms |   6.5076 ms |    424.492 ms |
| PostgreSQL     |    2.15x |    649.366 ms |  10.3512 ms |   9.6826 ms |    651.269 ms |
| MySqlConnector |    3.03x |    917.979 ms |   7.5867 ms |   6.7254 ms |    917.177 ms |
| MySql          |    4.24x |  1,282.074 ms |  25.3866 ms |  69.0662 ms |  1,277.800 ms |
| SqlServer      |   39.24x | 11,875.196 ms | 190.5093 ms | 178.2025 ms | 11,881.457 ms |

SQL Server's performance goes from slow to abysmal when executing large SQL Insert statements, effectively
suggesting there's no good way to import large amounts of data in a SQL Server Linux instance from code, you'll likely 
get better performance from an external solution like [bcp utility or SSIS](https://learn.microsoft.com/en-us/sql/relational-databases/import-export/bulk-import-and-export-of-data-sql-server).

:::

## Single Insert Performance

This benchmark measures the performance of multiple single inserts:

<chart-js :data="{
    labels: [
        '1,000 Rows',
    ],
    datasets: [
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [781.708]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [315.935]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [747.250]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [833.046]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [690.398]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |       Mean |      Error |     StdDev |     Median |
|----------------|---------:|-----------:|-----------:|-----------:|-----------:|
| SQLite Memory  |       1x |  15.970 ms |  0.3430 ms |  1.0004 ms |  15.334 ms |
| PostgreSQL     |   19.78x | 315.935 ms |  6.3063 ms | 16.6133 ms | 316.822 ms |
| SqlServer      |   43.23x | 690.398 ms | 13.6920 ms | 36.3094 ms | 685.644 ms |
| MySqlConnector |   46.79x | 747.250 ms | 14.5164 ms | 19.3789 ms | 748.131 ms |
| SQLite Disk    |   48.95x | 781.708 ms | 12.4966 ms | 10.4352 ms | 780.188 ms |
| MySql          |   52.16x | 833.046 ms | 16.5805 ms | 28.1550 ms | 837.304 ms |
:::

Here we see overhead cost of multiple I/O calls being significantly slower than an In Memory SQLite database whilst
PostgreSQL continues to be a standout performer vs other RDBMS's.

### Linux Benchmarks Takeaways

It's not clear if SQL Server's poor performance is due to having a suboptimal Linux port, running from Docker 
or running in a Linux VM with only 7GB RAM. 

Either way if you're running SQL Server on Linux you may want to consider running your own load tests to check you're 
not getting poor performance in your environment and whether or not you're better off running SQL Server on a Windows instance. 

## Intel iMac 5K Benchmarks

The Intel iMac 5K / 24GB RAM benchmarks were run on **Windows 10**, specifications reported by BenchmarkDotNet:

```txt
BenchmarkDotNet v0.13.6, Windows 10 (10.0.19045.3208/22H2/2022Update)
Intel Core i7-7700K CPU 4.20GHz (Kaby Lake), 1 CPU, 8 logical and 4 physical cores
.NET SDK 7.0.203
  [Host]     : .NET 6.0.20 (6.0.2023.32017), X64 RyuJIT AVX2
  Job-ILIRNE : .NET 6.0.20 (6.0.2023.32017), X64 RyuJIT AVX2
```

With all benchmarks running against local databases, installed using their native Windows Installers.

## Optimized Bulk Insert Performance

These benchmarks below uses the default optimal Bulk Insert implementation for each RDBMS:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [2.638, 23.424, 240.111]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 205, 86, 0.2)',
            borderColor: 'rgb(255, 205, 86)',
            borderWidth: 1,
            data: [9.884, 102.848, 1049.120]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [4.507, 17.796, 265.161]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [23.952, 145.037, 714.550]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [21.791, 143.624, 580.433]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [6.432, 53.330, 580.433]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |      Mean |     Error |    StdDev |    Median |
|----------------|---------:|----------:|----------:|----------:|----------:|
| SQLite Memory  |       1x |  2.638 ms | 0.0675 ms | 0.1981 ms |  2.660 ms |
| PostgreSQL     |    1.71x |  4.507 ms | 0.1548 ms | 0.4442 ms |  4.431 ms |
| SqlServer      |    2.44x |  6.432 ms | 0.1342 ms | 0.3763 ms |  6.319 ms |
| SQLite Disk    |    3.75x |  9.884 ms | 0.2986 ms | 0.8226 ms |  9.814 ms |
| MySqlConnector |    8.26x | 21.791 ms | 1.4271 ms | 4.0017 ms | 20.746 ms |
| MySql          |    9.08x | 23.952 ms | 1.3798 ms | 3.8234 ms | 22.884 ms |

#### Inserting 10,000 Rows

| Database       | Relative |       Mean |     Error |     StdDev |     Median |
|----------------|---------:|-----------:|----------:|-----------:|-----------:|
| PostgreSQL     |       1x |  17.796 ms | 0.3128 ms |  0.4486 ms |  17.743 ms |
| SQLite Memory  |    1.32x |  23.424 ms | 0.4573 ms |  0.5444 ms |  23.354 ms |
| SqlServer      |     3.0x |  53.330 ms | 0.7668 ms |  0.7173 ms |  53.499 ms |
| SQLite Disk    |    5.78x | 102.848 ms | 2.0194 ms |  5.2845 ms | 101.927 ms |
| MySqlConnector |    8.07x | 143.624 ms | 6.7792 ms | 19.3413 ms | 139.740 ms |
| MySql          |    8.15x | 145.037 ms | 6.1655 ms | 17.7889 ms | 141.211 ms |

#### Inserting 100,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |   240.111 ms |  2.6344 ms |  2.3353 ms |   239.452 ms |
| PostgreSQL     |    1.10x |   265.161 ms | 12.6705 ms | 36.9604 ms |   261.027 ms |
| SqlServer      |    2.42x |   580.433 ms |  9.3144 ms |  7.7779 ms |   580.560 ms |
| MySql          |    2.98x |   714.550 ms | 13.6676 ms | 28.5294 ms |   707.452 ms |
| MySqlConnector |    3.04x |   729.408 ms | 14.6735 ms | 43.2651 ms |   722.036 ms |
| SQLite Disk    |    4.37x | 1,049.120 ms | 20.9215 ms | 29.3290 ms | 1,045.173 ms |

#### Inserting 1,000,000 Rows

| Database       | Relative |     Mean |    Error |   StdDev |
|----------------|---------:|---------:|---------:|---------:|
| SQLite Memory  |       1x |  2.573 s | 0.0508 s | 0.0624 s |
| PostgreSQL     |    1.51x |  3.895 s | 0.0769 s | 0.2067 s |
| SqlServer      |    2.31x |  5.934 s | 0.1002 s | 0.0888 s |
| MySqlConnector |    2.56x |  6.585 s | 0.1253 s | 0.1287 s |
| MySql          |    2.64x |  6.803 s | 0.1339 s | 0.1920 s |
| SQLite Disk    |    4.33x | 11.146 s | 0.1228 s | 0.1149 s |

:::

Here we see that PostgreSQL continue to take the Bulk Insert crown platform even on Windows
and SQL Server is also a good performer now that it's running on its native Windows/Intel platform.

It also highlights a significant performance of SQLite disk write performance on Windows/x64 which may be due to
how [macOS implements fsync](https://news.ycombinator.com/item?id=30370551).

### Relative performance of ARM vs Intel

Whilst they're not directly comparable with their different configurations, their relative performance numbers
provides some indication of how much faster my new Macbook Air M2 is compared to my primary 
Intel iMac 5K Desktop, even with the overhead of running RDBMS's from within Docker containers:

<chart-js :data="{
    labels: [
        'M2 Macbook Air',
        'Intel iMac 5K'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [166.747, 240.111]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 205, 86, 0.2)',
            borderColor: 'rgb(255, 205, 86)',
            borderWidth: 1,
            data: [199.697, 265.161]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [115.645, 265.161]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [310.966, 714.550]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [308.574, 729.408]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [835.181, 580.433]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}
#### Relative performance for Bulk Insert of 100,000 records

| Database       | M2 ARM | Intel i7-7700K |
|----------------|-------:|---------------:|
| PostgreSQL     |     1x |          2.92x |
| SQLite Memory  |     1x |          1.44x |
| SQLite Disk    |     1x |          5.25x |
| MySqlConnector |     1x |          2.30x |
| MySql          |     1x |          2.30x |
| SqlServer      |  1.44x |             1x |
:::

Where other than SQL Server's poor performance on Linux/Rosetta, the benchmarks run much faster on Apple M2 Macbook Air.

## Multiple Inserts Rows Performance

These benchmarks show the performance of executing **Multiple Row Inserts** in batches of **1000** -
measuring the comparative performance of each RDBMS in executing large SQL Insert statements:

<chart-js :data="{
    labels: [
        '1,000 Rows',
        '10,000 Rows',
        '100,000 Rows'
    ],
    datasets: [
        {
            label: 'SQLite Memory',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [2.710, 25.784, 246.285]
        },
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(255, 205, 86, 0.2)',
            borderColor: 'rgb(255, 205, 86)',
            borderWidth: 1,
            data: [9.944, 105.551, 1048.217]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [7.270, 64.107, 522.203]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [24.016, 217.394, 1656.828]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [20.869, 184.193, 1359.005]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [75.994, 819.829, 0]
        }
    ]
}"></chart-js>

:::{.text-xs .text-gray-500 .text-center}
_Last SQL Server result removed due to poor outlier performance_
:::

:::{.table .table-striped .text-base}
#### Inserting 1,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |     2.710 ms |  0.0726 ms |  0.2084 ms |     2.745 ms |
| PostgreSQL     |    2.68x |     7.270 ms |  0.1439 ms |  0.2874 ms |     7.192 ms |
| SQLite Disk    |    3.67x |     9.944 ms |  0.3215 ms |  0.8692 ms |     9.754 ms |
| MySqlConnector |    7.70x |    20.869 ms |  1.2050 ms |  3.4380 ms |    19.956 ms |
| MySql          |    8.86x |    24.016 ms |  1.3810 ms |  3.9845 ms |    22.311 ms |
| SqlServer      |   28.04x |    75.994 ms |  1.3113 ms |  1.5610 ms |    76.456 ms |

#### Inserting 10,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |    25.784 ms |  0.6895 ms |  2.0005 ms |    26.021 ms |
| PostgreSQL     |    2.49x |    64.107 ms |  1.2164 ms |  1.0783 ms |    64.225 ms |
| SQLite Disk    |    4.09x |   105.551 ms |  2.3293 ms |  6.7579 ms |   103.989 ms |
| MySqlConnector |    7.14x |   184.193 ms |  4.3627 ms | 12.4471 ms |   180.157 ms |
| MySql          |    8.43x |   217.394 ms |  5.6931 ms | 16.6968 ms |   214.477 ms |
| SqlServer      |   31.80x |   819.829 ms | 16.1172 ms | 38.3043 ms |   829.895 ms |

#### Inserting 100,000 Rows

| Database       | Relative |         Mean |      Error |     StdDev |       Median |
|----------------|---------:|-------------:|-----------:|-----------:|-------------:|
| SQLite Memory  |       1x |   246.285 ms |  3.9630 ms |  3.3093 ms |   246.185 ms |
| PostgreSQL     |    2.12x |   522.203 ms |  9.1596 ms |  8.1198 ms |   519.607 ms |
| SQLite Disk    |    4.26x | 1,048.217 ms | 17.4291 ms | 14.5541 ms | 1,044.868 ms |
| MySqlConnector |    5.52x | 1,359.005 ms | 26.2161 ms | 29.1391 ms | 1,360.109 ms |
| MySql          |    6.73x | 1,656.828 ms | 32.2114 ms | 49.1902 ms | 1,649.606 ms |
| SqlServer      |   34.75x | 8,558.850 ms | 89.6498 ms | 83.8585 ms | 8,582.563 ms |

#### Inserting 1,000,000 Rows

| Database       | Relative |     Mean |    Error |   StdDev |
|----------------|---------:|---------:|---------:|---------:|
| SQLite Memory  |       1x |  2.332 s | 0.0288 s | 0.0255 s |
| PostgreSQL     |    2.61x |  6.098 s | 0.1199 s | 0.1559 s |
| SQLite Disk    |    4.76x | 11.099 s | 0.1153 s | 0.1022 s |
| MySqlConnector |    5.92x | 13.800 s | 0.1668 s | 0.1393 s |
| MySql          |     6.7x | 15.546 s | 0.3095 s | 0.3801 s |
| SqlServer      |   33.11x | 77.221 s | 0.5015 s | 0.4445 s |
:::

PostgreSQL continues to shine here, significantly out performing other distributed RDBMS's for processing large, multi-row
INSERT statements whilst SQL Server has a surprisingly poor showing on its native Windows/Intel platform - likely the 
result of a naive unoptimized implementation for these types of large SQL INSERT statements.

## Single Insert Performance

This benchmark measures the performance of multiple single inserts (i.e. when Bulk Insert is not available):

<chart-js :data="{
    labels: [
        '1,000 Rows',
    ],
    datasets: [
        {
            label: 'SQLite Disk',
            backgroundColor: 'rgba(201, 203, 207, 0.2)',
            borderColor: 'rgb(201, 203, 207)',
            borderWidth: 1,
            data: [7620.93]
        },
        {
            label: 'PostgreSQL',
            backgroundColor: 'rgba(153, 102, 255, 0.2)',
            borderColor: 'rgb(153, 102, 255)',
            borderWidth: 1,
            data: [465.43]
        },
        {
            label: 'MySQL',
            backgroundColor: 'rgba(54, 162, 235, 0.2)',
            borderColor: 'rgb(54, 162, 235)',
            borderWidth: 1,
            data: [4282.89]
        },
        {
            label: 'MySqlConnector',
            backgroundColor: 'rgba(255, 159, 64, 0.2)',
            borderColor: 'rgb(255, 159, 64)',
            borderWidth: 1,
            data: [4261.57]
        },
        {
            label: 'SQL Server',
            backgroundColor: 'rgba(255, 99, 132, 0.2)',
            borderColor: 'rgb(255, 99, 132)',
            borderWidth: 1,
            data: [582.33]
        }
    ]
}"></chart-js>

:::{.table .table-striped .text-base}

| Database       | Relative |        Mean |     Error |    StdDev |
|----------------|---------:|------------:|----------:|----------:|
| SQLite Memory  |       1x |    12.30 ms |  0.244 ms |  0.453 ms |
| PostgreSQL     |   37.84x |   465.43 ms |  9.082 ms | 10.812 ms |
| SqlServer      |   47.34x |   582.33 ms |  8.988 ms |  8.408 ms |
| MySqlConnector |  346.47x | 4,261.57 ms | 81.441 ms | 96.950 ms |
| MySql          |  348.20x | 4,282.89 ms | 83.393 ms | 99.273 ms |
| SQLite Disk    |  619.59x | 7,620.93 ms | 89.152 ms | 83.393 ms |

:::

We hope these results have been informative and have highlighted opportunities for improvements in your own systems
needing to perform large inserts or data imports. 

Feel free to leave any feedback or suggest improvements or other environments to run these benchmarks on in the comments.


# Install PostgreSQL, MySql and SQL Server on Apple Silicon
Source: https://servicestack.net/posts/postgres-mysql-sqlserver-on-apple-silicon

Becoming a [recent owner](https://twitter.com/demisbellot/status/1678256650753552384) of Apple's shiny new 15" Macbook Air
was my first foray into Apple's exciting new ARM chips where they've somehow managed to package all day battery life
with performance exceeding my primary Intel iMac 5K Desktop into the the thinnest of form factors. 

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/macbookair_2x.jpg)](https://www.apple.com/macbook-air/)

As a .NET developer that primarily develops on Windows/WSL and deploys to Linux x86 servers I've been hesitant to
move to Apple's new ARM hardware architecture with the potential friction and incompatibilities that might entail,
however with more ServiceStack Customers moving to Apple's new computers it's become increasingly important to 
also ensure we can deliver a great development experience of ServiceStack on Apple Silicon.

### Everything just works

What I wasn't expecting is for all the development tools I use daily would not only work flawlessly, but to do so natively
with all running native ARM builds:

 - Oh My Zsh
 - .NET SDK
 - Rider
 - Node.js
 - VS Code
 - Docker
 - GitHub Desktop
 - Discord
 - DataGrip

This has got to be the smoothest transition to a new general purpose CPU architecture in the history of computing of 
which credit should go to all involved, Apple for making new hardware so appealing to encourage mass adoption and 
all developers of these products for releasing native macOS ARM builds to support their Users on Apple Silicon.

### Installing all popular RDBMS's

Our [OrmLite](https://account.servicestack.net/ormlite/) multi & cross-platform ORM is the first product we want to profile 
and benchmark to ensure it also has great performance on Apple Silicon. For this I wanted to have local installations
of each supported Sqlite, PostgreSQL, MySql and SQL Server RDBMS to load test against, which as it was easier than it should be,
I wanted to document here so that it's hopeful useful for others needing to do the same thing. 

Installing developer tools has not only become frictionless, but in today's world of ubiquitous Docker adoption, it's
become much easier to run complex RDBMS software on macOS ARM, even SQL Server - the unlikeliest of combinations!

First thing you'll need to install is [Docker Desktop](https://www.docker.com/products/docker-desktop/) for macOS
by clicking on their [Apple Chip](https://desktop.docker.com/mac/main/arm64/Docker.dmg) download which will let you 
run each RDBMS in a containerized Docker App.

After Docker is running, installing and running PostgreSQL and MySql can be done with a single command:

### Install and run PostgreSQL

:::copy
docker run --name postgres -e POSTGRES_PASSWORD=p@55wOrd -p 127.0.0.1:5432:5432 -d postgres
:::

### Install and run MySql

:::copy
docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=p@55wOrd -d mysql:latest
:::

> Feel free to update commands to use your preferred strong password instead

### Install and run SQL Server

SQL Server requires more resources than the popular RDBMS's and as it doesn't have a native ARM Docker Image requires
a bit more configuration. 

First you'll want to ensure you have at least **4GB RAM** available to containers from 
the **Resources** Tab in Docker Settings you can open with `⌘,`

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/docker-resources.png)

My MacBook Air's **24GB RAM** configuration defaulted to **7.9 GB**, if you have a lower configuration you'll want to ensure
**4GB** is available to SQL Server.

Next in the **General** tab you'll want to ensure **Use Virtualization Framework** and **VirtioFS** is checked which will
allow SQL Server **AMD64** Image will run on Apple's new 
[Virtualization framework](https://developer.apple.com/documentation/virtualization):

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/docker-general.png)

After which you'll be able to install and run SQL Server with:

:::copy
docker run --platform=linux/amd64 --name mssql -e ACCEPT_EULA=1 -e MSSQL_SA_PASSWORD=p@55wOrd -p 1433:1433 -d mcr.microsoft.com/mssql/server:2022-latest
:::

You'll be able to check if all Docker containers are now running by clicking on the **Containers** tab in Docker Desktop:

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/docker-containers.png)

### lazydocker

That's pretty sweet, although I'm even more excited about 
[lazydocker](https://github.com/jesseduffield/lazydocker) as an alternative to Docker Desktop for managing 
Docker containers which can be installed with:

:::sh
brew install lazydocker
:::

As it's a Terminal UI it can be run everywhere where there's a Terminal, in local and remote terminals as well as 
Rider and VS Code's built-in Terminal UIs where you can quickly perform Docker tasks without breaking your development
workflow:

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/lazydocker.png)](https://github.com/jesseduffield/lazydocker)

## DataGrip

Now we can see they're all running, lets connect to them. You could use the command line tools specific to each database
but my preference is to use [JetBrains DataGrip](https://www.jetbrains.com/datagrip/) which lets you connect and manage 
any RDBMS from a single Desktop App, including many of the most popular NoSQL data stores.

### Connect to all Database connections

In **Database Explorer**, click on the `+` New Icon to add a new Data Source to **Microsoft SQL Server**, **MySql**
and **PostgreSQL** using the passwords used to run the Docker commands (e.g.`p@55wOrd`) and the default user names 
for each RDBMS:

 - SQL Server: `sa`
 - MySQL: `root`
 - PostgreSQL: `postgres`

After connecting to all databases you should end up with active connections to all empty databases:

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/datagrip-databases.png)](https://www.jetbrains.com/datagrip/)

Which you can open a **New > Query Console** or `⇧⌘L` to start executing generic queries against like `SELECT @@VERSION`
in SQL Server to display the version of SQL Server that's running:

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/datagrip-mssql-version.png)](https://www.jetbrains.com/datagrip/)

### Create Test Databases

But to do anything interesting lets create `test` database which you can create with `New > Database` for SQL Server and 
PostgreSQL or `New > Schema` in MySQL:

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/datagrip-test.png)](https://www.jetbrains.com/datagrip/)

### .NET Database Admin UI

Next thing to try is accessing them from the same Application, which ServiceStack .NET Apps have great
support for with its built-in [Database Admin UI](https://docs.servicestack.net/admin-ui-database) which lets you 
browse and query an App's configured databases:

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="NZkeyuc_prg" style="background-image: url('https://img.youtube.com/vi/NZkeyuc_prg/maxresdefault.jpg')"></lite-youtube>
</div>

You can easily try this out from the database-enabled [razor](https://razor.web-templates.io) project template which 
adopts our [Simple, Modern JavaScript](/posts/javascript) approach that uses the native 
JavaScript Modules support in modern Web Browsers to avoid any complex npm build tooling. 

We can create an App to test against your databases by installing the [.NET SDK](https://dotnet.microsoft.com/en-us/download) and
`x` [dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::sh
dotnet tool install --global x
:::

This will let you create any [ServiceStack Project Template](/start) with your preferred Project Name from the command-line, e.g:

:::sh
x new razor DatabaseTest
:::

Which creates a new .NET App that you can open with your preferred .NET IDE or text editor, e.g:

:::sh
code DatabaseTest/DatabaseTest
:::

By default the App is configured to use a local SQLite database, we can extend it to connect to different RDBMS's
by adding the necessary RDBMS and `AdminDatabaseFeature` NuGet packages in `DatabaseTest.csproj`:

```xml
<PackageReference Include="ServiceStack.OrmLite.MySql" Version="8.*" />
<PackageReference Include="ServiceStack.OrmLite.PostgreSQL" Version="8.*" />
<PackageReference Include="ServiceStack.OrmLite.SqlServer.Data" Version="8.*" />
<PackageReference Include="ServiceStack.Server" Version="8.*" />
```

New dependencies can be installed with VS Code's **Restore** popup or by explicitly running `dotnet restore`.

We can then register named connections for each of our databases by replacing the existing `Configure.Db.cs` with:

```csharp
public class ConfigureDb : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices((context,services) => {
            var dbFactory = new OrmLiteConnectionFactory(
                context.Configuration.GetConnectionString("DefaultConnection") ?? "App_Data/db.sqlite",
                SqliteDialect.Provider);

            dbFactory.RegisterConnection("postgres", 
                "Server=localhost;User Id=postgres;Password=p@55wOrd;Database=test;Pooling=true;MinPoolSize=0;MaxPoolSize=200",
                PostgreSqlDialect.Provider);

            dbFactory.RegisterConnection("mysql", 
                "Server=localhost;User Id=root;Password=p@55wOrd;Database=test;Pooling=true;MinPoolSize=0;MaxPoolSize=200",
                MySqlDialect.Provider);

            dbFactory.RegisterConnection("mssql", 
                "Server=localhost;User Id=sa;Password=p@55wOrd;Database=test;MultipleActiveResultSets=True;Encrypt=False;",
                SqlServer2012Dialect.Provider);

            services.AddSingleton<IDbConnectionFactory>(dbFactory);
        })
        .ConfigureAppHost(appHost => {
            // Enable built-in Database Admin UI at /admin-ui/database
            appHost.Plugins.Add(new AdminDatabaseFeature());
        });
}
```

This will now let us access the [registered databases](https://docs.servicestack.net/ormlite/getting-started#multiple-database-connections)
in our APIs, but first lets populate the databases with some data. 

### Multi Database Migrations

When a new project is created it populates its default configured SQLite database with some test data, we can do the same 
for the other registered database by duplicating the App's initial [DB migration](https://docs.servicestack.net/ormlite/db-migrations) 
to a new DB `Migration1001.cs` with:

:::sh
sed "s/1000/1001/" ./Migrations/Migration1000.cs > ./Migrations/Migration1001.cs
:::

Then annotating it with a `[NamedConnection]` attribute for each registered database:

```csharp
[NamedConnection("mssql")]
[NamedConnection("mysql")]
[NamedConnection("postgres")]
public class Migration1001 : MigrationBase
{
    //...
}
```

That can then be executed with:

:::sh
npm run migrate
:::

Where it will execute all new DB Migrations, in this case apply the same Migration to each configured database.

Now that our App's databases are all populated and ready to go, we can run it with: 

:::sh
npm run dev
:::

Then view the built-in Admin Database UI at:

:::sh
https://localhost:5001/admin-ui/database
:::

and signing in with the Admin user created in `Configure.AuthRepository.cs`:

 - `admin@email.com`
 - `p@55wOrd`

Where it displays all the App's configured database tables on its home page:

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/admin-db-home.png)

Whose contents can be viewed by drilling down and clicking on each table:  

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/admin-db-mssql-bookings.png)

Which displays its rows using the [AutoQuery Grid Vue Component](https://docs.servicestack.net/vue/autoquerygrid) that
can be sorted and filtered as needed:

![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/admin-db-postgres-coupons.png)

## Vue .mjs project template features

Whilst you have the App running, check out its other high-productivity features:

### Create a multi-user Booking system with AutoQuery

The App's Bookings APIs are built using [AutoQuery CRUD](https://docs.servicestack.net/autoquery-crud), allowing for 
rapid development of typed CRUD Services using only declarative POCO DTOs:

<div class="not-prose text-center">
    <a class="text-xl text-indigo-600" href="https://localhost:5001/bookings-auto">https://localhost:5001/bookings-auto</a>
</div>
<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="rSFiikDjGos" style="background-image: url('https://img.youtube.com/vi/rSFiikDjGos/maxresdefault.jpg')"></lite-youtube>
</div>

In addition, all AutoQuery APIs benefit from the built-in [Locode's](https://docs.servicestack.net/locode/) Auto Management UI:

<div class="not-prose text-center">
    <a class="text-xl text-indigo-600" href="https://localhost:5001/locode">https://localhost:5001/locode</a>
</div>

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/db-test-locode.png)](https://docs.servicestack.net/locode/)

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="hkuO_DMFXmc" style="background-image: url('https://img.youtube.com/vi/hkuO_DMFXmc/maxresdefault.jpg')"></lite-youtube>
</div>

As well as end-to-end typed integrations with the most [popular programming languages](/service-reference) accessible 
from the [code tab](https://docs.servicestack.net/api-explorer#code-tab) of the built-in 
[API Explorer](https://docs.servicestack.net/api-explorer):

<div class="not-prose text-center">
    <a class="text-xl text-indigo-600" href="https://localhost:5001/ui/QueryBookings?tab=code">https://localhost:5001/ui/QueryBookings?tab=code</a>
</div>

[![](/img/posts/postgres-mysql-sqlserver-on-apple-silicon/db-test-ui-code.png)](https://docs.servicestack.net/api-explorer)

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="lUDlTMq9DHU" style="background-image: url('https://img.youtube.com/vi/lUDlTMq9DHU/maxresdefault.jpg')"></lite-youtube>
</div>

I hope this has been an informative post and highlighted some cool products and features, any questions or feedback 
is welcome by commenting below.


# Trying Microsoft's Semantic Kernel
Source: https://servicestack.net/posts/semantic-kernel-gptmeetngs

In the rapidly evolving landscape of AI, a constant challenge that developers face is the integration and use of advanced models, like OpenAI's Large Language Models (LLMs), into our applications. To help bridge the gap between developers and these intricate models, Microsoft has introduced Semantic Kernel, a .NET library that offers a robust and more interactive platform to leverage OpenAI's ChatGPT API.

## The Journey So Far

Our [initial demonstration of the GPT Meeting Agent](https://github.com/NetCoreApps/GPTMeetingAgent/tree/42e18e9c6e7f9809760e3fecf40208d5759b18a3) offered an exciting glimpse into how developers could leverage the reasoning capability of Large Language Models such as `gpt-3.5-turbo` with the ServiceStack framework. We [showcased an application](https://github.com/NetCoreApps/GPTMeetingAgent) that could use In Context Learning, Chain Of Thought Prompting, and the defining of your own ServiceStack services using TypeScript definition format.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="7vChIGHWPuI" style="background-image: url('https://img.youtube.com/vi/7vChIGHWPuI/maxresdefault.jpg')"></lite-youtube>
</div>

The LLM was then able to determine which ServiceStack service should be called and what parameter values to use. This demonstration included APIs for searching for users, checking availability, reserving meeting room resources, and creating a booking. The application was then integrated with ServiceStack's Vue.js library, generating UI forms for each service the AI called. This not only illustrated what the Agent was doing but also gave the user control of more critical APIs like booking the actual meeting.

This integration was accomplished directly through OpenAI's ChatGPT API. While it served its purpose, we wanted to make it easier for those that had projects that already leveraged OpenAI integration. Microsoft's Semantic Kernel library offered a solution that would allow us to do just that, and is looking to be a promising avenue for future development.

## The Shift to Semantic Kernel

Enter [Microsoft's Semantic Kernel](https://github.com/microsoft/semantic-kernel). This .NET Library makes the integration with OpenAI's ChatGPT API far more flexible and developer-friendly. It allows developers to interact with LLMs as 'functions', catering to a wide range of use cases.

By switching the API integration with Semantic Kernel for the GPT Meeting Agent, we've made it easier to incorporate a custom Agent with your ServiceStack services, and this is by just switching over how the LLM APIs are integrated. The Semantic Kernel makes configuration easier and opens the door to the use of Open Source Large Language Models without the need for significant changes to the codebase, as well as future improvements as this library evolves.

```csharp
var kernel = Kernel.Builder
    .WithOpenAIChatCompletionService("gpt-3.5-turbo", chatGptApiKey)
    // Easy to switch out to custom implementations.
    //.WithOobaboogaApiChatCompletionService("http://localhost:5010/api/v1/generate");
    .Build();
```

## Advantages of Semantic Kernel

Semantic Kernel allows you to encapsulate your interaction with the LLMs, giving you the ability to use the model as a function. This abstraction facilitates the implementation of use cases where you need the model to serve as an endpoint that can handle and respond to a diverse array of requests. It means your application can stay relatively lean, focusing more on managing the responses from these models than worrying about their specific implementation details.

## Moving Forward: ServiceStack and LLM Integration

Currently, at the time of writing, the `function` support from OpenAI is not yet available via the Semantic Kernel library. But, we see this as a future improvement opportunity, which could further enhance the Semantic Kernel library's functionality and streamline its integration with OpenAI's APIs.

Our vision for the future involves evaluating further integration with Semantic Kernel to add more value to your ServiceStack projects if they need to leverage LLMs.

The goal is to create a more efficient and streamlined development experience that simplifies the integration of LLMs into your ServiceStack applications. The developer's focus should be on the business logic, the services that they are creating, and not on the technical complexities of integrating with AI models, this is where we think we can add value to .NET developers.

In the context of the GPT Meeting Agent demonstration, this would mean even smoother interactions between the AI agent and the ServiceStack services, resulting in a more intuitive and responsive user experience. By being able to leverage ServiceStack UI features like our Vue.js library's AutoForm components, we hope to make it a lot easier to create new ways of interacting with these models, like the ways we have demonstrated in the GPT Meetings applications.

## Wrapping Up

Microsoft's Semantic Kernel provides an avenue for developers to interact more flexibly with Large Language Models, opening up new possibilities and potential use cases. By integrating the Semantic Kernel library in our GPT Meeting Agent demonstration, we've made our application more configurable and opened the door for integrating Open Source Large Language Models without major codebase alterations.

Through further development, we aim to make this technology easier to leverage in your ServiceStack applications, helping you to create more intelligent, responsive, and effective software.


# Introducing Razor Press
Source: https://servicestack.net/posts/razor-press

In a nutshell the new [Razor Press template](https://razor-press.web-templates.io) is a Simple, Powerful, Fast, Flexible & FREE! Use Razor Pages to generate 
beautiful Tailwind static websites powered by Markdown & enhanced with Vue Auto deploys to GitHub Pages to Host 
for FREE on GitHub CDN

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="uqEa_DfFFDQ" style="background-image: url('https://img.youtube.com/vi/uqEa_DfFFDQ/maxresdefault.jpg')"></lite-youtube>
</div>

After having finished porting [docs.servicestack.net to Razor SSG](/posts/new_razor_ssg_docs), we've extracted the core
functionality useful for documentation and content centric websites into a reusable project template you can create
with the [x dotnet tool](https://docs.servicestack.net/dotnet-tool):

:::sh
x new razor-press ProjectName
:::

Live Demo: [razor-press.web-templates.io](https://razor-press.web-templates.io)

## What is Razor Press?

Razor Press is a Razor Pages powered Markdown alternative to Ruby's Jekyll, Vue & VitePress that's ideal for
generating fast, static content-centric & documentation websites. Inspired by [VitePress](https://vitepress.dev),
it's designed to effortlessly create documentation around content written in Markdown, rendered using C# Razor Pages
and beautifully styled with [tailwindcss](https://tailwindcss.com) and [@tailwindcss/typography](https://tailwindcss.com/docs/typography-plugin).

The resulting statically generated HTML pages can easily be deployed anywhere, where it can be hosted by any HTTP Server or CDN.
By default it includes GitHub Actions to deploy it your GitHub Repo's **gh-pages** branch where it's hosted for FREE
on [GitHub Pages](https://pages.github.com) CDN which can be easily configured to use your
[Custom Domain](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site).

## Use Cases

Razor Press utilizes the same technology as
[Razor SSG](https://razor-ssg.web-templates.io/posts/razor-ssg) which is the template we recommend for developing any
statically generated sites with Razor like Blogs, Portfolios, and Marketing Sites as it includes more Razor & Markdown
features like blogs and integration with [Creator Kit](https://servicestack.net/creatorkit/) - a companion OSS project
offers the necessary tools any static website can use to reach and retain users, from managing subscriber mailing lists to
moderating a feature-rich comments system.

Some examples built with Razor SSG include:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="https://servicestack.net">
        <img class="p-2" src="https://docs.servicestack.net/img/pages/ssg/servicestack.net-home-1440.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">servicestack.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://diffusion.works">
        <div style="max-height:350px;overflow:hidden">
        <img class="p-2" src="https://servicestack.net/img/posts/vue-diffusion/vuediffusion-search.png"></div>
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">diffusion.works</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://jamstacks.net">
        <img class="p-2" src="https://docs.servicestack.net/img/pages/release-notes/v6.9/jamstacks-screenshot.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">jamstacks.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://xkcd.netcore.io">
        <img class="p-2" src="https://docs.servicestack.net/img/pages/release-notes/v6.9/xkcd-screenshot.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">xkcd.netcore.io</div>
    </a>
</div>

## Documentation

Razor Press is instead optimized for creating documentation and content-centric websites, with built-in features useful
for documentation websites including:

- Customizable Sidebar Menus
- Document Maps
- Document Page Navigation
- Autolink Headers

#### Markdown Extensions

- Markdown Content Includes
- Tip, Info, Warning, Danger sections
- Copy and Shell command widgets

But given **Razor Press** and **Razor SSG** share the same implementation, their features are easily transferable, e.g.
The [What's New](/whatsnew) and [Videos](/videos) sections are
[features copied](https://razor-ssg.web-templates.io/posts/razor-ssg#whats-new-feature) from Razor SSG as they can be
useful in Documentation websites.

## Customizable {#custom-anchor .custom}

The source code of all Markdown and Razor Pages features are included in the template with all Markdown extensions
implemented in the [Markdown*.cs](https://github.com/NetCoreTemplates/razor-press/tree/main/MyApp) files allowing for
easier inspection, debugging and customization.

To simplify updating Markdown features in future we recommend against modifying the included `Markdown.*` files and instead
add any Markdig pipeline extensions or custom containers using `MarkdigConfig` in `Configure.Ssg.cs`:

```csharp
MarkdigConfig.Set(new MarkdigConfig
{
    ConfigurePipeline = pipeline =>
    {
        // Extend Markdig Pipeline
    },
    ConfigureContainers = config =>
    {
        config.AddBuiltInContainers();
        // Add Custom Block or Inline containers
    }
});
```

### Update Markdown Extensions & Dependencies

Updating to the latest JavaScript dependencies and Markdown extensions can be done by running:

:::sh
npm install
:::

Which as the template has no npm dependencies, is just an alias for running `node postinstall.js`

## Example

The largest website generated with Razor Press is currently the ServiceStack's documentation at
[docs.servicestack.net](https://docs.servicestack.net):

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/?light">
        <img class="p-2" src="https://servicestack.net/img/posts/razor-ssg/docs.servicestack.net.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/?dark">
        <img class="p-2" src="https://servicestack.net/img/posts/razor-ssg/docs.servicestack.net-dark.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>

A **500+** pages documentation website ported from VitePress, which prompted the creation of Razor Press after
experiencing issues with VitePress's SSR/SPA model whose workaround became too time consuming to maintain.

The new Razor SSG implementation now benefits from Razor Pages flexible layouts and partials where pages can be optionally
implemented in just markdown, Razor or a hybrid mix of both. The [Vue](https://docs.servicestack.net/vue/) splash page is an example of this implemented in a custom
[/Vue/Index.cshtml](https://github.com/NetCoreTemplates/razor-press/blob/main/MyApp/Pages/Vue/Index.cshtml) Razor Page.

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/vue/">
        <img class="p-2" src="https://docs.servicestack.net/img/pages/ssg/razor-pages-vue.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>

## Feedback & Feature Requests Welcome

Up to this stage [docs.servicestack.net](https://docs.servicestack.net) has been the primary driver for Razor Press
current feature-set, re-implementing all the previous VitePress features it used with C#, Razor Pages and Markdig extensions.

In future we'll look at expanding this template with generic Markdown features suitable for documentation or content-centric
websites, for which we welcome any feedback or new feature requests at:

<div class="not-prose">
   <h3 class="m-0 py-8 text-3xl text-center text-blue-600"><a href="https://servicestack.net/ideas">https://servicestack.net/ideas</a></h3>
</div>


# JSON Patch secrets into appsettings.json
Source: https://servicestack.net/posts/using-json-patch

Another useful feature in the ServiceStack `x` [dotnet tool](https://docs.servicestack.net/dotnet-tool) I wanted to highlight
is its built-in support for JSON Patching. This feature provides you with a robust mechanism for modifying your JSON files, 
providing a precise and granular way to manage configurations, that's especially useful when automating changes from a 
continuous integration environment.

## What is JSON Patching?

JSON Patch, as specified in [RFC 6902](https://tools.ietf.org/html/rfc6902), is a format for expressing a sequence of 
operations to apply to a JSON document. It allows us to add, remove, replace, copy, move and test elements within 
the JSON structure, making it a versatile tool in managing and altering configurations.

Here's a quick overview of the supported operations:

| Operation   | Notes                                                                                                  |
|-------------|--------------------------------------------------------------------------------------------------------|
| **add**     | Adds a new property or array element. For an existing property, it sets a new value.                   |
| **remove**  | Removes a property or array element.                                                                   |
| **replace** | Same as 'remove' followed by 'add' at the same location.                                               |
| **move**    | Same as 'remove' from the source followed by 'add' to the destination using the value from the source. |
| **copy**    | Same as 'add' to the destination using the value from the source.                                      |
| **test**    | Returns a success status code if the value at the path equals the provided value.                      |

The `x` dotnet tool implements these operations, making it a handy utility for managing your JSON configurations.

## An Example with SMTP Settings

Let's dive into a practical example. We'll be working with an `appsettings.json` file, a common sight in .NET projects, 
which often houses sensitive settings like SMTP config. Suppose we have the following structure with an empty `smtp` object:

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "smtp": {}
}
```

We need to fill this `smtp` object with settings such as username, password, host, port, and more. To automate filling 
these values, we can use the ServiceStack `x` tool to apply a `json.patch`. 

The `json.patch` file to accomplish this would look something like:

```json
[
  {
    "op": "add",
    "path": "/smtp",
    "value": {
      "UserName": "AWS_ACCESS_KEY_ID",
      "Password": "AWS_SECRET_ACCESS_KEY",
      "Host": "email-smtp.us-east-1.amazonaws.com",
      "Port": 587,
      "From": "email address",
      "FromName": "From Name",
      "Bcc": "bcc email address"
    }
  }
]
```

Once this patch is applied, our `appsettings.json` transforms into:

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "smtp": {
    "UserName": "AWS_ACCESS_KEY_ID",
    "Password": "AWS_SECRET_ACCESS_KEY",
    "Host": "email-smtp.us-east-1.amazonaws.com",
    "Port": 587,
    "From": "email address",
    "FromName": "From Name",
    "Bcc": "bcc email address"
  }
}
```

You can apply this patch using the `x` tool's `patch` command:

```bash
x patch appsettings.json.patch
```

This expects both the `appsettings.json.patch` and `appsettings.json` files to be local. Optionally, you can specify 
both files if their names differ.

```bash
x patch changes.json.patch appsettings.json
```

## Using in CI Environments

Another significant benefit of this feature is the convenience it provides in CI environments, such as GitHub Actions. 
Secrets and other environment-specific configurations often need to be injected during the CI process. 
This is where JSON patching can be extremely useful.

Consider the following step in a GitHub Actions workflow:

```yml
- name: Apply SMTP Settings
  working-directory: ./MyApp
  run: | 
    cat <<EOF >> appsettings.json.patch
    ${{ secrets.APPSETTINGS_PATCH}}
    EOF
    x patch appsettings.json.patch
```

Here, the SMTP settings are stored securely as GitHub secrets as JSON patch syntax, and then added to the `appsettings.json` 
file during the CI process using the `x` tool. This ensures that sensitive data like your SMTP password remains secure 
and is not hardcoded into your app's source code.

## Wrapping Up

The ability to use JSON Patch files to manipulate JSON data adds a powerful tool to your .NET developer toolkit. 
Whether you're managing complex configurations or securing sensitive data in CI processes, 
ServiceStack's `x` [dotnet tool](https://docs.servicestack.net/dotnet-tool) and its JSON Patching feature gives you 
an easier way to automate changes to JSON files.


# New Razor SSG generated docs.servicestack.net
Source: https://servicestack.net/posts/new-razor-ssg-docs

Following in the footsteps of [porting servicestack.net](/posts/new_razor_ssg_website) website 
[from Jekyll](https://github.com/mythz/site) to 
[Razor SSG](https://github.com/ServiceStack/servicestack.net), we've decided to 
also take control over our last active VitePress website and port our [docs.servicestack.net](https://docs.servicestack.net) to 
[Razor SSG](https://razor-ssg.web-templates.io/), giving us complete control over its implementation allowing us to 
resolve any issues and add any features ourselves as needed, at the same time freeing us from the complexity and brittleness 
of the npm ecosystem with a more robust C# and Razor Pages SSG based implementation.

## VitePress Issues

Our 500 page [docs.servicestack.net](https://docs.servicestack.net) started experiencing growing pains under [VitePress](https://vitepress.dev)
which started experiencing rendering issues that we believe stems from VitePress's SSR/SPA hydration model that for
maximum performance would convert the initial downloaded SSR content into an SPA to speed up navigation between pages.

However several pages began to randomly show duplicate content and sometimes not display the bottom section of pages at all. 
For a while we worked around these issues by running custom JavaScript to detect and remove duplicate content from the DOM 
after the page loaded as well as moving bottom fragments of pages into separate includes and external Vue components 
for the pages with missing content. 

However as the time to detect and workaround these issues across all our documentation started becoming too time consuming,
it was time to consider a more permanent and effective solution. 

## Porting to Razor SSG

Given we've already spent time & effort porting docs.servicestack.net 
[from Jekyll to VitePress](/posts/jekyll-to-vitepress) less than 2 years ago and after the success 
we had of rapidly [porting servicestack.net](/posts/new_razor_ssg_website) to Razor SSG and rapidly creating 
[Vue Stable Diffusion](https://servicestack.net/posts/vue-stable-diffusion) with Razor SSG in a fraction of the time it 
took to develop the equivalent [Blazor Diffusion](https://docs.servicestack.net/blazor-diffusion), it was clear we
should also do the same for the new documentation website. 

Porting **docs.servicestack.net** ended up being fairly straightforward process that was completed in just a few days, 
with most of the time spent on implementing existing VitePress features we used in C# and Markdig Extensions,
a new Responsive Tailwind Layout and adding support for Dark Mode which was previously never supported.  

Fortunately none of VitePress's SSR/SPA hydration issues manifested in the Razor SSG port which adopted the 
cleaner traditional architecture of generating clean HTML from Markdown and Razor Pages and enhanced on the client-side with Vue.

We're extremely happy with the result, a much lighter and cleaner HTML generated site that now finally supports Dark Mode!

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net?light">
        <img class="p-2" src="/img/posts/razor-ssg/docs.servicestack.net.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net?dark">
        <img class="p-2" src="/img/posts/razor-ssg/docs.servicestack.net-dark.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>

<div class="my-8 flex justify-center">
    <a class="text-3xl text-indigo-600 hover:text-indigo-800" href="https://github.com/ServiceStack/docs.servicestack.net">https://github.com/ServiceStack/docs.servicestack.net</a>
</div>

## Razor Pages Benefits

The new Razor SSG implementation now benefits from Razor Pages flexible layouts and partials where pages can be optionally
implemented in just markdown, Razor or a hybrid mix of both. The [Vue](https://docs.servicestack.net/vue/?light) splash page is an example of this implemented in a custom
[/Vue/Index.cshtml](https://github.com/ServiceStack/docs.servicestack.net/blob/main/MyApp/Pages/Vue/Index.cshtml) Razor Page:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/vue/?light">
        <img class="p-2" src="/img/posts/razor-ssg/razor-pages-vue.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>

Other benefits include a new Documentation Map feature with live scroll updating, displayed on the right side of each documentation page.

## Razor Press Docs Website Template

We've extracted the new Documentation features into a new Razor SSG based project template optimized for creating
documentation and content-centric websites which we'll release within the next few days after we've finished documenting it.

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://razor-press.web-templates.io">
        <img class="p-2" src="https://raw.githubusercontent.com/ServiceStack/Assets/master/csharp-templates/razor-press.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">razor-press.web-templates.io</div>
    </a>
</div>

In a nutshell the new [Razor Press template](https://razor-press.web-templates.io) is a Simple, Powerful, Fast, Flexible & FREE! Use Razor Pages to generate
beautiful Tailwind static websites powered by Markdown & enhanced with Vue Auto deploys to GitHub Pages to Host
for FREE on GitHub CDN

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="uqEa_DfFFDQ" style="background-image: url('https://img.youtube.com/vi/uqEa_DfFFDQ/maxresdefault.jpg')"></lite-youtube>
</div>

Just like [our release of Razor SSG](/posts/razor-ssg) it will be a free project template you can use to create beautiful 
static Tailwind websites with Razor Pages with built-in GitHub Action workflows that deploys to GitHub Pages CDN 
where it can be hosted for FREE under your own custom domain name.

## Example

Currently the largest website generated with Razor Press is the ServiceStack's documentation at [docs.servicestack.net](https://docs.servicestack.net):

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/?light">
        <img class="p-2" src="https://servicestack.net/img/posts/razor-ssg/docs.servicestack.net.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/?dark">
        <img class="p-2" src="https://servicestack.net/img/posts/razor-ssg/docs.servicestack.net-dark.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>

A **500+** pages documentation website ported from VitePress, which prompted the creation of Razor Press after
experiencing issues with VitePress's SSR/SPA model whose workaround became too time consuming to maintain.

The new Razor SSG implementation now benefits from Razor Pages flexible layouts and partials where pages can be optionally
implemented in just markdown, Razor or a hybrid mix of both. The [Vue](https://docs.servicestack.net/vue/) splash page is an example of this implemented in a custom
[/Vue/Index.cshtml](https://github.com/NetCoreTemplates/razor-press/blob/main/MyApp/Pages/Vue/Index.cshtml) Razor Page.

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700" href="https://docs.servicestack.net/vue/">
        <img class="p-2" src="https://docs.servicestack.net/img/pages/ssg/razor-pages-vue.png">
        <div class="bg-gray-50 dark:bg-gray-800 text-gray-600 dark:text-gray-300 font-semibold group-hover:bg-indigo-700 group-hover:text-white text-center py-2">docs.servicestack.net</div>
    </a>
</div>


# Vue Stable Diffusion
Source: https://servicestack.net/posts/vue-stable-diffusion

Just as [blazordiffusion.com](https://blazordiffusion.com) was created to showcase ServiceStack's
Blazor Server and Blazor WASM [project templates and components](/blazor), we've also recreated a new Stable Diffusion UI 
in Vue to showcase the [Razor SSG](https://razor-ssg.web-templates.io) Project Template and Tailwind
[Vue Component Library](https://docs.servicestack.net/vue/) that's now available at:

<h3 class="not-prose text-center pb-8">
    <a class="text-4xl text-blue-600 hover:underline" href="https://diffusion.works">https://diffusion.works</a>
</h3>

## Blazor Diffusion with a Vue UI

Weighing close to [100 APIs](https://api.blazordiffusion.com/metadata), Blazor Diffusion is good representation of a 
medium-sized real-world App that can be used to compare the end user UX of different popular UI technologies used to
develop Web Apps. 

These Diffusion Apps are especially comparable as both Blazor WASM and Vue are both SSG Jamstack Apps deployed to a CDN
which both access the same [https://api.blazordiffusion.com](https://api.blazordiffusion.com/metadata) backend .NET APIs and
both make use of the Tailwind [Blazor Component Library](https://servicestack.net/blazor#blazor-component-gallery) and
[Vue Component Library](https://docs.servicestack.net/vue/) rewritten in Vue, so any differences in UX are predominantly
differences in what the UI technologies can deliver.

We'll look at covering the development workflow, productivity, startup and runtime performance of Blazor Server, 
Blazor WASM and Vue in a future post, for now you can compare their GitHub code-bases and Live Demos of each or download 
and run them locally to evaluate their code-base size, development workflow and performance to evaluate the different
UI technologies:

| Name          | Repo                                                                                  | Live Demo                                                        |
|---------------|---------------------------------------------------------------------------------------|------------------------------------------------------------------|
| Vue           | [NetCoreApps/VueDiffusion](https://github.com/NetCoreApps/VueDiffusion)               | [https://diffusion.works](https://diffusion.works)               |
| Blazor WASM   | [NetCoreApps/BlazorDiffusionWasm](https://github.com/NetCoreApps/BlazorDiffusionWasm) | [blazordiffusion.com](https://blazordiffusion.com)               |
| Blazor Server | [NetCoreApps/BlazorDiffusion](https://github.com/NetCoreApps/BlazorDiffusion)         | [server.blazordiffusion.com](https://server.blazordiffusion.com) |

It's best to evaluate Blazor Server by running it locally as it in particular has poor responsiveness when served over 
high internet latencies, but loads and runs exceptional well in low latency environments like Intranets which is the 
only environment where we'd recommend hosting it.

## Razor SSG

Vue Diffusion is built differently from other Razor SSG Apps as instead of being pre-rendered from static content
like Markdown documents, it's prerendered from https://blazordiffusion.com APIs to render its dynamic 
[Albums](https://diffusion.works/albums/), [Top](https://diffusion.works/top) and [Latest](https://diffusion.works/latest) 
pages at deployment which it does by configuring the App's [Service Gateway](https://docs.servicestack.net/service-gateway) to reference 
[external Blazor Diffusion APIs](https://github.com/NetCoreApps/VueDiffusion/blob/0bbbca3970c07c0cf261ae32c24736ae287981f9/MyApp/Configure.Ssg.cs#L22):

```csharp
services.AddSingleton<IServiceGateway>(implementationFactory: 
    provider => new JsonApiClient(AppConfig.Instance.ApiBaseUrl!));
```

Resulting in all APIs invoked within Razor Pages being delegated to external Blazor Diffusion APIs as the data source
to generate its prerendered Razor Pages.

## Features

For a preview of the development model of Razor SSG enhanced with Vue Components, checkout some of the different pages 
and their implementations:

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Stable Diffusion Search</span>
        <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Index.cshtml">
            Index.cshtml
        </a>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-search.png)](https://diffusion.works)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Generate Images</span>
        <span>
            <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Create.cshtml">
                Create.cshtml
            </a>
            <span class="text-gray-400">|</span> 
            <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/wwwroot/mjs/components/Create.mjs">
                Create.mjs
            </a>
        </span>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-create.png)](https://diffusion.works/create)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Favorites</span>
        <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Favorites.cshtml">
            Favorites.cshtml
        </a>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-favorites.png)](https://diffusion.works/favorites)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Albums</span>
        <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Albums.cshtml">
            Albums.cshtml
        </a>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-albums.png)](https://diffusion.works/albums/)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Selected Image</span>
        <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/wwwroot/mjs/components/Artifacts.mjs#L303">
            Artifacts.mjs
        </a>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-selected.png)](https://diffusion.works/?view=63121)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Top Images</span>
        <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Top.cshtml">
            Top.cshtml
        </a>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-top.png)](https://diffusion.works/top)

<div class="not-prose">
    <h3 class="text-3xl flex justify-between">
        <span>Latest Images</span>
        <span>
            <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Latest.cshtml">
                Latest.cshtml
            </a>
            <span class="text-gray-400">|</span> 
            <a class="text-blue-600 hover:underline" href="https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/Pages/Latest.cshtml.cs">
                Latest.cshtml.cs
            </a>
        </span>
    </h3>
</div>

[![](/img/posts/vue-diffusion/vuediffusion-latest.png)](https://diffusion.works/latest)

Most of these pages also utilize the reusable Vue 3 components defined in: 

- [Artifacts.mjs](https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/wwwroot/mjs/components/Artifacts.mjs)
- [Auth.mjs](https://github.com/NetCoreApps/VueDiffusion/blob/main/MyApp/wwwroot/mjs/components/Auth.mjs)

## Stale-While-Revalidate APIs

We'll have a lot more to write up about our experiences with Vue Diffusion vs Blazor Diffusion in future
[Blog Posts](https://servicestack.net/blog), but we wanted to highlight the performance enhancing technique it uses
to improve perceived performance between pages by utilizing `@servicestack/vue` new State-While-Revalidate (SWR) APIs.

Latency is the biggest performance killer when hosting Web Applications on the Internet, so much so that we'd
historically look to start with a [Single Page App template](https://jamstacks.net) in order to provide the
best UX up until the advent of native ES Modules support in modern browsers meant we could rid ourselves of
SPA complexity and adopt a [Simple, Modern JavaScript](https://jamstacks.net/posts/javascript) Multi Page App (MPA)
approach combined with [htmx's Boost](https://htmx.org/attributes/hx-boost/) feature to improve performance
by avoiding full page reloads.

However we found that to be a fragile approach when navigating back/forward between pages as you'd need to be
mindful of what scripts to place between `<head>` and `<body>` tags and which scripts need to be re-executed
between navigations, reintroducing some of the stateful SPA complexity we want to avoid with a traditional MPA Web App.

We instead discovered we could get just as good UX with stateless full page reloads of pre-rendered HTML pages
if we use SWR to fetch all the API data needed to render the page on first load:

[![](/img/posts/vue-diffusion/diffusion-swr.gif)](https://diffusion.works)

This is easily achieved in reactive Vue.js UIs by invoking API requests with the new `swr()` client API where if
the same API request had been run before it will execute the callback immediately with its "stale" cached results
in `localStorage` first, before executing the callback again after receiving the API response with the latest data:

```ts
import { useClient } from "@servicestack/vue"
const client = useClient()

const results = ref([])
const topAlbums = ref([])
//...

onMounted(async () => {
    await Promise.all([
        client.swr(request.value, api => {
            results.value = api.response?.results || []
            //...
        }),
        client.swr(new AnonData(), async api => {
            topAlbums.value = api.response?.topAlbums || []
            //...
        }),
    ])
})
```

This results in UIs being immediately rendered on load and if the API response has changed, the updated reactive 
collections will re-render the UI with the updated data.


# Build Beautiful Admin UIs, Fast
Source: https://servicestack.net/posts/admin-uis

Built into the latest [blazor-vue](https://blazor-vue.web-templates.io) project template include examples of both Server 
Multi Razor Page and Client rendered Admin UI Pages demonstrating how to use the new `<sidebar-layout>` and 
`<auto-query-grid>` Vue Tailwind components to build beautiful Admin UI Pages in minutes.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="wlRA4_owEsc" style="background-image: url('https://img.youtube.com/vi/wlRA4_owEsc/maxresdefault.jpg')"></lite-youtube>
</div>

As we use [AutoQuery](/autoquery) to develop most new APIs we often lean on Locode's instant Auto Admin UI at the start 
of every project to manage backend RDBMS tables freeing, us to focus most of our efforts on the user-facing
features adding the most value. 

This was also the case for [CreatorKit](/creatorkit/) where its [Locode](https://docs.servicestack.net/locode/) Admin UI:

<div class="not-prose my-8">
   <a href="https://creatorkit.netcore.io/locode/" class="max-w-4xl">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img src="/img/posts/admin-ui/locode-admin-ui.png">
      </div>
   </a>
</div>

Which was sufficient enough to develop CreatorKit's [embeddable Tailwind Components](/creatorkit/components), however
the Admin Portal itself also makes up a significant part of the user-facing value proposition in CreatorKit that we
ultimately decided to develop a Custom UI for its Admin Portal in order to gain complete control of its UX and freedom
to create the optimized UI we want to in future.

Adopting a [Simple, Modern JavaScript](/posts/javascript) approach free of build tools complexity with the 
instant live reload of a simple static [index.html](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/wwwroot/portal/index.html) 
yielded the most productive workflow, combined with all 
[built-in UIs having been rebuilt](https://docs.servicestack.net/releases/v6_07#new-locode-api-explorer-admin-uis-now-in-vue-3)
in reusable Vue 3 components meant we were able to replicate most of Locode's functionality into a customizable Vue App
in no time! 

<div class="not-prose bg-white pb-8">
    <div class="relative overflow-hidden pt-8">
        <div class="mx-auto max-w-7xl px-6">
            <a href="creatorkit/">
            <img src="/img/pages/creatorkit/portal.png" alt="App screenshot" class="mb-[-12%] rounded-xl shadow-2xl ring-1 ring-gray-900/10" width="2432" height="1442"></a>
            <div class="relative" aria-hidden="true">t
                <div class="absolute -inset-x-20 bottom-0 bg-gradient-to-t from-white pt-[7%]"></div>
            </div>
        </div>
    </div>
 </div>

### SidebarLayout Vue Component

So much so that we believe this is a great progressive option for others needing to move beyond Locode or
looking to create their own customizable Admin UI. To minimize the effort we've encapsulated Tailwind's Responsive 
[Sidebar Navigation](https://tailwindui.com/components/application-ui/navigation/sidebar-navigation) into a reusable
`<SidebarLayout />` Vue 3 component and upgraded the [Blazor Vue](https://blazor-vue.web-templates.io) project template
to include both a Razor Pages [server rendered Admin UI](https://blazor-vue.web-templates.io/admin/): 

<div class="not-prose my-8">
   <a href="https://blazor.web-templates.io/admin/" class="max-w-4xl">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img src="/img/posts/admin-ui/server-admin-ui.png">
      </div>
   </a>
</div>

As well as a 
[client rendered Admin UI](https://blazor-vue.web-templates.io/admin/) created from a single static
[index.html](https://github.com/NetCoreTemplates/blazor-vue/blob/main/MyApp/wwwroot/admin/index.html)

<div class="not-prose my-8">
   <a href="https://blazor-vue.web-templates.io/admin/" class="max-w-4xl">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img src="/img/posts/admin-ui/client-admin-ui.png">
      </div>
   </a>
</div>

We'll quickly go through how we can easily extend each layout to with new pages and AutoQueryGrid components to
manage new tables.

## Client Admin UI

The customizable UI is maintained within a single JS Object literal that defines all the sections in the 
Admin UI. Each section defines the Menu Item on the Sidebar, the Title of the Page and the Vue 3 component
body to display when selected:

```js
const sections = {
   Dashboard: {
      icon: `<svg fill="none">...</svg>`,
      component: {
         template:`
            <div>
              <dl class="mt-5 grid grid-cols-1 gap-5 sm:grid-cols-3">
                <div v-for="stat in stats" @click="$emit('nav',stat.label)" class="...">
                  <dt class="...">Total {{humanize(stat.label)}}</dt>
                  <dd class="...">{{formatNumber(stat.total)}}</dd>
                </div>
              </dl>
            </div>`,
         setup() {
            const client = useClient()
            const stats = ref([])
            client.swr(new AdminData(), r => stats.value = r.response?.pageStats || [])
            const formatNumber = value => new Intl.NumberFormat().format(value)
            return { stats, humanize, formatNumber }
         }
      },
   },
   Bookings: {
      type: 'Booking',
      component: {
         template:`<AutoQueryGrid :type="type" 
            selected-columns="id,name,roomType,roomNumber,bookingStartDate,cost,couponId,discount"
            :header-titles="{ roomNumber:'Room No', bookingStartDate:'Start Date', couponId:'Coupon' }"
            :visible-from="{ roomNumber:'lg', cost:'md', couponId:'xl', discount:'never' }" />`,
      },
   },
   Coupons: {
      type: 'Coupon',
      component: {
         template:`<AutoQueryGrid :type="type" />`,
      },
   },
} 
```

Each section can use optional properties to customize their appearance with the properties below: 

 - `icon` - SVG Icon to use in the Sidebar Menu Item
 - `type` - Metadata **Type** to use to populate icon or reference in component 
 - `label` - The label of the menu item on the Sidebar
 - `title` - The Title of the page
 - `group` - The Sidebar group where the menu item should be displayed in
 - `component` - The page's Vue 3 component body

All properties except for `component` are optional, defaulting to the section name and default icon when not provided. 

The above example creates an Admin UI with 3 different types of pages:
 - A custom component to display the Dashboard
 - A customized responsive AutoQueryGrid to manage Bookings
 - A default AutoQueryGrid to manage Coupons

Most of the time you'll just need to use the default AutoQueryGrid to enable a CRUD UI to manage your RDBMS tables.

<div class="not-prose my-8">
   <a href="https://github.com/NetCoreApps/BlazorDiffusionVue/blob/main/BlazorDiffusion/wwwroot/admin/index.html" class="max-w-4xl">
      <div class="block flex justify-center shadow hover:shadow-lg rounded overflow-hidden">
         <img src="/img/posts/admin-ui/vuediffusion-admin-ui.png">
      </div>
   </a>
</div>

Which was all that was needed to render [Blazor Diffusion's](https://blazordiffusion.com) Admin UI:

```ts
const sections = {
     Dashboard: {
         icon: `<svg>...</svg>`,
         component: {
             template:`
             <div>
               <dl class="...">
                 <div v-for="stat in stats" @click="$emit('nav',stat.label)" class="...">
                   <dt class="...">Total {{humanize(stat.label)}}</dt>
                   <dd class="...">{{formatNumber(stat.total)}}</dd>
                 </div>
               </dl>
             </div>`,
             setup() {
                 const client = useClient()
                 const stats = ref([])
                 client.swr(new AdminData(), r => stats.value = r.response?.pageStats || [])
                 const formatNumber = value => new Intl.NumberFormat().format(value)
                 return { stats, humanize, formatNumber }
             }
         },
     },
     Creatives: {
         type: 'Creative',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     Artists: {
         type: 'Artist',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     Modifiers: {
         type: 'Modifier',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     CreativeArtists: {
         type: 'CreativeArtist',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     CreativeModifiers: {
         type: 'CreativeModifier',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     Artifacts: {
         type: 'Artifact',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     ArtifactLikes: {
         type: 'ArtifactLike',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     ArtifactComments: {
         type: 'ArtifactComment',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     ArtifactCommentVotes: {
         type: 'ArtifactCommentVote',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     Albums: {
         group: 'Albums',
         type: 'Album',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     AlbumArtifacts: {
         group: 'Albums',
         type: 'AlbumArtifact',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     AlbumLikes: {
         group: 'Albums',
         type: 'AlbumLike',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     ArtifactStats: {
         group: 'Analytics',
         type: 'ArtifactStat',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     SearchStats: {
         group: 'Analytics',
         type: 'SearchStat',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
     Signups: {
         group: 'Analytics',
         type: 'Signup',
         component: { template:`<AutoQueryGrid :type="type" />` },
     },
 }
```

### Server Admin UI

For developers preferring to develop Server rendered Web Apps the **blazor** project template also includes a 
Multi Page App (MPA) Razor Pages & Vue App which defines the Admin Sections in the custom C# 
[Admin/](https://github.com/NetCoreTemplates/blazor/tree/main/MyApp/Components/Pages/Admin)

```html
<NavList Title="Admin">
    <NavListItem Title="Bookings" href="/admin/bookings" Icon=@typeof(Booking).GetIcon()>
        Create and Manage Bookings
    </NavListItem>
    <NavListItem Title="Coupons" href="/admin/coupons" Icon=@typeof(Coupon).GetIcon()>
        Create and Manage Coupons
    </NavListItem>
</NavList>
```

Where each section links to the different Admin UI Razor Pages: 

#### [Admin/Index.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Index.razor)

Generates the Admin UI Dashboard which retrieves the data for the page from the `AdminData` API invoked
with the internal [Service Gateway](https://docs.servicestack.net/service-gateway):

```csharp
@page
@{
    ViewData["Title"] = "Dashboard";
    var adminData = await Html.Gateway().SendAsync(new AdminData());
}

<div>
    <dl class="mt-5 grid grid-cols-1 gap-5 sm:grid-cols-3">
        @foreach (var item in adminData.PageStats)
        {
            <a href="/admin/@item.Label.ToLower()" class="...">
                <dt class="...">Total @item.Label</dt>
                <dd class="...">@item.Total</dd>
            </a>
        }
    </dl>
</div>
```

#### [Admin/Bookings.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Bookings.razor)

Defines the same responsive AutoQueryGrid to manage the Bookings RDBMS Table: 

```html
<AutoQueryGrid Model="Booking" Apis="Apis.AutoQuery<QueryBookings,CreateBooking,UpdateBooking,DeleteBooking>()" />
```

#### [Admin/Coupons.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Coupons.razor)

Utilizes the default AutoQueryGrid component to manage the Coupons RDBMS Table: 

```html
<AutoQueryGrid Model="Coupon" Apis="Apis.AutoQuery<QueryCoupons>()" />
```

## Creating Custom Admin UIs

Easiest way to start creating Custom Admin UIs is to start with a new **blazor-vue** Razor Pages template containing both
client and server rendered Admin UIs:

<div class="not-prose flex justify-center"><a class="hover:no-underline" href="https://account.servicestack.net/archive/NetCoreTemplates/blazor-vue?Name=MyApp">
    <div class="bg-white dark:bg-gray-800 px-4 py-4 mr-4 mb-4 rounded-lg shadow-lg text-center items-center justify-center hover:shadow-2xl dark:border-2 dark:border-pink-600 dark:hover:border-blue-600 dark:border-2 dark:border-pink-600 dark:hover:border-blue-600"
         style="min-width: 150px;">
        <div class="text-center font-extrabold flex items-center justify-center mb-2">
            <div class="text-4xl text-blue-400 my-3">
                <svg class="w-12 h-12 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 32 32">
                    <path fill="currentColor" d="M23.844 27.692a16.332 16.332 0 0 1-6.645 1.3q-6.364 0-10.013-3.243a11.3 11.3 0 0 1-3.649-8.9a13.716 13.716 0 0 1 3.785-9.898A12.716 12.716 0 0 1 16.9 3.008a11.676 11.676 0 0 1 8.425 3.006a9.994 9.994 0 0 1 3.142 7.533a10.187 10.187 0 0 1-2.318 7.114a7.532 7.532 0 0 1-5.817 2.547a2.613 2.613 0 0 1-1.845-.642a2.323 2.323 0 0 1-.764-1.6a4.9 4.9 0 0 1-4.148 2.243a4.6 4.6 0 0 1-3.507-1.479a5.706 5.706 0 0 1-1.384-4.063a9.913 9.913 0 0 1 2.2-6.357q2.2-2.763 4.8-2.763a5.063 5.063 0 0 1 4.256 1.716l.311-1.338h2.405l-2.081 9.08a10.716 10.716 0 0 0-.352 2.243q0 .972.744.972a4.819 4.819 0 0 0 3.877-2.047a8.93 8.93 0 0 0 1.621-5.681a7.98 7.98 0 0 0-2.675-6.175a9.887 9.887 0 0 0-6.919-2.432a10.6 10.6 0 0 0-8.158 3.467a12.066 12.066 0 0 0-3.2 8.495a9.561 9.561 0 0 0 3.06 7.573q3.06 2.7 8.586 2.7a13.757 13.757 0 0 0 5.675-1.054ZM19.466 12.25a3.977 3.977 0 0 0-3.6-1.716q-1.824 0-3.263 2.23a8.726 8.726 0 0 0-1.439 4.824q0 3.635 2.905 3.635a3.771 3.771 0 0 0 2.651-1.183a6.309 6.309 0 0 0 1.7-3.2Z"></path>
                </svg>
            </div>
        </div>
        <div class="mb-3 text-xl font-medium text-gray-700 dark:text-gray-200">Razor Pages</div>
        <div class="flex justify-center h-8">
            <div class="mr-1"><span class="px-2 h-8 rounded-lg bg-blue-50 dark:bg-blue-900 text-blue-500 dark:text-blue-400 text-sm">autoquery</span></div>
            <div class="mr-1"><span class="px-2 h-8 rounded-lg bg-blue-50 dark:bg-blue-900 text-blue-500 dark:text-blue-400 text-sm">auth</span></div>
        </div>
        <div class="archive-name px-4 pb-2 text-blue-600 dark:text-indigo-400">MyApp.zip</div>
    </div>
</a></div>

Alternatively you can download their pages to incorporate them into your existing Tailwind Projects:

**Client Admin UI**
 - [/admin/index.html](https://github.com/NetCoreTemplates/blazor-vue/blob/main/MyApp/wwwroot/admin/index.html) 

**Server Admin UI**
 - [Index.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Index.razor)
 - [Bookings.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Bookings.razor)
 - [Coupons.razor](https://github.com/NetCoreTemplates/blazor/blob/main/MyApp/Components/Pages/Admin/Coupons.razor)

### Conclusion

In summary, the `<SidebarLayout>` and `<AutoQueryGrid>` 
[Tailwind Vue Components](https://docs.servicestack.net/vue/) are valuable tools for 
software developers seeking to expedite creation of efficient and user-friendly Admin UIs by
providing a solid foundation to effortlessly build beautiful, customizable Admin Pages in no time, freeing up
valuable time & developer resources to focus the majority of efforts into creating the user-facing features that matter most.


# Introducing CreatorKit
Source: https://servicestack.net/posts/creatorkit

[![](/img/pages/creatorkit/creatorkit-brand.svg)](/creatorkit/)

[CreatorKit](/creatorkit/) is a simple, customizable, self-hostable alternative solution to using Mailchimp for managing 
an organization's mailing lists, accepting newsletter subscriptions, defining customizable email layouts and templates 
and sending rich HTML emails to your Customers and subscribers using your preferred SMTP provider.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="_qDVtfcHf14" style="background-image: url('https://img.youtube.com/vi/_qDVtfcHf14/maxresdefault.jpg')"></lite-youtube>
</div>

It also provides a private alternative to using Disqus to enhance websites with a threading and commenting system
on your preferred blog posts and website pages that you want to be able to connect with your community on.

<div class="flex justify-center">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="lgpl-VjxtdU" style="background-image: url('https://img.youtube.com/vi/lgpl-VjxtdU/maxresdefault.jpg')"></lite-youtube>
</div>

<div class="not-prose bg-white py-12">
    <div class="mx-auto max-w-7xl px-6 lg:px-8">
        <div class="mx-auto max-w-2xl sm:text-center">
            <h2 class="text-base font-semibold leading-7 text-indigo-600">Everything you need</h2>
            <p class="mt-2 text-3xl font-bold tracking-tight text-gray-900 sm:text-4xl">
                Grow your on-site community
            </p>
            <p class="mt-6 text-lg leading-8 text-gray-600">
                CreatorKit offers all the tools you need to reach and retain users, from managing subscriber mailing lists
                to moderating a feature-rich comments system
            </p>
        </div>
    </div>
    <div class="relative overflow-hidden pt-8">
        <div class="mx-auto max-w-7xl px-6 lg:px-8">
            <a href="creatorkit/install">
            <img src="/img/pages/creatorkit/portal.png" alt="App screenshot" class="mb-[-12%] rounded-xl shadow-2xl ring-1 ring-gray-900/10" width="2432" height="1442"></a>
            <div class="relative" aria-hidden="true">
                <div class="absolute -inset-x-20 bottom-0 bg-gradient-to-t from-white pt-[7%]"></div>
            </div>
        </div>
    </div>
    <div class="mx-auto mt-8 max-w-7xl px-4">
        <dl class="mx-auto grid max-w-2xl grid-cols-1 gap-x-6 gap-y-4 text-base leading-7 text-gray-600 sm:grid-cols-2 lg:mx-0 lg:max-w-none lg:grid-cols-3">
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 20 20">
                        <path fill="currentColor" d="M9 2a4 4 0 1 0 0 8a4 4 0 0 0 0-8Zm-4.991 9A2.001 2.001 0 0 0 2 13c0 1.691.833 2.966 2.135 3.797C5.417 17.614 7.145 18 9 18v-5a2 2 0 0 1 1.996-2H4.009Zm10.5 4.927l-4.496-2.623A1.5 1.5 0 0 1 11.5 12h6a1.5 1.5 0 0 1 1.5 1.5v.009l-4.49 2.418Zm.228 1.013L19 14.645V17.5a1.5 1.5 0 0 1-1.5 1.5h-6a1.5 1.5 0 0 1-1.5-1.5v-3.046l4.248 2.478a.5.5 0 0 0 .49.008Z"/>
                    </svg>
                    Mailing List Subscriptions
                </dt>
            </div>
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                        <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 5a1 1 0 0 1 1-1h14a1 1 0 0 1 1 1v2a1 1 0 0 1-1 1H5a1 1 0 0 1-1-1zm0 8a1 1 0 0 1 1-1h4a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1H5a1 1 0 0 1-1-1zm10-1h6m-6 4h6m-6 4h6"/>
                    </svg>
                    Email Templates
                </dt>
            </div>
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                        <path fill="currentColor" d="M10 20q-.425 0-.713-.288T9 19v-5q-2.075 0-3.538-1.463T4 9q0-2.075 1.463-3.538T9 4h8q.425 0 .713.288T18 5q0 .425-.288.713T17 6h-1v13q0 .425-.288.713T15 20q-.425 0-.713-.288T14 19V6h-3v13q0 .425-.288.713T10 20Z"></path>
                    </svg>
                    Rich Emails
                </dt>
            </div>
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                        <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m7 12l5 5l-1.5 1.5a3.536 3.536 0 1 1-5-5L7 12zm10 0l-5-5l1.5-1.5a3.536 3.536 0 1 1 5 5L17 12zM3 21l2.5-2.5m13-13L21 3m-11 8l-2 2m5 1l-2 2"/>
                    </svg>
                    Simple Integrations
                </dt>
            </div>
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                        <path fill="currentColor" d="M8.27 3L3 8.27v7.46L8.27 21h7.46C17.5 19.24 21 15.73 21 15.73V8.27L15.73 3M9.1 5h5.8L19 9.1v5.8L14.9 19H9.1L5 14.9V9.1m6 5.9h2v2h-2v-2m0-8h2v6h-2V7"/>
                    </svg>
                    Content Moderation
                </dt>
            </div>
            <div class="relative pl-9">
                <dt class="inline font-semibold text-gray-900">
                    <svg class="absolute left-1 top-1 h-5 w-5 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 20 20">
                        <path fill="currentColor" d="M16 2h4v15a3 3 0 0 1-3 3H3a3 3 0 0 1-3-3V0h16v2zm0 2v13a1 1 0 0 0 1 1a1 1 0 0 0 1-1V4h-2zM2 2v15a1 1 0 0 0 1 1h11.17a2.98 2.98 0 0 1-.17-1V2H2zm2 8h8v2H4v-2zm0 4h8v2H4v-2zM4 4h8v4H4V4z"></path>
                    </svg>
                    Newsletter generation
                </dt>
            </div>
        </dl>
    </div>
</div>

### Enhance static websites

We're developing CreatorKit as an ideal companion for JAMStack or statically generated branded websites like
[Razor SSG](https://razor-ssg.web-templates.io/posts/razor-ssg)
enabling you to seamlessly integrate features such as newsletter subscriptions, email management, comments, voting, 
and moderation into your existing websites without the complexity of a custom solution. 

It's ideally suited for Websites who want to keep all Mailing Lists Contacts and Authenticated User Comments in a 
different, private self-hosted site, isolated from your existing Customer Accounts and internal Systems.

With CreatorKit, you can enjoy the convenience of managing your blog's comments, votes, and subscriptions directly
from your own hosted [CreatorKit Portal](/portal/) without needing to rely on complex content management systems to 
manage your blog's interactions with your readers.

Additionally, CreatorKit makes it easy to send emails and templates to different mailing lists, making it the perfect
tool for managing your email campaigns. Whether you're a blogger, marketer, or entrepreneur, CreatorKit is a great
solution for maximizing your blog's functionality and engagement.

## Features

The CreatorKit Portal offers a complete management UI to manage mailing lists, email newsletter and marketing campaigns,
thread management and moderation workflow.

### Email Management

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-messages">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-messages.png" alt=""></a>
</figure>

### Optimized Email UI's with Live Previews

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-messages#email-ui">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-messages-simple.png" alt=""></a>
</figure>

### Custom HTML Templates

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-messages#sending-custom-html-emails">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-messages-custom.png" alt=""></a>
</figure>

### HTML Email Templates

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-messages#sending-html-markdown-emails">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-messages-markdown.png" alt=""></a>
</figure>

### Mailing List Email Runs

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-mailruns">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-mailrun-custom.png" alt=""></a>
</figure>

### Newsletter Generation

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-mailruns#generating-newsletters">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-mailrun-newsletter.png" alt=""></a>
</figure>

### Comment Moderation

<figure class="mt-4">
    <a class="my-8 max-w-4xl mx-auto block" href="/creatorkit/portal-posts">
        <img class="rounded shadow hover:shadow-lg" src="/img/pages/creatorkit/portal-report.png" alt=""></a>
</figure>

### Use for FREE

CreatorKit is a FREE customizable .NET App included with [ServiceStack](https://servicestack.net) which is
[Free for Individuals and Open Source projects](https://servicestack.net/free) or for organizations that continue to
host their forked CreatorKit projects on GitHub or GitLab. As a stand-alone hosted product there should be 
minimal need for any customizations with initial [Mailining Lists, Subscribers](/creatorkit/install#before-you-run), 
[App Settings](/creatorkit/install#whats-included) and branding information maintained in
customizable [CSV](/creatorkit/install#before-you-run) and [text files](/creatorkit/customize).

To get started follow the [installation instructions](/creatorkit/install) to download and configure it with your 
organization's website settings.

## Future

As we're using CreatorKit ourselves to power all dynamic Mailing List and Comment System features on 
[https://servicestack.net](servicestack.net) (inc. this page!), we'll be continuing to develop it with useful features to
empower static websites with more generic email templates and potential to expand it with commerce features, inc.
Stripe integration, products & subscriptions, ordering system, invoicing, quotes, PDF generation, etc.

Follow [@ServiceStack](https://twitter.com/ServiceStack), Watch or Star [NetCoreApps/CreatorKit](https://github.com/NetCoreApps/CreatorKit)
or Join our CreatorKit-powered Monthly Newsletter to follow and keep up to date with new features:

<div class="not-prose">
    <div class="mt-8 mx-auto max-w-md" data-mail="JoinMailingList" data-props="{ submitLabel:'Join our newsletter' }"></div>
</div>

As a design goal [CreatorKit's components](/creatorkit/components) will be easily embeddable into any external website,
where it will be integrated into the [Razor SSG](/posts/razor-ssg) project template to serve as a working demonstration 
and reference implementation. As such it's a great option if you're looking to create a Fast, FREE, CDN hostable,
[simple, modern](/posts/javascript) statically generated website created with Razor & Markdown
like [ServiceStack/servicestack.net](https://github.com/ServiceStack/servicestack.net). 

### Feedback welcome

If you'd like to prioritize features you'd like to see first or propose new, generically useful features for
static websites, please let us know in [servicestack.net/ideas](https://servicestack.net/ideas).


# Creating ChatGPT Agents to call System APIs
Source: https://servicestack.net/posts/chat-gpt-agents

<div class="py-8 max-w-7xl mx-auto px-4 sm:px-6">
    <lite-youtube class="w-full mx-4 my-4" width="560" height="315" videoid="7vChIGHWPuI" style="background-image: url('https://img.youtube.com/vi/7vChIGHWPuI/maxresdefault.jpg')"></lite-youtube>
</div>


## Introduction

We've been working on different patterns and proof of concepts to enable [ChatGPT](https://openai.com) to leverage an App's ServiceStack APIs to solve given tasks.
This is done through various prompting techniques which allows the Agent to reason about the context of the conversation and make decisions to reach for different 'tools' (your APIs) when trying to achieve a specific goal.

Those tools are your own ServiceStack APIs which can be configured through a plugin in the GptMeetingAgent project.

```csharp
var gptAgentFeature = new GptAgentFeature();

gptAgentFeature.RegisterAgent(new GptAgentData
    {
        Name = "BookingAgent",
        PromptBase = File.ReadAllText(
            $"{Path.Combine("Prompts", "BasePromptExample.txt")}"
        ),
        Role = "An AI that makes meeting bookings between staff."
    },
    agentFactory: data => new OpenAiChatGptAgent(chatGptApiKey,data),
    includeApis: new List<string>
    {
        Tags.Teams,
        Tags.Calendar,
    });
```

The above configuration registers a new Agent with the name `BookingAgent` and a description of what it does as the `Role`.

It also is configured to include the APIs with the `Tags.Teams` and `Tags.Calendar` tags, which are the APIs that the Agent will use to achieve its goals.
Only these APIs will be exposed to the Agent, and the Agent will only be able to use them in the context of the prompt that is provided to it.

The Agent will also use the `Description` of the API to assist the Agent in understanding how and when to use the API.

```csharp
[Tag("Teams"), Description("Search for users by name")]
public class SearchUsers : IReturn<SearchUsersResponse>
{
    public string Name { get; set; }
}
```

## So How Does This Work?

This proof of concept uses several different technologies and techniques to achieve this functionality. A lot of which are quite new, and not nearly as well known as common web development techniques.
As such, we will go through each of the technologies and techniques used to achieve this functionality as an introduction to using AI Agents in your own projects.

### OpenAI ChatGPT and Large Language Models

ChatGPT by OpenAI is a chat product that uses large language models that are trained on a large corpus of text from the internet.
The `GPT` in ChatGPT stands for `Generative Pre-trained Transformer` which is a type of neural network architecture that is used to train the model.

[![](/img/posts/chat-gpt-agents/Slide10_1080p.png)](https://arxiv.org/abs/1706.03762)

These Large Language Models are designed to predict the next 'token' or part of a word in a sentence, given the previous tokens.

[![](/img/posts/chat-gpt-agents/Slide11_1080p.png)](https://learn.microsoft.com/en-us/semantic-kernel/concepts-ai/tokens)

This text is called the `prompt` and is used to 'prime' the model with some context to generate the next token.

The use of the Transformer architecture has shown to be very effective at learning the structure of language as well as the semantic meaning of words and sentences.

As these models are scaled up, they are able to learn more and more complex patterns in language, and are able to generate more and more complex sentences.
Some state of the art models have been able to show emergent capabilities of reasoning and problem solving, which is what we are leveraging in this proof of concept.

### Chain-of-Thought

One technique that has been discovered when using these Large Language Models is the ability to prompt in a way that improves the ability of the model to perform complex reasoning.
This is called [Chain-of-Thought](https://arxiv.org/abs/2201.11903) and is a technique asks the model to output its 'thoughts' about a task that it is trying to achieve, and then feeding those thoughts back into the model in the next prompt.

![](/img/posts/chat-gpt-agents/Slide13_1080p.png)

We can get the model to do this with a surprisingly simple prompt.

```text
Ensure to reason about the tasks in a step by step manner, 
and provide details about your plan, reasoning, criticism, 
and reflection about your steps in the thoughts output.
```

We specifically ask the model to output details about its `plan`, `reasoning`, and `criticism` as they are a part of the final output that we want to generate.

![](/img/posts/chat-gpt-agents/Slide12_1080p.png)

This enables the model to break down tasks into smaller steps, and then reason about how to achieve those steps by giving it context of the previous thoughts that it has generated.
This reinforces why previous decisions were made, which in turn, influences future decisions.

### Agents

We can then combine the reasoning capabilities of the model with the ability to take action using 'tools' to achieve a goal.

Giving the model agency to use external tools and APIs creates the concept of an `Agent` that can be used to solve a variety of tasks.

We give the Agent access to these tools by describing them in the prompt. Using the type information about your Request DTOs, and the `[Description]` attribute, we can describe the tools that the Agent has access to.
In the prompt, we call these `Service Commands`.

```csharp
[Tag(Tags.Teams)]
[Description("Search for users by first or last name and get back their IDs and emails")]
public class SearchUsers : IReturn<SearchUsersResponse>
{
    public string Name { get; set; }
}
```

This turns into a Service Command with the following text in the prompt.

```text
1. Search for users by first or last name and get 
back their IDs and emails: "SearchUsers", definition: 
type SearchUsers = { name: string }
```

The Service Commands each have the Request DTO name as well as the description to help the Agent understand what the tool does and when to use it.
It also includes a TypeScript definition of the Request DTO so that the Agent can generate the correct request structure when it needs to use the tool, and populate with data from its own context.

### In Context Learning

So now our Agents have access to tools, can reason about problems, and provide feedback about its reasoning, but we still need it to use our own data.

The prompt technique of `In Context Learning` allows us to provide the Agent with data returned from the tools that it uses, and then use that data in the next prompt.

For example, if we want to book a meeting with a colleague, the model can use the `SearchUsers` tool to find the email address of the person we want to book a meeting with.

```json
{
  "commandName": "SearchUsers",
  "commandResponse": [{
      "displayName": "Lynne Schoen",
      "email": "Lynne_Schoen9@gmail.com",
      "id": 6
    }
  ]
}
```

We can then feed back the command and the response into the next prompt, which will allow the model to use the email address of the person we want to book a meeting with.

Another API that is available to the Agent is the `GetUserSchedule` API to check the availability of the person we want to book a meeting with.
This API requires a `userId`, which the Agent now has from the `SearchUsers` API.

```
2. Get a user's schedule: "GetUserSchedule", 
definition: type GetUserSchedule = { userId: number, schedules: string[], 
startTime: string, endTime: string, availabilityViewInterval: number }
```

### Output Format

Each time we prompt the Agent, we get back a consistent JSON structure with a `command` and `thoughts` object.

```json
{
    "command": {
        "name": "GetUserSchedule",
        "body": {
            "userId": 6,
            "schedules": ["primary"],
            "startTime": "2023-05-09T00:00",
            "endTime": "2023-05-16T00:00",
            "availabilityViewInterval": 60
        }
    },
    "thoughts": {
        "text": "Checking Lynne's schedule for availability on Tuesday ...",
        "reasoning": "Before creating a meeting, it is important to check ...",
        "plan": "- Use the GetUserSchedule command to retrieve Lynne's calendar ...",
        "criticism": "I may need to account for time zone differences ...",
        "speak": "I am now checking Lynne's schedule for availability..."
    }
}
```

Near the end of each interaction with the model, we specify the response format using the following prompt.

![](/img/posts/chat-gpt-agents/Slide17.PNG)

The `command` object contains the name of the command that was used, as well as the body of the command that was generated from the Agent's context.
The `thoughts` object contains the text that was generated by the model, as well as the `reasoning`, `plan`, and `criticism` that was generated by the model.

The `body` of the command is a TypeScript definition of the Request DTO that was used, and can be used to generate the correct structure of the command when it is used again.

### Putting it all together

We can now combine all of these concepts to create an Agent that can book a meeting with a colleague.

![](/img/posts/chat-gpt-agents/AgentFlow.drawio.svg)

In the GptMeetingAgent proof of concept, the browser client is the one taking action on behalf of our agent, making requests to your APIs and returning the response to the Agent.

We can use the `[ConfirmationRequired]` attribute to put in additional guard rails to prevent the Agent from taking action on sensitive APIs.

In the GptMeetingAgent project, this is used to prevent the Agent from booking a meeting without confirmation from the user.
Instead, we use the `Submit` of an `AutoForm` to confirm the booking.


## The GptMeetingAgent Project

The example we have built is a simple API that can search for users, list their schedules, and book meetings between users.

A lot of this example is actually generic and being driven by the Agent and the specific APIs in the example.

This can be demonstrated by introducing a new `ListMeetingRooms` API that we add to the APIs the Agent can use.

```csharp
[Tag(Tags.Teams), Description("Get a list of meeting rooms and their resources.")]
public class ListMeetingRooms : QueryDb<MeetingRoom> { }
```

This AutoQuery service lists meeting rooms along with their related resources like `Projector`,`SmartBoard`, and `Whiteboard`.

```csharp
public enum MeetingRoomResource
{
    SmartBoard,
    Projector,
    Whiteboard,
    VideoConference,
    SpeakerPhone
}
```

We can then extend the `CreateCalendarEvent` API that books the meeting to reference a meeting room by ID.

```csharp
[Tag(Tags.Calendar), Description("Create a meeting")]
[ConfirmationRequired("Are you sure you want to create a meeting?")]
public class CreateCalendarEvent : IReturn<CreateCalendarEventResponse>
{
    public string Subject { get; set; }
    public string Body { get; set; }
    [Input(Type = "datetime-local")]
    public DateTime Start { get; set; }
    [Input(Type = "datetime-local")]
    public DateTime End { get; set; }
    public string AttendeeEmail { get; set; }
    // Newly Added
    public int? MeetingRoomId { get; set; }
}
```

Restarting the application, our injected Service Commands now reflect both the new `ListMeetingRooms` API and the updated `CreateCalendarEvent` API in the Agent prompt.

Asking the Agent to "Ensure the meeting room has a projector" will now result in the behavior of our Agent using our new API, finding the request resource, and passing the correct `MeetingRoomId` when booking a meeting.

Both the Agent and the UI reflect this change in behavior. Our `AutoForms` for both APIs are displayed, and we didn't need to make any other changes other than modifying our APIs to support the new behavior.

You can checkout our [live demo of this example here](https://gptmeetings.netcore.io/).

![](/img/posts/chat-gpt-agents/Slide16.PNG)

## Request for Feedback

We are [releasing this project](https://github.com/NetCoreApps/GPTMeetingAgent) as a proof of concept to get feedback from the community on how they would like to use this technology in their own projects.
We plan on continuing to develop examples and documentation for a generic pattern to make this type of development easier to integrate by building a plugin similar to that in the GptMeetingAgent project.

We would love to hear your feedback on this project, and how you would like to use it in your own projects.
This will help us to prioritize and focus on the important features of the plugin and the documentation.

<style>
.video-hd { width: 640px; height: 360px }
</style>


# Bringing xkcd static dataset to life with AutoQuery
Source: https://servicestack.net/posts/autoquery-xkcd

One of the big advantages of using AutoQuery is the ability to turn data into an API with very little effort.

Once your data is exposed as an API, you can use it in any way you want, whether that's a web app, a mobile app, a desktop app, or even a CLI.

# Unsiloing Data with AutoQuery

A problem I encountered when working for a weather forecasting company is that modelers and data scientists would often create datasets that were only accessible via Jupyter notebook or a Python script which is hard to reuse or share.
This meant that the data was only accessible to a small number of people, and it was difficult to share the data with other teams or to use the data in other applications.
Some of these notebooks and scripts would need to use massive datasets for a very small amount of data. The company specialized in weather data and experiments could pull down 100s of GBs of data, but only use a few MBs of it.

Poor data accessibility is still something that slows down teams. While it is 'easier' sometimes for a single developer/modeler/data scientist to just pull data from static files, not having shared access to the data can make it difficult to reproduce or reuse.
Web APIs help un-silo the data, create more efficient access points and reduce the time it takes to get data from an experiment into a production application.

ServiceStack and AutoQuery can help solve this problem by making it easy to expose data as an API, and by simplifying the use of that data in any application.

:::youtube CrKtXVrPj8Q
Use AutoQuery to search Xkcd comics
:::

In this post, we'll look at how to use AutoQuery to create a web app from a dataset, and to make it a bit of fun, we are going to use a dataset of XKCD comics.
This dataset is from HuggingFace's [datasets](https://huggingface.co/datasets/olivierdehaene/xkcd) repository if you want to repeat
the process yourself, but any dataset in formats like CSV, JSON, etc or in an existing SQL database like SQLite will work with the same approach.
## What is the dataset?

The dataset is licensed under the Creative Commons Attribution-ShareAlike 3.0 license, and the code for this example is available on GitHub.

The dataset contains metadata of 2630 comics from the XKCD website, with the following fields:

- id
- title
- url: xkcd.com URL
- image_url
- explained_url: explainxkcd.com URL
- transcript: english text transcript of the comic
- explanation: english explanation of the comic

## AutoQuery Datasources

AutoQuery support different ways of accessing data to make it flexible for different use cases.
The most common way to access data is via a SQL database using `AutoQuery RDBMS`, but also supports using `AutoQuery Data` which can with in-memory data, another service or even AWS DynamoDb.

This this example, our data is in a JSONL file which separates row entries by a new line, so we could use AutoQuery Data to load the data into memory and expose it as an API.

First we will [declare a class that represents a row of data from our file](https://github.com/NetCoreApps/ssg-examples/blob/master/ExampleDataApis.ServiceModel/Xkcd.cs).

```csharp
public class XkcdComic
{
    public int Id { get; set; }
    public string Title { get; set; }
    public string ImageTitle { get; set; }
    public string Url { get; set; }
    public string ImageUrl { get; set; }
    public string ExplainedUrl { get; set; }
    public string Transcript { get; set; }
    public string Explanation { get; set; }
}
```

And then [load our data into memory from the JSONL file](https://github.com/NetCoreApps/ssg-examples/blob/master/ExampleDataApis/Configure.Db.Xkcd.cs#L13).

```csharp
var allLines = "path/to/your/dataset.jsonl".ReadAllText().Split("\n");
using var jsonConfig = JsConfig.With(new Config {
    TextCase = TextCase.SnakeCase
});
var comics = allLines
    .Where(x => !x.IsNullOrEmpty())
    .Select(JsonSerializer.DeserializeFromString<XkcdComic>)
    .ToList();
```

Now we can add our data as a AutoQueryData source when configuring our AppHost and the `AutoQueryDataFeature` Plugin.

```csharp
//AutoQuery Data Plugin
Plugins.Add(new AutoQueryDataFeature { MaxLimit = 100 }
    .AddDataSource(ctx => ctx.MemorySource(comics))
);
```

And lastly, to expose the data over an API, we declare a Request DTO that inherits from `QueryData<XKCDComic>`.

```csharp
[Route("/xkcd")]
public class QueryXkcdComicData : QueryData<XkcdComic> {}
```

We now have a web API that exposes the our static dataset as an API. Any this already support a lot of different ways you can sort and filter data while also supporting different formats like JSON, CSV, XML and others.

## Using QueryDb

Alternatively to using AutoQuery Data, we can use AutoQuery RDBMS to access our data from a SQLite database.
This is useful if you have a large dataset that you want to query, but don't want to load all the data into memory.

First we will load the data into the SQLite database.

```csharp
var dbFactory = new OrmLiteConnectionFactory("App_Data/db.sqlite", SqliteDialect.Provider);
container.Register<IDbConnectionFactory>(dbFactory);

using var db = dbFactory.Open();
if (db.CreateTableIfNotExists<XkcdComic>())
{
    db.InsertAll(comics);
}
```

Now we can use `QueryDb<T>` instead of `QueryData<T>` to access the data from the database, and [use the AutoQueryFeature Plugin](https://github.com/NetCoreApps/ssg-examples/blob/master/ExampleDataApis/Configure.AutoQuery.cs) instead of `AutoQueryDataFeature`.

```csharp
//AutoQuery Plugin for RDBMS
Plugins.Add(new AutoQueryFeature { MaxLimit = 100 });
// Use QueryDb<T> instead of QueryData<T>.
[Route("/xkcd")]
public class QueryXkcdComics : QueryDb<XkcdComic> {}
```

And that's it, we now have an API that we can use to query our XKCD comics data.

## Locode

One of the advantages of loading our data into SQLite is taking advantage of the built in Locode App that comes with the `ServiceStack.Server` library.
Restarting the application, we can now navigate to the Locode App at `http://localhost:5000/locode` and see our XKCD comics data.

[![Locode App](/img/posts/autoquery-xkcd/locode-app.png)](https://ssg-examples.netcore.io/locode/QueryXkcdComics)

Looking at the UI, we can see we get a table of data with filtering capabilities in a user friendly way.
Locode is driven off the web services that are exposed by the AutoQuery API, so we can also use the same data to create a web app.

We can look at the endpoint itself and the default [Auto HTML API page](https://docs.servicestack.net/auto-html-api) to see how to interact with the API in 10 different languages.

[![AutoHtml page](/img/posts/autoquery-xkcd/autohtml-page.png)](https://ssg-examples.netcore.io/api/QueryXkcdComics?take=25&format=html)

## Creating a web app with Razor and Vue.js

Now our dataset is available from an API, it is far more accessible to using in a variety of different ways.
In this example, we've hosted our dataset as a web API, and there is a [live demo of the API](https://ssg-examples.netcore.io/ui/QueryXkcdComics) we can use to create a web app.

We can do this from whatever language you prefer, but for this example, we'll use Razor and Vue.js via the [ServiceStack razor-ssg template](https://github.com/NetCoreTemplates/razor-ssg), and the [ServiceStack Vue library](https://docs.servicestack.net/vue/) to create our web app.
One of the reasons we chose to use this template is because we can prerender the whole application as a static site and deploy it to any CDN including GitHub Pages.

You will need the ServiceStack dotnet `x` tool installed to create a new project from the template.

:::sh
dotnet tool install -g x
:::

First we'll create a new project from the template.

:::sh
x new razor-ssg Xkcd
:::

This template comes with **Vue 3** and **TailwindCSS** already configured, so we can get started right away.
It also utilizes [JavaScript modules](./posts/javascript), so we can use the `import` syntax to import the ServiceStack Vue library without having to use a bundler like Webpack.
We can also create these Vue components inline on our razor pages, which will then be served as static files.This gives us instant feedback when we make changes to our code, instead of having to wait for a build step to complete.

Since our dataset is available directly from our API, this application doesn't need a dataset or other storage and adds another way we can interact with our dataset.

## Creating a page with Razor and Vue.js

Now we can create a page to display our XKCD comics data.
We can do this by creating a new Razor page in the `Pages` folder, and then add a `@page` directive to the top of the file to declare the route.
We are also going to add the `[RenderStatic]` attribute to the page to tell the ServiceStack razor-ssg template to prerender the page as a static site.

This server render the page, and then fetch data with Vue.js, which works the same when the page is rendered as static.

```html
@page "/comics-datagrid"
@attribute [RenderStatic]

@{
    ViewData["Title"] = "Xkcd Comics";
}

<div class="py-8">
    <div class="mx-auto max-w-7xl px-4 sm:px-6 lg:px-8">
        <div id="app">
            <data-grid :items="comics"></data-grid>
        </div>
    </div>
</div>

<script type="module">
    import { ref, onMounted } from "vue"
    import { QueryXkcdComics } from "dtos.mjs"
    import { useClient, useUtils } from "@@servicestack/vue"
    import { mount } from "/mjs/app.mjs"

    const App = {
        props: {comics: Array},
        setup(props) {
            const client = useClient()
            const {pushState} = useUtils()
            const comics = ref(props.comics || [])
            const request = ref(new QueryXkcdComics({ take:25 }))

            async function submit() {
                let results = await client.api(request.value)
                comics.value = results.response.results
            }
            onMounted(submit)

            return { comics, request, submit, client }
        }
    };

    mount('#app', App)
</script>
```

Since we don't yet have the data, we also need to use the ServiceStack client to fetch the data from the API.
We can do this with the `useClient` hook, which will return the `JsonServiceClient` instance.

```javascript
import { useClient, useUtils } from "@@servicestack/vue"
//...
const client = useClient()
```


## Creating a Vue component

Since we don't yet have the data, we also need to use the ServiceStack client to fetch the data from the API.
We can do this with the `useClient` hook, which will return the `JsonServiceClient` instance.

```javascript
const request = ref(new QueryXkcdComics({ take:25 }))

async function submit() {
    let results = await client.api(request.value)
    comics.value = results.response.results
}
onMounted(submit)
```

## Generating the DTOs

However, we don't yet have the `QueryXkcdComics` Request DTO available in our application since the template isn't aware of the use of the external API.
By default the `mjs/dtos.mjs` file uses the `BaseUrl` of `https://localhost:5001` to generate the DTOs, but our data is hosted at `https://ssg-examples.netcore.io`.
We can either change this `BaseUrl` in the `dtos.mjs` file or just delete it and regenerate it using: 

:::sh
x mjs https://ssg-examples.netcore.io
:::

Any additional updates will read from this `BaseUrl` and update the `dtos.mjs` file with the latest DTOs using the command:

:::sh
x mjs
:::

This command pulls the generated DTOs from the ServiceStack server, and updates the `mjs/dtos.mjs` file with the latest DTOs.
And this workflow works for any of the ServiceStack client libraries and supported languages.

![data-grid component](/img/posts/autoquery-xkcd/vue-datagrid.png)

Here we can see the use of the ServiceStack Vue components library and the `data-grid` component.
The `data-grid` component is a built in component that will render the data in a table within minimal markup.
This can be a great way to get the instant usability of a table without having to write a lot of code, and it can be used anywhere in your application.

The `data-grid` itself simply takes the `comics` array assigned to the `items` prop, and renders the data in a table.

```html
<div id="app">
    <data-grid :items="comics"></data-grid>
</div>
```

The `data-grid` component also has a number of other props that can be used to customize the table, such as `selected-columns` and `header-titles`.
Let's now customize the `data-grid` so we can have a view of the comics.

```html
<data-grid :items="comics"
           selected-columns="imageUrl,transcript"
           v-on:row-selected="rowSelected" :is-row-selected="row => row == selected"
           :header-titles="{ imageUrl:'Comic',transcript: 'Description' }"
           class="max-w-screen-lg mx-auto">
    <template #imageUrl="{ imageUrl }">
        <img :src="imageUrl" class="h-48 object-cover" loading="lazy">
    </template>
    <template #transcript="{ title, transcript, width, height }">
        <div class="flex flex-col max-w-3xl">
            <div class="flex justify-between">
                <h2 class="text-lg font-semibold text-gray-900">{{ title }}</h2>
                <div class="text-right text-sm font-semibold block">{{width}} x {{height}}</div>
            </div>
            <p class="whitespace-normal break-words overflow-hidden max-h-40">{{ transcript }}</p>
        </div>
    </template>
</data-grid>
<modal-comic v-if="selected" :comic="selected" v-on:done="selected=null"></modal-comic>
```

Above we've added a few more props to the `data-grid` component to customize the columns we want to display using `selected-columns`, and the titles of the columns by setting `header-titles` to a map of the column names to the titles we want to display.
We are also using `templates` to customize the rendering of the `imageUrl` and `transcript` columns to have more control over the layout.

[![Customized data-grid component](/img/posts/autoquery-xkcd/vue-datagrid-custom.png)](https://xkcd.netcore.io/comics-datagrid)

## AutoQueryGrid vs DataGrid

While the `data-grid` component is a great way to get a table up and running quickly, we can get even more functionality with the `auto-query-grid` component.
The `auto-query-grid` component is a wrapper around the `data-grid` component that adds a number of features to make it easier to use.

The `auto-query-grid` component will automatically fetch the data for you, and we can customize our columns using the same `template` pattern as the `data-grid`.

```html
<auto-query-grid type="XkcdComic"
     selected-columns="imageUrl,width,height,id,title,transcript,explanation,url"
     :visible-from="{ title:'sm', transcript:'xl', explanation:'2xl', url:'never' }"
     :header-titles="{ imageUrl:'Comic', transcript:'Description' }"
     v-on:row-selected="rowSelected"
     class="mx-auto">
    <template #imageUrl="{ id, imageUrl }">
        <img :src="imageUrl" class="h-12 object-cover" loading="lazy">
    </template>
    <template #transcript="{ transcript }">
        <p class="block max-w-sm text-ellipsis overflow-hidden" 
           :title="transcript">{{ transcript }}
        </p>
    </template>
    <template #explanation="{ explanation }">
        <p class="block max-w-sm text-ellipsis overflow-hidden">{{ explanation }}</p>
    </template>
</auto-query-grid>
<modal-comic v-if="selected" :comic="selected" v-on:done="selected=null"></modal-comic>
```

So the only custom behavior code you need is to wire up the `modal-comic` component to the `auto-query-grid` component, is the following:

```js
setup(props) {
    const selected = ref()
    function rowSelected(comic,e) {
        selected.value = comic
    }
    return { selected, rowSelected }
}
```


## Custom Grid with a Search Box

We can also fetch the data from the API dynamically using the `JsonServiceClient` from the ServiceStack Vue library, and use AutoQuery to easily filter for the data we want.

Let's create a separate page that will allow us to search comics by title, this time on our Index page.
We'll update our `Index.cshtml` file to the following markup.

First we'll add a search box to the top of the page to allow us to search for comics by title then display xkcd comics in a custom flexbox:

```html
<div id="app">
    <div class="flex flex-1 flex-col overflow-hidden">
        <div class="mb-8 mx-auto w-96">
            <h2 class="text-center mb-4 max-w-4xl font-display 
                       text-5xl font-bold tracking-tight text-slate-800 
                       dark:text-slate-200">
                search xkcd
            </h2>
            <text-input v-cloak 
                        class="w-full w-prose w-100" 
                        type="search" 
                        v-model="searchTerm" 
                        placeholder="search xkcd comic titles">
                
            </text-input>
        </div>
        <div v-cloak>
            <div v-if="!loading && hasInit" 
                 class="w-full pb-4 bg-white dark:bg-black border 
                        border-black flex flex-wrap">
                <div v-if="comics.length" 
                     class="border-2 border-slate-700 ml-4 mt-4 p-4 
                            flex justify-center items-center 
                            hover:shadow-lg hover:bg-slate-50 
                            dark:hover:bg-slate-900 max-w-[48%]" 
                     v-for="comic in comics">
                    <div v-on:click="showModal(comic)" 
                         class="cursor-pointer">
                        <h2 class="mb-4 text-lg font-semibold text-gray-900 
                                   dark:text-gray-100 text-center">
                            {{ comic.title }}
                        </h2>
                        <img :src="comic.imageUrl" :width="comic.width" 
                             :height="comic.height" class="h-48 object-cover" 
                             :aria-description="comic.explanation" 
                             :alt="comic.transcript" loading="lazy">
                    </div>
                </div>
                <div v-else class="w-full">
                    <h4 class="text-center text-lg pt-8 pb-4">
                        query returned no results
                    </h4>
                </div>
            </div>
            <div v-else class="flex justify-center items-center pt-8">
                <loading>searching xkcd...</loading>
            </div>
        </div>
        <modal-comic v-if="selected" :comic="selected" 
                     v-on:done="done">
        </modal-comic>
    </div>
</div>
```

Let's create our new component to display our initial data in a grid layout, and also have a search box at the top that we can use to make additional API calls to filter the data from the AutoQuery API.
We can do this again by using the `JsonServiceClient` with the Request DTO related to the API we want to call, but this time we will also pass some additional parameters to the `QueryXkcdComics` DTO to filter the data.

```js
import { ref, watch, onMounted } from "vue"
import { mount } from "app.mjs"
import { QueryXkcdComics } from "dtos.mjs"
import { useClient, useUtils } from "@@servicestack/vue"
import { queryString } from "@@servicestack/client"
const App = {
    setup(props) {
        const comics = ref(props.comics || [])
        const qs = queryString(location.search)
        const searchTerm = ref(qs.q || '')
        const client = useClient()
        const { pushState } = useUtils()
        const selected = ref()

        const loading = ref(false)
        const hasInit = ref(false)
        onMounted(async () => {
            if (qs.q) {
                let api = await client.api(
                    new QueryXkcdComics({ titleContains:searchTerm.value, orderByDesc:'id' }))
                comics.value = api.response.results
            } else {
                await init()
            }
            if (qs.id) {
                showModal(comics.value.find(x => x.id == qs.id))
            }
            hasInit.value = true
        })

        async function init() {
            loading.value = true;
            let randomIds = generateRandomNumbers(1,2630,12)
            if (qs.id) randomIds.unshift(parseInt(qs.id))
            let results = await client.api(new QueryXkcdComics({ ids:randomIds }))
            comics.value = results.response.results
            loading.value = false
        }
        const searchApi = createDebounce(async (query) => {
            if (!query || query.length === 0) {
                pushState({ q: undefined })
                let randomIds = generateRandomNumbers(1,2630,12);
                let api = await client.api(new QueryXkcdComics({ ids:randomIds }))
                if (api.succeeded) {
                    comics.value = api.response.results
                }
            } else {
                pushState({ q: searchTerm.value })

                await (async (titleContains) => {
                    let api = await client.api(
                        new QueryXkcdComics({ titleContains, orderByDesc:'id' }))
                    // discard any invalidated api responses
                    if (titleContains === searchTerm.value) {
                        comics.value = api.response.results
                    }
                })(searchTerm.value)
            }
            loading.value = false
        },250)
        watch(searchTerm, async(newValue,oldValue) => {
            loading.value = true
            searchApi(newValue)
        })
        function createDebounce(fnc, delayMs) {
            let timeout = null;
            return (...args) => {
                clearTimeout(timeout);
                timeout = setTimeout(() => {
                    fnc(...args);
                }, delayMs || 500);
            };
        }

        function showModal(comic) {
            selected.value = comic;
            pushState({ id:comic?.id || undefined })
        }
        function done() {
            showModal(null)
        }
        function generateRandomNumbers(low, high, count) {
            const result = [];
            for (let i = 0; i < count; i++) {
                result.push(Math.floor(Math.random() * (high - low + 1) + low));
            }
            return result;
        }

        return { comics, searchTerm, hasInit, loading, selected, showModal, done }
    },
}
mount('#app', App)
```

Notice here we're creating the Request DTO with: 

```csharp
new QueryXkcdComics({ titleContains, orderByDesc:'id' })
```

The `titleContains` property is interpreted by the AutoQuery API to filter the results by the `title` column in our 
SQLite database by the populated property value.

We didn't need to add this additional functionality, and this syntax works for any matching property on the DTO. 
E.g, we could have used `explanationContains` to filter the results by the `explanation` column in our SQLite database on our API server.
AutoQuery has many of these types of features built in that work out of the box, and since AutoQuery services are ServiceStack services, 
you can customize their behaviour by adding your own custom logic to the service.

[![Filtering comics by title](/img/posts/autoquery-xkcd/column-vue-search-physics.png)](https://xkcd.netcore.io/?q=physics)

## Conclusion

In this post we've seen how we can use the AutoQuery API to quickly create a REST API for our data, and then use the ServiceStack Vue library to quickly create a Vue application that can consume the API.
We've also seen how we can use the `x` tool to update our client DTOs to match the latest generated DTOs from the ServiceStack server.

This typed end-to-end workflow is a great way to quickly create a full-stack application, and the ServiceStack Vue library is a great way to quickly create a Vue application that can consume the AutoQuery API.

### Links

- **Live Demo UI** - https://xkcd.netcore.io
- **Live Demo API** - https://ssg-examples.netcore.io/ui/QueryXkcdComics
- **UI Source Code** - https://github.com/NetCoreApps/Xkcd
- **API Source Code** - [ExampleDataApis.ServiceModel/Xkcd.cs](https://github.com/NetCoreApps/ssg-examples/blob/master/ExampleDataApis.ServiceModel/Xkcd.cs)

Let us know what you think of the [ServiceStack Vue library](https://docs.servicestack.net/vue/), and if you have any feedback or suggestions for improvements.

### Feedback & Suggestions

- [ServiceStack/Discuss](https://github.com/ServiceStack/Discuss/discussions)
- [#ServiceStack channel on Discord](https://discord.gg/w4ayGbuYpA)


# Introducing Razor SSG
Source: https://servicestack.net/posts/razor-ssg

Razor SSG is a Razor Pages powered Markdown alternative to [Ruby's Jekyll](https://jekyllrb.com/) & 
[Next.js](https://nextjs.org) that's ideal for generating static websites & blogs using C#, Razor Pages & Markdown. 

### GitHub Codespaces Friendly

In addition to having a pure Razor + .NET solution to create fast, CDN-hostable static websites, it also aims to provide a
great experience from GitHub Codespaces, where you can create, modify, preview & check-in changes before the included GitHub Actions
auto deploy changes to its GitHub Pages CDN - all from your iPad!

[![](/img/posts/razor-ssg/codespaces.png)](https://github.com/features/codespaces)

To see this in action, we walk through the entire workflow of creating, updating and adding features to a custom Razor SSG website 
from just a browser using Codespaces, that auto publishes changes to your GitHub Repo's **gh-pages** branch where it's hosted for 
free on GitHub Pages CDN:

:::youtube MRQMBrXi5Sc
Razor SSG Websites in GitHub Codespaces
:::

### Enhance with simple, modern JavaScript

For enhanced interactivity, static markdown content can be [progressively enhanced](/posts/javascript) with Vue 3 components,
as done in this example that embed's the 
[GettingStarted.mjs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/wwwroot/mjs/components/GettingStarted.mjs) Vue Component to create new Razor SSG App's below with:

```html
<getting-started template="razor-ssg"></getting-started>
```

<div class="py-8 not-prose text-base flex justify-center">
  <getting-started template="razor-ssg"></getting-started>
</div>

Although with full control over the websites `_Layout.cshtml`, you're free to use any preferred JS Module or Web Component you prefer.

## Razor Pages

Your website can be built using either Markdown `.md` or Razor `.cshtml` pages, although it's generally recommended to 
use Markdown to capture the static content for your website for improved productivity and ease of maintenance.

### Content in Markdown, Functionality in Razor Pages

The basic premise behind most built-in features is to capture static content in markdown using a combination of 
folder structure & file name conventions in addition to each markdown page's frontmatter & content. This information
is then used to power each feature using Razor pages for precise layout and functionality.

The template includes the source code for each website feature, enabling full customization that also serves as good examples
for how to implement your own custom markdown-powered website features.

### Markdown Feature Structure

All markdown features are effectively implemented in the same way, starting with a **_folder** for maintaining its static markdown
content, a **.cs** class to load the markdown and a **.cshtml** Razor Page to render it: 

| Location | Description |
| - | - |
| `/_{Feature}` | Maintains the static markdown for the feature |
| `Markdown.{Feature}.cs` | Functionality to read the feature's markdown into logical collections |
| `{Feature}.cshtml` | Functionality to Render the feature |
| [Configure.Ssg.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Configure.Ssg.cs) | Initializes and registers the feature with ASP .NET's IOC |

Lets see what this looks like in practice by walking through the "Pages" feature: 

## Pages Feature

The pages feature simply makes all pages in the **_pages** folder, available from `/{filename}`.

Where the included pages:

### [/_pages](https://github.com/NetCoreTemplates/razor-ssg/tree/main/MyApp/_pages)
 - privacy.md
 - speaking.md
 - uses.md
 
Are made available from:

 - [/privacy](https://razor-ssg.web-templates.io/privacy)
 - [/speaking](https://razor-ssg.web-templates.io/speaking)
 - [/uses](https://razor-ssg.web-templates.io/uses)

### Loading Pages Markdown

The code that loads the Pages feature markdown content is in [Markdown.Pages.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Markdown.Pages.cs):

```csharp
public class MarkdownPages : MarkdownPagesBase<MarkdownFileInfo>
{
    public MarkdownPages(ILogger<MarkdownPages> log, IWebHostEnvironment env) 
        : base(log,env) {}
    
    List<MarkdownFileInfo> Pages { get; set; } = new();
    public List<MarkdownFileInfo> VisiblePages => Pages.Where(IsVisible).ToList();

    public MarkdownFileInfo? GetBySlug(string slug) => 
        Fresh(VisiblePages.FirstOrDefault(x => x.Slug == slug));

    public void LoadFrom(string fromDirectory)
    {
        Pages.Clear();
        var fs = AssertVirtualFiles();
        var files = fs.GetDirectory(fromDirectory).GetAllFiles().ToList();
        var log = LogManager.GetLogger(GetType());
        log.InfoFormat("Found {0} pages", files.Count);

        var pipeline = CreatePipeline();

        foreach (var file in files)
        {
            var doc = Load(file.VirtualPath, pipeline);
            if (doc == null)
                continue;

            Pages.Add(doc);
        }
    }
}
```

Which ultimately just loads Markdown files using the configured [Markdig](https://github.com/xoofx/markdig) pipeline in its `Pages` 
collection which is made available via its `VisiblePages` property which returns all documents in development whilst hiding
**Draft** and content published at a **Future Date** from production builds.

### Rendering Markdown Pages

The pages are then rendered in [Page.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Page.cshtml) Razor Page
that's available from `/{slug}`

```csharp
@page "/{slug}"
@model MyApp.Page
@inject MarkdownPages Markdown

@implements IRenderStatic<MyApp.Page>
@functions {
    public List<Page> GetStaticProps(RenderContext ctx)
    {
        var markdown = ctx.Resolve<MarkdownPages>();
        return markdown.VisiblePages.Map(page => new Page { Slug = page.Slug! });
    }
}

@{
    var doc = Markdown.GetBySlug(Model.Slug);
    if (doc.Layout != null) 
        Layout = doc.Layout == "none"
            ? null
            : doc.Layout;
    ViewData["Title"] = doc.Title;
}

<link rel="stylesheet" href="css/typography.css">
<section class="flex-col md:flex-row flex justify-center mt-16 mb-16 md:mb-12">
    <h1 class="text-4xl tracking-tight font-extrabold text-gray-900">
        @doc.Title
    </h1>
</section>    
<div class="mx-auto">
    <div class="mx-auto prose lg:prose-xl mb-24">
        @Html.Raw(doc.Preview)
    </div>
</div>

@await Html.PartialAsync("HighlightIncludes")
<script>hljs.highlightAll()</script>
```

Which uses a custom layout if one is defined in its frontmatter which 
[speaking.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/speaking.md) utilizes in its **layout** frontmatter:

```yaml
---
title: Speaking
layout: _LayoutContent
---
```

To render the page using [_LayoutContent.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Shared/_LayoutContent.cshtml)
visible by the background backdrop in its [/speaking](https://razor-ssg.web-templates.io/speaking) page.

## What's New Feature

The [/whatsnew](https://razor-ssg.web-templates.io/whatsnew) page is an example of creating a custom Markdown feature to implement a portfolio or a product releases page
where a new folder is created per release, containing both release date and release or project name, with all features in that release 
maintained markdown content sorted in alphabetical order:

### [/_whatsnew](https://github.com/NetCoreTemplates/razor-ssg/tree/main/MyApp/_whatsnew)

- **/2023-03-08_Animaginary**
  - feature1.md 
- **/2023-03-18_OpenShuttle**
   - feature1.md
- **/2023-03-28_Planetaria**
   - feature1.md 

What's New follows the same structure as Pages feature which is loaded in:

 - [Markdown.WhatsNew.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Markdown.WhatsNew.cs)

and rendered in:
- [WhatsNew.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/WhatsNew.cshtml)

## Blog Feature

The blog maintains its markdown posts in a flat folder which each Markdown post containing its publish date and URL slug it 
should be published under:

### [/_posts](https://github.com/NetCoreTemplates/razor-ssg/tree/main/MyApp/_posts)

 - ...
 - 2023-01-21_start.md
 - 2023-03-21_javascript.md
 - 2023-03-28_razor-ssg.md

As the Blog has more features it requires a larger [Markdown.Blog.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Markdown.Blog.cs)
to load its Markdown posts that is rendered in several different Razor Pages for each of its Views:

| Page | Description | Example |
| - | - | - |
| [Blog.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Blog.cshtml) | Main Blog layout | [/blog](https://razor-ssg.web-templates.io/blog) | 
| [Posts/Index.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Index.cshtml) | Navigable Archive grid of Posts | [/posts](https://razor-ssg.web-templates.io/posts) | 
| [Posts/Post.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Post.cshtml) | Individual Blog Post (like this!) | [/posts/razor-ssg](https://razor-ssg.web-templates.io/posts/razor-ssg) |
| [Author.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Author.cshtml) | Display Posts by Author | [/posts/author/lucy-bates](https://razor-ssg.web-templates.io/posts/author/lucy-bates) | 
| [Tagged.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Tagged.cshtml) | Display Posts by Tag | [/posts/tagged/markdown](https://razor-ssg.web-templates.io/posts/tagged/markdown) |
| [Year.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Year.cshtml) | Display Posts by Year | [/posts/year/2023](https://razor-ssg.web-templates.io/posts/year/2023) |

### General Features

Most unique markdown features are captured in their Markdown's frontmatter metadata, but in general these features
are broadly available for all features:

 - **Live Reload** - Latest Markdown content is displayed during **Development** 
 - **Custom Layouts** - Render post in custom Razor Layout with `layout: _LayoutAlt`
 - **Drafts** - Prevent posts being worked on from being published with `draft: true`
 - **Future Dates** - Posts with a future date wont be published until that date

### Initializing and Loading Markdown Features

All markdown features are initialized in the same way in [Configure.Ssg.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Configure.Ssg.cs)
where they're registered in ASP.NET Core's IOC and initialized after the App's plugins are loaded
by injecting with the App's [Virtual Files provider](https://docs.servicestack.net/virtual-file-system)
before using it to read from the directory where the markdown content for each feature is maintained: 

```csharp
public class ConfigureSsg : IHostingStartup
{
    public void Configure(IWebHostBuilder builder) => builder
        .ConfigureServices(services =>
        {
            services.AddSingleton<RazorPagesEngine>();
            services.AddSingleton<MarkdownPages>();
            services.AddSingleton<MarkdownWhatsNew>();
            services.AddSingleton<MarkdownBlog>();
        })
        .ConfigureAppHost(afterPluginsLoaded: appHost => {
            var pages = appHost.Resolve<MarkdownPages>();
            var whatsNew = appHost.Resolve<MarkdownWhatsNew>();
            var blogPosts = appHost.Resolve<MarkdownBlog>();
            
            var features = new IMarkdownPages[] { pages, whatsNew, blogPosts }; 
            features.Each(x => x.VirtualFiles = appHost.VirtualFiles);

            // Custom initialization
            blogPosts.Authors = Authors;

            // Load feature markdown content
            pages.LoadFrom("_pages");
            whatsNew.LoadFrom("_whatsnew");
            blogPosts.LoadFrom("_posts");
        });
    });
    //...
}
```

These dependencies are then injected in the feature's Razor Pages to query and render the loaded markdown content.

### Custom Frontmatter

You can extend the `MarkdownFileInfo` type used to maintain the markdown content and metadata of each loaded Markdown file
by adding any additional metadata you want included as C# properties on:

```csharp
// Add additional frontmatter info to include
public class MarkdownFileInfo : MarkdownFileBase
{
}
```

Any additional properties are automatically populated using ServiceStack's 
[built-in Automapping](https://docs.servicestack.net/auto-mapping) which includes rich support for converting string frontmatter 
values into native .NET types.

### Updating to latest versions

You can easily update all the JavaScript dependencies used in 
[postinstall.js](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/postinstall.js) by running:

:::sh
node postinstall.js
:::

This will also update the Markdown features `*.cs` implementations which is delivered as source files instead of an external
NuGet package to enable full customization, easier debugging whilst supporting easy upgrades. 

If you do customize any of the `.cs` files, you'll want to exclude them from being updated by removing them from:

```js
const hostFiles = [
    'Markdown.Blog.cs',
    'Markdown.Pages.cs',
    'Markdown.WhatsNew.cs',
    'MarkdownPagesBase.cs',
]
```

### Markdown Tag Helper

The included [MarkdownTagHelper.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/MarkdownTagHelper.cs) can be used
in hybrid Razor Pages like [About.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/About.cshtml) 
to render the [/about](https://razor-ssg.web-templates.io/about) page which requires the flexibility of Razor Pages with a static content component which you 
prefer to maintain inline with Markdown.

The `<markdown />` tag helper renders plain HTML, which you can apply [Tailwind's @typography](https://tailwindcss.com/docs/typography-plugin) 
styles by including **typography.css** and annotating it with your preferred `prose` variant, e.g:

```html
<link rel="stylesheet" href="css/typography.css">
<markdown class="prose">
  Markdown content...
</markdown>
```

## Static Static Generation (SSG)

All features up till now describes how this template implements a Markdown powered Razor Pages .NET application, where this template
differs in its published output, where instead of a .NET App deployed to a VM or App server it generates static `*.html` files that's
bundled together with `/wwwroot` static assets in the `/dist` folder that can be previewed by launching a HTTP Server from that
folder with the built-in npm script:

:::sh
npm run serve
:::

To run **npx http-server** on `http://localhost:8080` that you can open in a browser to preview the published version of your 
site as it would be when hosted on a CDN.

### Static Razor Pages

The static generation functionality works by scanning all your Razor Pages and prerendering the pages with prerendering instructions.

### Pages with Static Routes

Pages with static routes can be marked to be prerendered by annotating it with the `[RenderStatic]` attribute as done in
[About.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/About.cshtml):

```csharp
@page "/about"
@attribute [RenderStatic]
```

Which saves the pre-rendered page using the pages route with a .html suffix, e.g: `/{@page route}.html` whilst pages with static
routes with a trailing `/` are saved to `/{@page route}/index.html` as done for 
[Posts/Index.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Index.cshtml):

```csharp
@page "/posts/"
@attribute [RenderStatic]
```

#### Explicit generated paths

To keep the generated pages in-sync with using the same routes as your Razor Pages in development it's recommended to use the implied
rendered paths, but if preferred you can specify which path the page should be rendered to instead with:

```csharp
@page "/posts/"
@attribute [RenderStatic("/posts/index.html")]
```

### Pages with Dynamic Routes

Prerendering dynamic pages follows [Next.js getStaticProps](https://nextjs.org/docs/basic-features/data-fetching/get-static-props) 
convention which you can implement using `IRenderStatic<PageModel>` by returning a Page Model for each page that should be generated
as done in [Posts/Post.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Post.cshtml) and
[Page.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Page.cshtml):

```csharp
@page "/{slug}"
@model MyApp.Page

@implements IRenderStatic<MyApp.Page>
@functions {
    public List<Page> GetStaticProps(RenderContext ctx)
    {
        var markdown = ctx.Resolve<MarkdownPages>();
        return markdown.VisiblePages.Map(page => new Page { Slug = page.Slug! });
    }
}
...
```

In this case it returns a Page Model for every **Visible** markdown page in 
[/_pages](https://github.com/NetCoreTemplates/razor-ssg/tree/main/MyApp/_pages) that ends up rendering the following pages in `/dist`:

 - `/privacy.html`
 - `/speaking.html`
 - `/uses.html`

### Limitations

The primary limitations for developing statically generated Apps is that a **snapshot** of entire App is generated at deployment, 
which prohibits being able to render different content **per request**, e.g. for Authenticated users which would require executing
custom JavaScript after the page loads to dynamically alter the page's initial content.

Otherwise in practice you'll be able develop your Razor Pages utilizing Razor's full feature-set, the primary concessions stem
from Pages being executed in a static context which prohibits pages from returning dynamic content per request, instead any
"different views" should be maintained in separate pages.

#### No QueryString Params

As the generated pages should adopt the same routes as your Razor Pages you'll need to avoid relying on **?QueryString** params
and instead capture all required parameters for a page in its **@page route** as done for:

[Posts/Author.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Author.cshtml)

```csharp
@page "/posts/author/{slug}"
@model AuthorModel
@inject MarkdownBlog Blog

@implements IRenderStatic<AuthorModel>
@functions {
    public List<AuthorModel> GetStaticProps(RenderContext ctx) => ctx.Resolve<MarkdownBlog>()
        .AuthorSlugMap.Keys.Map(x => new AuthorModel { Slug = x });
}
...
```

Which lists all posts by an Author, e.g: [/posts/author/lucy-bates](https://razor-ssg.web-templates.io/posts/author/lucy-bates), likewise required for:

[Posts/Tagged.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Tagged.cshtml)

```csharp
@page "/posts/tagged/{slug}"
@model TaggedModel
@inject MarkdownBlog Blog

@implements IRenderStatic<TaggedModel>
@functions {
    public List<TaggedModel> GetStaticProps(RenderContext ctx) => ctx.Resolve<MarkdownBlog>()
        .TagSlugMap.Keys.Map(x => new TaggedModel { Slug = x });
}
...
```

Which lists all related posts with a specific tag, e.g: [/posts/tagged/markdown](https://razor-ssg.web-templates.io/posts/tagged/markdown), and for:

[Posts/Year.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Posts/Year.cshtml)

```csharp
@page "/posts/year/{year}"
@model YearModel
@inject MarkdownBlog Blog

@implements IRenderStatic<YearModel>
@functions {
    public List<YearModel> GetStaticProps(RenderContext ctx) => ctx.Resolve<MarkdownBlog>()
        .VisiblePosts.Select(x => x.Date.GetValueOrDefault().Year)
            .Distinct().Map(x => new YearModel { Year = x });
}

...
```

Which lists all posts published in a specific year, e.g: [/posts/year/2023](https://razor-ssg.web-templates.io/posts/year/2023).

Conceivably these "different views" could've been implemented by the same page with different `?author`, `?tag` and `?year`
QueryString params, but are instead extracted into different pages to support its statically generated `*.html` outputs.

## Prerendering Task

The **prerender** [AppTask](https://docs.servicestack.net/app-tasks) that pre-renders the entire website is also registered in
[Configure.Ssg.cs](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Configure.Ssg.cs):

```csharp
  .ConfigureAppHost(afterAppHostInit: appHost =>
  {
      // prerender with: `$ npm run prerender` 
      AppTasks.Register("prerender", args =>
      {
          var distDir = appHost.ContentRootDirectory.RealPath.CombineWith("dist");
          if (Directory.Exists(distDir))
              FileSystemVirtualFiles.DeleteDirectory(distDir);
          FileSystemVirtualFiles.CopyAll(
              new DirectoryInfo(appHost.ContentRootDirectory.RealPath.CombineWith("wwwroot")),
              new DirectoryInfo(distDir));
          var razorFiles = appHost.VirtualFiles.GetAllMatchingFiles("*.cshtml");
          RazorSsg.PrerenderAsync(appHost, razorFiles, distDir).GetAwaiter().GetResult();
      });
  });
  //...
```

Which we can see:
1. Deletes `/dist` folder
2. Copies `/wwwroot` contents into `/dist`
3. Passes all App's Razor `*.cshtml` files to `RazorSsg` to do the pre-rendering

Where it processes all pages with `[RenderStatic]` and `IRenderStatic<PageModel>` prerendering instructions to the 
specified `/dist` folder.

### Previewing prerendered site

To preview your SSG website, run the prerendered task with:  

:::sh
npm run prerender
:::

Which renders your site to `/_dist` which you can run a HTTP Server from with:

:::sh
npm run serve
:::

That you can preview with your browser at `http://localhost:8080`.

### Publishing

The included [build.yml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/.github/workflows/build.yml) GitHub Action
takes care of running the prerendered task and deploying it to your Repo's GitHub Pages where it will be available at:

    https://$org_name.github.io/$repo/

Alternatively you can use a [Custom domain for GitHub Pages](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/about-custom-domains-and-github-pages)
by registering a CNAME DNS entry for your preferred Custom Domain, e.g:

| Record | Type | Value | TTL|
| - | - | - | - |
| **mydomain.org** | CNAME | **org_name**.github.io | 3600 |

That you can either [configure in your Repo settings](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site#configuring-a-subdomain)
or if you prefer to maintain it with your code-base, save the domain name to `/wwwroot/CNAME`, e.g:

```
www.mydomain.org
```

### Benefits after migrating from Jekyll

Whilst still only at **v1** release, we found it already had a number of advantages over the existing Jekyll static website:

 - Faster live reloads
 - C#/Razor more type-save & productive than Ruby/Liquid
 - Greater flexibility in implementing new features
 - Better IDE support (from Rider)
 - Ability to reuse our .NET libraries
 - Better development experience

The last point ultimately prompted seeking an alternative solution as previously Jekyll was used from Windows/WSL which 
was awkward to manage from a different filesystem with Jekyll upgrades breaking RubyMine support forcing the use of 
text editors to maintain its code-base and content.

### Used by the new [servicestack.net](https://servicestack.net)

Deterred by the growing complexity of current SSG solutions, we decided to create a new solution using C#/Razor 
(our preferred technology for generating server HTML) with a clean implementation that allowed full control
with an **npm dependency-free** solution letting us adopt our preferred approach to 
[Simple, Modern JavaScript](/posts/javascript) without any build-tooling or SPA complexity.

We're happy with the results of [https://servicestack.net](https://servicestack.net) new Razor SSG website:

[![](/img/posts/razor-ssg/servicestack.net.png)](https://servicestack.net)

A clean, crisp code-base utilizing simple JS Module Vue 3 components, the source code of which is publicly maintained at:

- [https://github.com/servicestack/servicestack.net](https://github.com/servicestack/servicestack.net)

Which serves as a good example at how well this template scales for larger websites.

#### Markdown Videos Feature

It only needed one new Markdown feature to display our growing video library:

 - [/_videos](https://github.com/ServiceStack/servicestack.net/tree/main/MyApp/_videos) - Directory of Markdown Video collections 
 - [Markdown.Videos.cs](https://github.com/ServiceStack/servicestack.net/blob/main/MyApp/Markdown.Videos.cs) - Loading Video feature markdown content
 - [Shared/VideoGroup.cshtml](https://github.com/ServiceStack/servicestack.net/blob/main/MyApp/Pages/Shared/VideoGroup.cshtml) - Razor Page for displaying Video Collection

Which you're free to reuse in your own websites needing a similar feature.

#### Feedback & Feature Requests Welcome

In future we'll look at expanding this template with generic Markdown features suitable for websites, blogs & portfolios, 
or maintain a shared community collection if there ends up being community contributions of Razor SSG & Markdown features.

In the meantime, we welcome any feedback or new feature requests at:

### [https://servicestack.net/ideas](https://servicestack.net/ideas)


# New Razor SSG generated servicestack.net
Source: https://servicestack.net/posts/new_razor_ssg_website

## Celebrating 150M Total Downloads

We're happy to report that ServiceStack has eclipsed **150M total downloads!**

<div class="my-16 flex mx-auto justify-center">
   <a href="https://www.nuget.org/profiles/servicestack"><img class="w-96" src="/img/posts/razor-ssg/150M-downloads.png"></a>
</div>

Which comes just after a year from **100M downloads** in [v6 Release](https://docs.servicestack.net/releases/v6_00) when we announced 
[ServiceStack is now FREE for Individuals & OSS!](https://servicestack.net/free)

We're celebrating this new milestone with a **brand new website**, rewritten from Ruby's Jekyll with an exciting new 
[Razor SSG](https://razor-ssg.web-templates.io) project template which takes advantage of the advances we've made in the last few releases 
with our npm dependency-free approach to [Simple, Modern JavaScript](/posts/javascript), 
the built-in support for
[Prerendering Razor Pages](/posts/prerendering) and the rich Tailwind 
[Vue Component Library](https://docs.servicestack.net/vue/)
to create an enjoyable experience for creating Fast, FREE, beautiful, CDN-hostable static generated websites & blogs!

<div class="not-prose my-16 px-4 sm:px-6">
<div class="text-center"><h3 id="new-website" class="text-4xl sm:text-5xl md:text-6xl tracking-tight font-extrabold text-gray-900">
    <a class="text-indigo-600 hover:text-indigo-600" href="https://servicestack.net">servicestack.net</a>
</h3></div>
<p class="mx-auto mt-5 max-w-prose text-xl text-gray-500">
    Checkout the new Vue, Tailwind & Razor SSG powered <a class="text-indigo-600 hover:text-indigo-600" href="https://servicestack.net">servicestack.net</a> website!
</p>
<div class="my-8">
<a href="https://servicestack.net" class="not-prose max-w-4xl"><div class="block flex justify-center shadow hover:shadow-lg rounded"><img class="" src="/img/whatsnew/v6.8/servicestack.net-home-1440.png"></div></a>
</div></div>

### Migrating away from Jekyll

We have a lot to thank Jekyll for as the pioneer of statically generated websites and igniting the [Jamstack](https://jamstack.org) movement
showing the benefits of markdown powered, statically-generated websites. However managing Ruby from Windows/WSL has always been a source of friction
which was awkward to manage from a different filesystem which ultimately prompted seeking an alternative solution after Jekyll upgrades broke RubyMine
support forcing the use of text editors to maintain our website code-base and content.

### Search for a better SSG Solution

Our experience with maintaining extensive [documentation in VitePress](https://servicestack.net/posts/jekyll-to-vitepress) and static website
content in Jekyll has effectively taught us that any heavy content website that can be maintained in Markdown and statically generated, should be.
Not only is it the fastest way to deliver static content from CDN edge caches, inexpensive & portable to host,
but it's vastly more reliable than an App Server that's dependent on uptime of VMs, infrastructure dependencies, load balancer and firewall misconfigurations,
account or billing issues, etc.

### SSG C# Razor Pages with Vue & Tailwind

Deterred by the growing complexity of current SSG solutions, we decided to create a new solution using C# & Razor Pages
with a clean implementation that allowed full control with an **npm dependency-free** solution letting us adopt our preferred approach to
[Simple, Modern JavaScript](/posts/javascript) without any build-tooling or SPA complexity.

We're happy with the results of [https://servicestack.net](https://servicestack.net) new Razor SSG website -
a clean, crisp code-base utilizing simple JS Module Vue 3 components, the source code of which is publicly maintained at:

<div class="my-8 flex justify-center">
    <a class="text-3xl text-indigo-600 hover:text-indigo-800" href="https://github.com/servicestack/servicestack.net">https://github.com/servicestack/servicestack.net</a>
</div>

Which serves as a good example at how well Razor SSG scales for larger websites.

### Benefits from Jekyll

Whilst still only at **v1** release, we found it already had a number of advantages over the existing Jekyll static website:

- Faster live reloads
- C#/Razor more type-save & productive than Ruby/Liquid
- Greater flexibility in implementing new features
- Better IDE support (from Rider)
- Ability to reuse our .NET libraries
- Better development experience from Windows


# Simple, Modern JavaScript
Source: https://servicestack.net/posts/javascript

<svg class="sm:float-left mr-8 w-24 h-24" style="margin-top:0" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 630 630">
<rect width="630" height="630" fill="#f7df1e"/>
<path d="m423.2 492.19c12.69 20.72 29.2 35.95 58.4 35.95 24.53 0 40.2-12.26 40.2-29.2 0-20.3-16.1-27.49-43.1-39.3l-14.8-6.35c-42.72-18.2-71.1-41-71.1-89.2 0-44.4 33.83-78.2 86.7-78.2 37.64 0 64.7 13.1 84.2 47.4l-46.1 29.6c-10.15-18.2-21.1-25.37-38.1-25.37-17.34 0-28.33 11-28.33 25.37 0 17.76 11 24.95 36.4 35.95l14.8 6.34c50.3 21.57 78.7 43.56 78.7 93 0 53.3-41.87 82.5-98.1 82.5-54.98 0-90.5-26.2-107.88-60.54zm-209.13 5.13c9.3 16.5 17.76 30.45 38.1 30.45 19.45 0 31.72-7.61 31.72-37.2v-201.3h59.2v202.1c0 61.3-35.94 89.2-88.4 89.2-47.4 0-74.85-24.53-88.81-54.075z"/>
</svg>

JavaScript has progressed significantly in recent times where many of the tooling & language enhancements
that we used to rely on external tools for is now available in modern browsers alleviating the need for
complex tooling and npm dependencies that have historically plagued modern web development.

The good news is that the complex npm tooling that was previously considered mandatory in modern JavaScript App 
development can be considered optional as we can now utilize modern browser features like 
[async/await](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/async_function),
[JavaScript Modules](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules), 
[dynamic imports](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/import), 
[import maps](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script/type/importmap)
and [modern language features](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide) for a 
sophisticated development workflow without the need for any npm build tools. 

### Bringing Simplicity Back

The [Blazor Vue](/posts/net8-best-blazor) template focuses on simplicity and eschews many aspects that has 
complicated modern JavaScript development, specifically:

 - No npm node_modules or build tools
 - No client side routing
 - No heavy client state

Effectively abandoning the traditional SPA approach in lieu of a simpler [MPA](https://docs.astro.build/en/concepts/mpa-vs-spa/) 
development model using Razor Pages for Server Rendered content with any interactive UIs progressively enhanced with JavaScript.

#### Freedom to use any JS library

Avoiding the SPA route ends up affording more flexibility on which JS libraries each page can use as without heavy bundled JS
blobs of all JS used in the entire App, it's free to only load the required JS each page needs to best implement its
required functionality, which can be any JS library, preferably utilizing ESM builds that can be referenced from a
[JavaScript Module](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules), taking advantage of the module system
native to modern browsers able to efficiently download the declarative matrix of dependencies each script needs.

### Best libraries for progressive Multi Page Apps

It includes a collection of libraries we believe offers the best modern development experience in Progressive
MPA Web Apps, specifically:

#### [Tailwind CLI](https://tailwindcss.com/docs/installation)
Tailwind enables a responsive, utility-first CSS framework for creating maintainable CSS at scale without the need for any CSS
preprocessors like Sass, which is configured to run from an npx script to avoid needing any node_module dependencies.

#### [Vue 3](https://vuejs.org/guide/introduction.html)
Vue is a popular Progressive JavaScript Framework that makes it easy to create interactive Reactive Components whose
[Composition API](https://vuejs.org/api/composition-api-setup.html) offers a nice development model without requiring any
pre-processors like JSX.

Where creating a component is as simple as:

```js
const Hello = {
    template: `<b>Hello, {{name}}!</b>`,
    props: { name:String }
}
```
<div class="text-center text-2xl py-2">
    <hello name="Vue 3"></hello>
</div>

Or a simple reactive example:

```js
import { ref } from "vue"

const Counter = {
    template: `<b @click="count++">Counter {{count}}</b>`,
    setup() {
        let count = ref(1)
        return { count }
    }
}
```

<div class="text-center text-2xl py-2 cursor-pointer select-none">
    <counter></counter>
</div>

### Vue Components in Markdown

Inside `.md` Markdown pages Vue Components can be embedded using Vue's progressive 
[HTML Template Syntax](https://vuejs.org/guide/essentials/template-syntax.html):

```html
<hello name="Vue 3"></hello>
<counter></counter>
```

### Vue Components in Razor Pages

Inside `.cshtml` Razor Pages these components can be mounted using the standard [Vue 3 mount](https://vuejs.org/api/application.html#app-mount) API, but to 
make it easier we've added additional APIs for declaratively mounting components to pages using `data-component` and `data-props`
attributes:  

```html
<div data-component="Hello" data-props="{ name: 'Vue 3' }"></div>
```

Alternatively they can be programmatically added using the custom `mount` method in `api.mjs`:

```js
import { mount } from "/mjs/api.mjs"
mount('#counter', Counter)
```

Both methods create components with access to all your Shared Components and any 3rd Party Plugins which
we can preview in this example that uses **@servicestack/vue**'s 
[PrimaryButton](https://docs.servicestack.net/vue/navigation#primarybutton)
and [ModalDialog](https://docs.servicestack.net/vue/modals):


```js
const Plugin = {
    template:`<div>
        <PrimaryButton @click="show=true">Open Modal</PrimaryButton>
        <ModalDialog v-if="show" @done="show=false">
            <div class="p-8">Hello @servicestack/vue!</div>
        </ModalDialog>
    </div>`,
    setup() {
        const show = ref(false)
        return { show }
    }
}
```

```html
<plugin></plugin>
```

<div class="text-center">
    <plugin id="plugin" class="text-2xl py-4"></plugin>
</div>

### Vue HTML Templates

An alternative progressive approach for creating Reactive UIs with Vue is by embedding its HTML markup directly in `.html` pages using
[HTML Template Syntax](https://vuejs.org/guide/essentials/template-syntax.html) which is both great for performance
as the DOM UI can be rendered before the Vue Component is initialized. UI elements you want hidden can use Vue's 
[v-cloak](https://vuejs.org/api/built-in-directives.html#v-cloak) attribute where they'll be hidden until components are initialized. 

It's also great for development as it lets you cohesively maintain most pages functionality need in the HTML page itself - in
isolation with the rest of the website, i.e. instead of spread across multiple external `.js` source files that for 
SPAs unnecessarily increases the payload sizes of JS bundles with functionality that no other pages need. 

With Vue's HTML syntax you can maintain the Vue template in HTML and just use embedded JavaScript for the Reactive UI's functionality, e.g:

```html
<div id="app">
    <primary-button v-on:click="show=true">Open Modal</primary-button>
    <modal-dialog v-if="show" v-on:done="show=false">
        <div class="p-8">Hello @servicestack/vue!</div>
    </modal-dialog>
</div>
<script>
const App = {
    setup() {
        const show = ref(false)
        return { show }
    }
}
mount('#app', App)
</script>
```

This is the approach used to develop [Vue Stable Diffusion](/posts/vue-stable-diffusion) where all functionality specific
to the page is maintained in the page itself, whilst any common functionality is maintained in external JS Modules loaded
on-demand by the Browser when needed.

### @servicestack/vue
[@servicestack/vue](https://github.com/ServiceStack/servicestack-vue) is our growing Vue 3 Tailwind component library with a number of rich Tailwind components useful 
in .NET Web Apps, including Input Components with auto form validation binding which is used by all HTML forms in
the [razor](https://github.com/NetCoreTemplates/razor) template. 

<vue-component-gallery></vue-component-gallery>

### @servicestack/client
[@servicestack/client](https://docs.servicestack.net/javascript-client) is our generic JS/TypeScript client library
which enables a terse, typed API for using your App's typed DTOs from the built-in 
[JavaScript ES6 Classes](https://docs.servicestack.net/javascript-add-servicestack-reference) support to enable an effortless 
end-to-end Typed development model for calling your APIs **without any build steps**, e.g:

```html
<input type="text" id="txtName">
<div id="result"></div>

<script type="module">
import { JsonApiClient, $1, on } from '@servicestack/client'
import { Hello } from '/types/mjs'

on('#txtName', {
    async keyup(el) {
        const client = JsonApiClient.create()
        const api = await client.api(new Hello({ name:el.target.value }))
        $1('#result').innerHTML = api.response.result
    }
})
</script>
```

For better IDE intelli-sense during development, save the annotated Typed DTOs to disk with:

:::sh
npm run dtos
:::

That can be referenced instead to unlock your IDE's static analysis type-checking and intelli-sense benefits during development:

```js
import { Hello } from '/js/dtos.mjs'
client.api(new Hello({ name }))
```

You'll typically use all these libraries in your **API-enabled** components as seen in the 
[HelloApi.mjs](https://github.com/NetCoreTemplates/razor/blob/main/MyApp/wwwroot/mjs/components/HelloApi.mjs)
component on the home page which calls the [Hello](/ui/Hello) API on each key press:

```js
import { ref } from "vue"
import { useClient } from "@servicestack/vue"
import { Hello } from "../dtos.mjs"

export default {
    template:/*html*/`<div class="flex flex-wrap justify-center">
        <TextInput v-model="name" @keyup="update" />
        <div class="ml-3 mt-2 text-lg">{{ result }}</div>
    </div>`,
    props:['value'],
    setup(props) {
        let name = ref(props.value)
        let result = ref('')
        let client = useClient()

        async function update() {
            let api = await client.api(new Hello({ name }))
            if (api.succeeded) {
                result.value = api.response.result
            }
        }
        update()

        return { name, update, result }
    }
}
```

Which we can also mount below:

```html
<hello-api value="Vue 3"></hello-api>
```

<hello-api value="Vue 3" class="w-full font-semibold"></hello-api>

We'll also go through and explain other features used in this component:

#### `/*html*/`

Although not needed in [Rider](rider) (which can automatically infer HTML in strings), the `/*html*/` type hint can be used 
to instruct tooling like the [es6-string-html](https://marketplace.visualstudio.com/items?itemName=Tobermory.es6-string-html)
VS Code extension to provide syntax highlighting and an enhanced authoring experience for HTML content in string literals. 

### useClient

[useClient()](https://docs.servicestack.net/vue/use-client) provides managed APIs around the `JsonServiceClient` 
instance registered in Vue App's with:

```js
let client = JsonApiClient.create()
app.provide('client', client)
```

Which maintains contextual information around your API calls like **loading** and **error** states, used by `@servicestack/vue` components to 
enable its auto validation binding. Other functionality in this provider include:

```js
let { 
    api, apiVoid, apiForm, apiFormVoid, // Managed Typed ServiceClient APIs
    loading, error,                     // Maintains 'loading' and 'error' states
    setError, addFieldError,            // Add custom errors in client
    unRefs                              // Returns a dto with all Refs unwrapped
} = useClient()
```

Typically you would need to unwrap `ref` values when calling APIs, i.e:

```js
let client = JsonApiClient.create()
let api = await client.api(new Hello({ name:name.value }))
```

#### useClient - api

This is unnecessary in useClient `api*` methods which automatically unwraps ref values, allowing for the more pleasant API call:

```js
let api = await client.api(new Hello({ name }))
```

#### useClient - unRefs

But as DTOs are typed, passing reference values will report a type annotation warning in IDEs with type-checking enabled, 
which can be resolved by explicitly unwrapping DTO ref values with `unRefs`:

```js
let api = await client.api(new Hello(unRefs({ name })))
```

#### useClient - setError

`setError` can be used to populate client-side validation errors which the 
[SignUp.mjs](https://github.com/NetCoreTemplates/vue-mjs/blob/main/MyApp/wwwroot/Pages/SignUp.mjs)
component uses to report an invalid submissions when passwords don't match:

```js
const { api, setError } = useClient()
async function onSubmit() {
    if (password.value !== confirmPassword.value) {
        setError({ fieldName:'confirmPassword', message:'Passwords do not match' })
        return
    }
    //...
}
```

### Form Validation

All `@servicestack/vue` Input Components support contextual validation binding that's typically populated from API
[Error Response DTOs](https://docs.servicestack.net/error-handling) but can also be populated from client-side validation
as done above.

#### Explicit Error Handling

This populated `ResponseStatus` DTO can either be manually passed into each component's **status** property as done in [/TodoMvc](/TodoMvc):

```html
<template id="TodoMvc-template">
    <div class="mb-3">
        <text-input :status="store.error" id="text" label="" placeholder="What needs to be done?"
                    v-model="store.newTodo" v-on:keyup.enter.stop="store.addTodo()"></text-input>
    </div>
    <!-- ... -->
</template>
```

Where if you try adding an empty Todo the `CreateTodo` API will fail and populate its `store.error` reactive property with the
APIs Error Response DTO which the `<TextInput />` component checks to display any field validation errors adjacent to the HTML Input
with matching `id` fields:

```js
let store = {
    /** @type {Todo[]} */
    todos: [],
    newTodo:'',
    error:null,
    async addTodo() {
        this.todos.push(new Todo({ text:this.newTodo }))
        let api = await client.api(new CreateTodo({ text:this.newTodo }))
        if (api.succeeded)
            this.newTodo = ''
        else
            this.error = api.error
    },
    //...
}
```

#### Implicit Error Handling

More often you'll want to take advantage of the implicit validation support in `useClient()` which makes its state available to child
components, alleviating the need to explicitly pass it in each component as seen in razor's
[Contacts.mjs](https://github.com/NetCoreTemplates/razor/blob/net6/MyApp/wwwroot/Pages/Contacts.mjs) `Edit` component for its
Contacts page which doesn't do any manual error handling:

```js
const Edit = {
    template:/*html*/`<SlideOver @done="close" title="Edit Contact">
    <form @submit.prevent="submit">
      <input type="submit" class="hidden">
      <fieldset>
        <ErrorSummary except="title,name,color,filmGenres,age,agree" class="mb-4" />
        <div class="grid grid-cols-6 gap-6">
          <div class="col-span-6 sm:col-span-3">
            <SelectInput id="title" v-model="request.title" :options="enumOptions('Title')" />
          </div>
          <div class="col-span-6 sm:col-span-3">
            <TextInput id="name" v-model="request.name" required placeholder="Contact Name" />
          </div>
          <div class="col-span-6 sm:col-span-3">
            <SelectInput id="color" v-model="request.color" :options="colorOptions" />
          </div>
          <div class="col-span-6 sm:col-span-3">
            <SelectInput id="favoriteGenre" v-model="request.favoriteGenre" :options="enumOptions('FilmGenre')" />
          </div>
          <div class="col-span-6 sm:col-span-3">
            <TextInput type="number" id="age" v-model="request.age" />
          </div>
        </div>
      </fieldset>
    </form>
    <template #footer>
      <div class="flex justify-between space-x-3">
        <div><ConfirmDelete @delete="onDelete">Delete</ConfirmDelete></div>
        <div><PrimaryButton @click="submit">Update Contact</PrimaryButton></div>
      </div>
    </template>
  </SlideOver>`,
    props:['contact'],
    emits:['done'],
    setup(props, { emit }) {
        const client = useClient()
        const request = ref(new UpdateContact(props.contact))
        const colorOptions = propertyOptions(getProperty('UpdateContact','Color'))

        async function submit() {
            const api = await client.api(request.value)
            if (api.succeeded) close()
        }

        async function onDelete () {
            const api = await client.apiVoid(new DeleteContact({ id:props.id }))
            if (api.succeeded) close()
        }

        const close = () => emit('done')

        return { request, enumOptions, colorOptions, submit, onDelete, close }
    }
}
```

Effectively making form validation binding a transparent detail where all `@servicestack/vue`
Input Components are able to automatically apply contextual validation errors next to the fields they apply to:

![](https://raw.githubusercontent.com/ServiceStack/docs/master/docs/images/scripts/edit-contact-validation.png)

### AutoForm Components

We can elevate our productivity even further with
[Auto Form Components](https://docs.servicestack.net/vue/autoform) that can automatically generate an
instant API-enabled form with validation binding by just specifying the Request DTO you want to create the form of, e.g:

```html
<AutoCreateForm type="CreateBooking" formStyle="card" />
```

<div class="not-prose">
    <auto-create-form type="CreateBooking" form-style="card"></auto-create-form>
</div>

The AutoForm components are powered by your [App Metadata](https://docs.servicestack.net/vue/use-appmetadata) which allows creating 
highly customized UIs from [declarative C# attributes](https://docs.servicestack.net/locode/declarative) whose customizations are
reused across all ServiceStack Auto UIs, including:

 - [API Explorer](https://docs.servicestack.net/api-explorer) 
 - [Locode](https://docs.servicestack.net/locode/)
 - [Blazor Tailwind Components](https://docs.servicestack.net/templates-blazor-components)

### Form Input Components

In addition to including Tailwind versions of the standard [HTML Form Inputs](https://docs.servicestack.net/vue/form-inputs) controls to create beautiful Tailwind Forms,
it also contains a variety of integrated high-level components:

- [FileInput](https://docs.servicestack.net/vue/fileinput)
- [TagInput](https://docs.servicestack.net/vue/taginput)
- [Autocomplete](https://docs.servicestack.net/vue/autocomplete)

### useAuth

Your Vue.js code can access Authenticated Users using [useAuth()](https://docs.servicestack.net/vue/use-auth)
which can also be populated without the overhead of an Ajax request by embedding the response of the built-in
[Authenticate API](/ui/Authenticate?tab=details) inside `_Layout.cshtml` with:

```html
<script type="module">
import { useAuth } from "@@servicestack/vue"
const { signIn } = useAuth()
signIn(@await Html.ApiAsJsonAsync(new Authenticate()))
</script>
```

Where it enables access to the below [useAuth()](https://docs.servicestack.net/vue/use-auth) utils for inspecting the 
current authenticated user:  

```js
const { 
    signIn,           // Sign In the currently Authenticated User
    signOut,          // Sign Out currently Authenticated User
    user,             // Access Authenticated User info in a reactive Ref<AuthenticateResponse>
    isAuthenticated,  // Check if the current user is Authenticated in a reactive Ref<boolean>
    hasRole,          // Check if the Authenticated User has a specific role
    hasPermission,    // Check if the Authenticated User has a specific permission
    isAdmin           // Check if the Authenticated User has the Admin role
} = useAuth()
```

This is used in [Bookings.mjs](https://github.com/NetCoreTemplates/razor/blob/main/MyApp/wwwroot/pages/Bookings.mjs)
to control whether the `<AutoEditForm>` component should enable its delete functionality:

```js
export default {
    template/*html*/:`
    <AutoEditForm type="UpdateBooking" :deleteType="canDelete ? 'DeleteBooking' : null" />
    `,
    setup(props) {
        const { hasRole } = useAuth()
        const canDelete = computed(() => hasRole('Manager'))
        return { canDelete }
    }
}
```

#### [JSDoc](https://jsdoc.app)

We get great value from using [TypeScript](https://www.typescriptlang.org) to maintain our libraries typed code bases, however it
does mandate using an external tool to convert it to valid JS before it can be run, something the new Razor Vue.js templates expressly avoids.

Instead it adds JSDoc type annotations to code where it adds value, which at the cost of slightly more verbose syntax enables much of the
same static analysis and intelli-sense benefits of TypeScript, but without needing any tools to convert it to valid JavaScript, e.g:

```js
/** @param {KeyboardEvent} e */
function validateSafeName(e) {
    if (e.key.match(/[\W]+/g)) {
        e.preventDefault()
        return false
    }
}
```

#### TypeScript Language Service

Whilst the code-base doesn't use TypeScript syntax in its code base directly, it still benefits from TypeScript's language services 
in IDEs for the included libraries from the TypeScript definitions included in `/lib/typings`, downloaded in
[postinstall.js](https://github.com/NetCoreTemplates/razor/blob/main/MyApp/postinstall.js) after **npm install**.

### Import Maps

[Import Maps](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script/type/importmap) is a useful browser feature that allows
specifying optimal names for modules, that can be used to map package names to the implementation it should use, e.g:

```csharp
@Html.StaticImportMap(new() {
    ["vue"]                  = "/lib/mjs/vue.mjs",
    ["@servicestack/client"] = "/lib/mjs/servicestack-client.mjs",
    ["@servicestack/vue"]    = "/lib/mjs/servicestack-vue.mjs",
})
```

Where they can be freely maintained in one place without needing to update any source code references.
This allows source code to be able to import from the package name instead of its physical location:

```js
import { ref } from "vue"
import { useClient } from "@servicestack/vue"
import { JsonApiClient, $1, on } from "@servicestack/client"
```

It's a great solution for specifying using local unminified debug builds during **Development**, and more optimal CDN hosted 
production builds when running in **Production**, alleviating the need to rely on complex build tools to perform this code transformation for us:

```csharp
@Html.ImportMap(new()
{
    ["vue"]                  = ("/lib/mjs/vue.mjs",                 "https://unpkg.com/vue@3/dist/vue.esm-browser.prod.js"),
    ["@servicestack/client"] = ("/lib/mjs/servicestack-client.mjs", "https://unpkg.com/@servicestack/client@2/dist/servicestack-client.min.mjs"),
    ["@servicestack/vue"]    = ("/lib/mjs/servicestack-vue.mjs",    "https://unpkg.com/@servicestack/vue@3/dist/servicestack-vue.min.mjs")
})
```

Note: Specifying exact versions of each dependency improves initial load times by eliminating latency from redirects. 

Or if you don't want your Web App to reference any external dependencies, have the ImportMap reference local minified production builds instead:

```csharp
@Html.ImportMap(new()
{
    ["vue"]                  = ("/lib/mjs/vue.mjs",                 "/lib/mjs/vue.min.mjs"),
    ["@servicestack/client"] = ("/lib/mjs/servicestack-client.mjs", "/lib/mjs/servicestack-client.min.mjs"),
    ["@servicestack/vue"]    = ("/lib/mjs/servicestack-vue.mjs",    "/lib/mjs/servicestack-vue.min.mjs")
})
```

#### Polyfill for Safari

Unfortunately Safari is the last modern browser to [support import maps](https://caniuse.com/import-maps) which is only now in
Technical Preview. Luckily this feature can be polyfilled with the [ES Module Shims](https://github.com/guybedford/es-module-shims):

```html
@if (Context.Request.Headers.UserAgent.Any(x => x.Contains("Safari") && !x.Contains("Chrome")))
{
    <script async src="https://ga.jspm.io/npm:es-module-shims@1.6.3/dist/es-module-shims.js"></script>
}
```

### Fast Component Loading

SPAs are notorious for being slow to load due to needing to download large blobs of JavaScript bundles that it needs to initialize
with their JS framework to mount their App component before it starts fetching the data from the server it needs to render its components.

A complex solution to this problem is to server render the initial HTML content then re-render it again on the client after the page loads.
A simpler solution is to avoid unnecessary ajax calls by embedding the JSON data the component needs in the page that loads it, which is what 
[/TodoMvc](/TodoMvc) does to load its initial list of todos using the [Service Gateway](https://docs.servicestack.net/service-gateway) 
to invoke APIs in process and embed its JSON response with:

```html
<script>todos = @await ApiResultsAsJsonAsync(new QueryTodos())</script>
<script type="module">
import TodoMvc from "/Pages/TodoMvc.mjs"
import { mount } from "/mjs/app.mjs"
mount('#todomvc', TodoMvc, { todos })
</script>
```

Where `ApiResultsAsJsonAsync` is a simplified helper that uses the `Gateway` to call your API and returns its unencoded JSON response:

```csharp
(await Gateway.ApiAsync(new QueryTodos())).Response?.Results.AsRawJson();
```

The result of which should render the List of Todos instantly when the page loads since it doesn't need to perform any additional Ajax requests
after the component is loaded.

### Fast Page Loading

We can get SPA-like page loading performance using htmx's [Boosting](https://htmx.org/docs/#boosting) feature which avoids full page reloads
by converting all anchor tags to use Ajax to load page content into the page body, improving perceived performance from needing to reload 
scripts and CSS in `<head>`.

This is used in [Header.cshtml](https://github.com/NetCoreTemplates/razor/blob/main/MyApp/Pages/Shared/Header.cshtml) to **boost** all
main navigation links:

```html
<nav hx-boost="true">
    <ul>
        <li><a href="/Blog">Blog</a></li>
    </ul>
</nav>
```

htmx has lots of useful [real world examples](https://htmx.org/examples/) that can be activated with declarative attributes, 
another useful feature is the [class-tools](https://htmx.org/extensions/class-tools/) extension to hide elements from 
appearing until after the page is loaded:

```html
<div id="signin"></div>
<div class="hidden mt-5 flex justify-center" classes="remove hidden:load">
    @Html.SrcPage("SignIn.mjs")
</div>
```

Which is used to reduce UI yankiness from showing server rendered content before JS components have loaded. 

### @servicestack/vue Library

[@servicestack/vue](https://docs.servicestack.net/vue/) is our cornerstone library for enabling a highly productive 
Vue.js development model across our [Vue Tailwind Project templates](https://docs.servicestack.net/templates-vue) which 
we'll continue to significantly invest in to unlock even greater productivity benefits in all Vue Tailwind Apps.

In addition to a variety of high-productive components, it also contains a core library of functionality 
underpinning the Vue Components that most Web Apps should also find useful: 

<vue-component-library class="mt-4"></vue-component-library>


# Develop using JetBrains Rider
Source: https://servicestack.net/posts/rider

<a href="https://www.jetbrains.com/rider/">
<img src="https://raw.githubusercontent.com/ServiceStack/docs/master/docs/images/svg/rider.svg" 
     class="sm:float-left mr-8 w-24 h-24" style="margin-top:0"></a>

[JetBrains Rider](https://www.jetbrains.com/rider/) is our recommended IDE for any C# + JavaScript development as it 
offers a great development UX for both, including excellent support 
for TypeScript and popular JavaScript Framework SPA assets like [Vue SFC's](https://v3.vuejs.org/guide/single-file-component.html).

#### Setup Rider IDE

As Rider already understands and provides excellent HTML/JS/TypeScript support you'll be immediately productive out-of-the-box,
we can further improve the development experience for Vue.js Apps by adding an empty **vue** dependency to **package.json**:

```json
{
  "devDependencies": {
    "vue": ""
  }
}
```

As this is just a heuristic Rider looks for to enable its Vue support, installing the dependency itself isn't used or required.

Other than that the only plugin we recommend adding is:

<a href="https://plugins.jetbrains.com/plugin/15321-tailwind-css" class="text-2xl flex items-center" style="text-decoration:none">
     <svg class="sm:float-left w-12 h-12" style="margin:0 .5rem 0 0" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="#44a8b3" d="M9 13.7q1.4-5.6 7-5.6c5.6 0 6.3 4.2 9.1 4.9q2.8.7 4.9-2.1q-1.4 5.6-7 5.6c-5.6 0-6.3-4.2-9.1-4.9q-2.8-.7-4.9 2.1Zm-7 8.4q1.4-5.6 7-5.6c5.6 0 6.3 4.2 9.1 4.9q2.8.7 4.9-2.1q-1.4 5.6-7 5.6c-5.6 0-6.3-4.2-9.1-4.9q-2.8-.7-4.9 2.1Z"/></svg>
     <span class="">Tailwind CSS Plugin</span>
</a>

Which provides provides intelli-sense support for [Tailwind CSS](https://tailwindcss.com).

### Start both dotnet and Tailwind

The only additional development workflow requirement to use tailwind is to start it running in the background 
which can be done from a new Terminal:

```bash
$ npm run ui:dev
```

We find `dotnet watch` offers the most productive iterative development workflow for .NET which refreshes on save 
which works great with Tailwind which rewrites your `app.css` on save.

How you want to run them is largely a matter of preference, our personal preference is to run the **dev** and **ui:dev**
npm scripts in your **package.json**:

![](https://raw.githubusercontent.com/ServiceStack/docs/master/docs/images/scripts/dotnet-tailwind.png)


### Rider's Task Runner

Where they will appear in Rider's useful task runner widget where you'll be able to easily, stop and rerun all project tasks:

![](https://github.com/ServiceStack/docs/raw/master/docs/images/spa/rider-run-widget.png)

### Running from the terminal

These GUI tasks are just managing running CLI commands behind-the-scenes, which if you prefer you can use JetBrains
excellent multi-terminal support to run `$ dotnet watch` and `$ npm run ui:dev` from separate or split Terminal windows.

### Deploying to Production

This template also includes the necessary GitHub Actions to deploy this Apps production static assets to GitHub Pages CDN, 
for more info, checkout [GitHub Actions Deployments](deploy).

### Get Started

If you're new to Vue 3 a good place to start is [Vue 3 Composition API](https://vuejs.org/api/composition-api-setup.html).


# Prerendering Razor Pages
Source: https://servicestack.net/posts/prerendering

Prerendering static content is a popular technique used by [JAMStack](https://jamstack.org) Apps to improve the
performance, reliability and scalability of Web Apps that's able to save unnecessary computation at runtime by 
generating static content at deployment which can be optionally hosted from a CDN for even greater performance.

As such we thought it a valuable technique to include in the new [vue-mjs](https://github.com/NetCoreTemplates/vue-mjs) 
template to show how it can be easily achieved within a Razor Pages Application. Since prerendered content is only updated at deployment, 
it's primarily only useful for static content like Blogs which is powered by the static markdown content in 
[_blog/posts](https://github.com/NetCoreTemplates/vue-mjs/tree/prerender/MyApp/wwwroot/_blog/posts) whose content
is prerendered to: 

  - [https://vue-mjs.web-templates.io/blog/](/blog)

### Parsing Markdown Files

All the functionality to load and render Markdown files is maintained in 
[Configure.Markdown.cs](https://github.com/NetCoreTemplates/vue-mjs/blob/prerender/MyApp/Configure.Markdown.cs),
most of which is spent populating the POCO below from the content and frontmatter of each Markdown file:  

```csharp
public class MarkdownFileInfo
{
    public string Path { get; set; } = default!;
    public string? Slug { get; set; }
    public string? FileName { get; set; }
    public string? HtmlFileName { get; set; }
    public string? Title { get; set; }
    public string? Summary { get; set; }
    public string? Splash { get; set; }
    public string? Author { get; set; }
    public List<string> Tags { get; set; } = new();
    public DateTime? Date { get; set; }
    public string? Content { get; set; }
    public string? Preview { get; set; }
    public string? HtmlPage { get; set; }
    public int? WordCount { get; set; }
    public int? LineCount { get; set; }
}
```

Which uses the popular [Markdig](https://github.com/xoofx/markdig) library to parse the frontmatter into a
Dictionary that it populates the POCO with using the built-in [Automapping](https://docs.servicestack.net/auto-mapping):

```csharp
var content = VirtualFiles.GetFile(path).ReadAllText();
var document = Markdown.Parse(content, pipeline);
var block = document
    .Descendants<Markdig.Extensions.Yaml.YamlFrontMatterBlock>()
    .FirstOrDefault();
var doc = block?
    .Lines // StringLineGroup[]
    .Lines // StringLine[]
    .Select(x => $"{x}\n")
    .ToList()
    .Select(x => x.Replace("---", string.Empty))
    .Where(x => !string.IsNullOrWhiteSpace(x))
    .Select(x => KeyValuePairs.Create(x.LeftPart(':').Trim(), x.RightPart(':').Trim()))
    .ToObjectDictionary()
    .ConvertTo<MarkdownFileInfo>();
```

Since this is a [Jekyll inspired blog](https://jekyllrb.com/docs/step-by-step/08-blogging/) it derives the **date** and **slug** for each 
post from its file name which has the nice property of maintaining markdown blog posts in chronological order:

```csharp
doc.Slug = file.Name.RightPart('_').LastLeftPart('.');
doc.HtmlFileName = $"{file.Name.RightPart('_').LastLeftPart('.')}.html";

var datePart = file.Name.LeftPart('_');
if (DateTime.TryParseExact(datePart, "yyyy-MM-dd", CultureInfo.InvariantCulture,
        DateTimeStyles.AdjustToUniversal, out var date))
{
    doc.Date = date;
}
```

The rendering itself is done using Markdig's `HtmlRenderer` which renders the Markdown content into a HTML fragment:  

```csharp
var pipeline = new MarkdownPipelineBuilder()
    .UseYamlFrontMatter()
    .UseAdvancedExtensions()
    .Build();
var writer = new StringWriter();
var renderer = new Markdig.Renderers.HtmlRenderer(writer);
pipeline.Setup(renderer);
//...

renderer.Render(document);
writer.Flush();
doc.Preview = writer.ToString();
```

At this point we've populated Markdown Blog Posts into a POCO which is the data source used to implement all the blog's functionality. 

We can now start prerendering entire HTML Pages by rendering the markdown inside the 
[Post.cshtml](https://github.com/NetCoreTemplates/vue-mjs/blob/prerender/MyApp/Pages/Posts/Post.cshtml) Razor Page by populating its PageModel
from the `MarkdownFileInfo` POCO. It also sets a `Static` flag that tells the Razor Page that this page is being statically rendered so 
it can render the appropriate links.

```csharp
var page = razorPages.GetView("/Pages/Posts/Post.cshtml");
var model = new Pages.Posts.PostModel(this) { Static = true }.Populate(doc);
doc.HtmlPage = RenderToHtml(page.View, model);

public string RenderToHtml(IView? page, PageModel model)
{
    using var ms = MemoryStreamFactory.GetStream();
    razorPages.WriteHtmlAsync(ms, page, model).GetAwaiter().GetResult();
    ms.Position = 0;
    var html = Encoding.UTF8.GetString(ms.ReadFullyAsMemory().Span);
    return html;
}
```

The use of `GetResult()` on an async method isn't ideal, but something we have to live with until there's a better way 
to run async code on Startup.

The actual rendering of the Razor Page is done with ServiceStack's `RazorPagesEngine` feature which sets up the necessary 
Http, View and Page contexts to render Razor Pages, registered in ASP.NET Core's IOC at:

```csharp
.ConfigureServices(services => {
    services.AddSingleton<RazorPagesEngine>();
})
```

The process of saving the prerendered content is then simply a matter of saving the rendered Razor Page at the preferred locations,
done for each post and the [/blog](/blog) index page using the
[Posts/Index.cshtml](https://github.com/NetCoreTemplates/vue-mjs/blob/prerender/MyApp/Pages/Posts/Index.cshtml) Razor Page:

```csharp
foreach (var file in files)
{
    // prerender /blog/{slug}.html
    if (renderTo != null)
    {
        log.InfoFormat("Writing {0}/{1}...", renderTo, doc.HtmlFileName);
        fs.WriteFile($"{renderTo}/{doc.HtmlFileName}", doc.HtmlPage);
    }
}

// prerender /blog/index.html
if (renderTo != null)
{
    log.InfoFormat("Writing {0}/index.html...", renderTo);
    RenderToFile(razorPages.GetView("/Pages/Posts/Index.cshtml").View, 
        new Pages.Posts.IndexModel { Static = true }, $"{renderTo}/index.html");
}
```

### Prerendering Pages Task

Next we need to come up with a solution to run this from the command-line.
[App Tasks](https://docs.servicestack.net/app-tasks) is ideal for this which lets you run one-off tasks within the full context of your App 
but without the overhead of maintaining a separate .exe with duplicated App configuration & logic, instead we can run the .NET App to 
run the specified Tasks then exit before launching its HTTP Server.

To do this we'll register this task with the **prerender** AppTask name:

```csharp
AppTasks.Register("prerender", args => blogPosts.LoadPosts("_blog/posts", renderTo: "blog"));
```

Which we can run now from the command-line with:

```bash
$ dotnet run --AppTasks=prerender
```

To make it more discoverable, this is also registered as an npm script in `package.json`:

```json
{
    "scripts": {
        "prerender": "dotnet run --AppTasks=prerender"
    }
}
```

That can now be run to prerender this blog to `/wwwroot/blog` with: 

```bash
$ npm run prerender
```

### Prerendering at Deployment

To ensure this is always run at deployment it's also added as an MS Build task in 
[MyApp.csproj](https://github.com/NetCoreTemplates/vue-mjs/blob/prerender/MyApp/MyApp.csproj):

```xml
<Target Name="AppTasks" AfterTargets="Build" Condition="$(APP_TASKS) != ''">
    <CallTarget Targets="Prerender" Condition="$(APP_TASKS.Contains('prerender'))" />
</Target>
<Target Name="Prerender">
    <Message Text="Prerender..." />
    <Exec Command="dotnet run --AppTasks=prerender" />
</Target>
```

Configured to run when the .NET App is published in the GitHub Actions deployment task in 
[/.github/workflows/release.yml](https://github.com/NetCoreTemplates/vue-mjs/blob/prerender/.github/workflows/release.yml):

```yaml
 # Publish .NET Project
 - name: Publish dotnet project
   working-directory: ./MyApp
   run: | 
     dotnet publish -c Release /p:APP_TASKS=prerender
```

Where it's able to control which App Tasks are run at deployment. 

### Pretty URLs for static .html pages

A nicety we can add to serving static `.html` pages is giving them [Pretty URLs](https://en.wikipedia.org/wiki/Clean_URL)
by registering the Plugin: 

```csharp
Plugins.Add(new CleanUrlsFeature());
```

Which allows prerendered pages to be accessed with and without its file extension:

 - [vue-mjs.web-templates.io/blog/prerendering](https://vue-mjs.web-templates.io/blog/prerendering)
 - [vue-mjs.web-templates.io/blog/prerendering.html](https://vue-mjs.web-templates.io/blog/prerendering.html)

###


# Develop using Visual Studio
Source: https://servicestack.net/posts/vs

A popular alternative development environment to our preferred [JetBrains Rider](rider) IDE is to use
Visual Studio, the primary issue with this is that VS Code is a better IDE with richer support for JavaScript and npm
projects whilst Visual Studio is a better IDE for C# Projects. 

Essentially this is why we recommend Rider where it's best at both, where both C# and JS/TypeScript projects can 
be developed from within the same solution.

### Developing with just VS Code

<a href="https://visualstudio.microsoft.com/" title="VS Code" class="sm:float-left mr-8">
    <svg class="w-24 h-24" style="margin-top:1rem" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 254"><defs><linearGradient id="logosVisualStudioCode0" x1="50%" x2="50%" y1="0%" y2="100%"><stop offset="0%" stop-color="#FFF"/><stop offset="100%" stop-color="#FFF" stop-opacity="0"/></linearGradient><path id="logosVisualStudioCode1" d="M180.828 252.605a15.872 15.872 0 0 0 12.65-.486l52.501-25.262a15.94 15.94 0 0 0 9.025-14.364V41.197a15.939 15.939 0 0 0-9.025-14.363l-52.5-25.263a15.877 15.877 0 0 0-18.115 3.084L74.857 96.35l-43.78-33.232a10.614 10.614 0 0 0-13.56.603L3.476 76.494c-4.63 4.211-4.635 11.495-.012 15.713l37.967 34.638l-37.967 34.637c-4.623 4.219-4.618 11.502.012 15.714l14.041 12.772a10.614 10.614 0 0 0 13.56.604l43.78-33.233l100.507 91.695a15.853 15.853 0 0 0 5.464 3.571Zm10.464-183.649l-76.262 57.889l76.262 57.888V68.956Z"/></defs><mask id="logosVisualStudioCode2" fill="#fff"><use href="#logosVisualStudioCode1"/></mask><path fill="#0065A9" d="M246.135 26.873L193.593 1.575a15.885 15.885 0 0 0-18.123 3.08L3.466 161.482c-4.626 4.219-4.62 11.502.012 15.714l14.05 12.772a10.625 10.625 0 0 0 13.569.604L238.229 33.436c6.949-5.271 16.93-.315 16.93 8.407v-.61a15.938 15.938 0 0 0-9.024-14.36Z" mask="url(#logosVisualStudioCode2)"/><path fill="#007ACC" d="m246.135 226.816l-52.542 25.298a15.887 15.887 0 0 1-18.123-3.08L3.466 92.207c-4.626-4.218-4.62-11.502.012-15.713l14.05-12.773a10.625 10.625 0 0 1 13.569-.603l207.132 157.135c6.949 5.271 16.93.315 16.93-8.408v.611a15.939 15.939 0 0 1-9.024 14.36Z" mask="url(#logosVisualStudioCode2)"/><path fill="#1F9CF0" d="M193.428 252.134a15.892 15.892 0 0 1-18.125-3.083c5.881 5.88 15.938 1.715 15.938-6.603V11.273c0-8.318-10.057-12.483-15.938-6.602a15.892 15.892 0 0 1 18.125-3.084l52.533 25.263a15.937 15.937 0 0 1 9.03 14.363V212.51c0 6.125-3.51 11.709-9.03 14.363l-52.533 25.262Z" mask="url(#logosVisualStudioCode2)"/><path fill="url(#logosVisualStudioCode0)" fill-opacity=".25" d="M180.828 252.605a15.874 15.874 0 0 0 12.65-.486l52.5-25.263a15.938 15.938 0 0 0 9.026-14.363V41.197a15.939 15.939 0 0 0-9.025-14.363L193.477 1.57a15.877 15.877 0 0 0-18.114 3.084L74.857 96.35l-43.78-33.232a10.614 10.614 0 0 0-13.56.603L3.476 76.494c-4.63 4.211-4.635 11.495-.012 15.713l37.967 34.638l-37.967 34.637c-4.623 4.219-4.618 11.502.012 15.714l14.041 12.772a10.614 10.614 0 0 0 13.56.604l43.78-33.233l100.506 91.695a15.857 15.857 0 0 0 5.465 3.571Zm10.464-183.65l-76.262 57.89l76.262 57.888V68.956Z" mask="url(#logosVisualStudioCode2)"/></svg>
</a>

If you prefer the dev UX of a lightweight text editor or your C# project isn't large, than VS Code on its own
can provide a great development UX which is also what [Vue recommends themselves](https://v3.vuejs.org/api/sfc-tooling.html#ide-support),
to be used together with the [Volar extension](https://marketplace.visualstudio.com/items?itemName=johnsoncodehk.volar).

VSCode's [Integrated Terminal](https://code.visualstudio.com/docs/editor/integrated-terminal) has great multi-terminal 
support you can toggle between the editor and terminal with `Ctrl+` or open a new Terminal Window with
<code>Ctrl+Shift+`</code> to run Tailwind with:

```bash
$ npm run ui:dev
```

Then in a new Terminal Window, start a new watched .NET App with:

```bash
$ dotnet watch
```

With both projects started you can open a browser tab running at `https://localhost:5001` where it 
will automatically reload itself at every `Ctrl+S` save point.

#### Useful VS Code extensions

We recommend these extensions below to enhance the development experience of this template:

 - [Tailwind CSS IntelliSense](https://marketplace.visualstudio.com/items?itemName=bradlc.vscode-tailwindcss) - Add Intellisense for Tailwind classes
 - [es6-string-html](https://marketplace.visualstudio.com/items?itemName=Tobermory.es6-string-html) - Add HTML Syntax Highlighting in string literals 

### Using Visual Studio

<a href="https://code.visualstudio.com/" title="Visual Studio" class="sm:float-left mr-8">
    <svg class="w-24 h-24" style="margin-top:1rem" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256"><defs><linearGradient id="logosVisualStudio0" x1="50%" x2="50%" y1=".002%" y2="100%"><stop offset="0%" stop-color="#FFF"/><stop offset="100%" stop-color="#FFF" stop-opacity="0"/></linearGradient></defs><path fill="#52218A" d="M36.987 200.406a10.667 10.667 0 0 1-11.04 1.734L6.56 194.006A10.667 10.667 0 0 1 0 184.22V70.46a10.667 10.667 0 0 1 6.56-9.787l19.387-8a10.667 10.667 0 0 1 11.04 1.733l4.346 3.6a5.893 5.893 0 0 0-9.333 4.8v129.067a5.893 5.893 0 0 0 9.333 4.8l-4.346 3.733Z"/><path fill="#6C33AF" d="M6.56 194.006A10.667 10.667 0 0 1 0 184.22v-.88a6.16 6.16 0 0 0 10.667 4.133L176 4.673a16 16 0 0 1 18.187-3.093l52.746 25.386A16 16 0 0 1 256 41.393v.613a10.107 10.107 0 0 0-16.507-7.813l-198.16 162.48l-4.346 3.733a10.667 10.667 0 0 1-11.04 1.734L6.56 194.006Z"/><path fill="#854CC7" d="M6.56 60.673A10.667 10.667 0 0 0 0 70.46v.88a6.16 6.16 0 0 1 10.667-4.134L176 250.006a16 16 0 0 0 18.187 3.094l52.746-25.387A16 16 0 0 0 256 213.286v-.613a10.107 10.107 0 0 1-16.507 7.813L41.333 58.006l-4.346-3.733a10.667 10.667 0 0 0-11.04-1.6l-19.387 8Z"/><path fill="#B179F1" d="M194.187 253.1A16 16 0 0 1 176 250.006a9.387 9.387 0 0 0 16-6.64v-232a9.387 9.387 0 0 0-16-6.693a16 16 0 0 1 18.187-3.093l52.746 25.36A16 16 0 0 1 256 41.366v171.947a16 16 0 0 1-9.067 14.427l-52.746 25.36Z"/><path fill="url(#logosVisualStudio0)" fill-opacity=".25" d="M183.707 254.273a16 16 0 0 0 10.48-1.173l52.746-25.36A16 16 0 0 0 256 213.313V41.366a16 16 0 0 0-9.067-14.426L194.187 1.58A16 16 0 0 0 182.24.806A16 16 0 0 0 176 4.673L90.987 98.7L41.333 58.006l-4.346-3.733a10.667 10.667 0 0 0-9.627-2.213a6.8 6.8 0 0 0-1.413.48L6.56 60.673A10.667 10.667 0 0 0 0 69.66v115.36a10.664 10.664 0 0 0 6.56 8.986l19.387 8a6.8 6.8 0 0 0 1.413.48c3.378.882 6.973.056 9.627-2.213l4.346-3.6l49.654-40.693L176 250.006a16 16 0 0 0 7.707 4.267ZM192 73.153l-66.107 54.187L192 181.526V73.153ZM32 90.726l33.093 36.614L32 163.953V90.726Z"/></svg>
</a>

As your C# project grows you'll want to consider running the back-end C# Solution with Visual Studio .NET with its
much improved intelli-sense, navigation, tests runner & debug capabilities. 

As we've never had a satisfactory experience trying develop npm or JS/TypeScript projects with VS.NET, we'd recommend only
using VS.NET for C# and Razor and continuing to use VSCode for everything else. 

If you'd prefer to use Visual Studio for front-end development we recommend moving all JS to external files for a better
Dev UX, e.g:

```html
<script type="module" src="/Pages/SignIn.mjs"></script>
```

### Deploying to Production

This template also includes the necessary GitHub Actions to deploy this Apps production static assets to GitHub Pages CDN,
for more info, checkout [GitHub Actions Deployments](deploy).

### Get Started

If you're new to Vue 3 a good place to start is [Vue 3 Composition API](https://vuejs.org/api/composition-api-setup.html).


# Jamstack Hosting Costs @ $.40 /mo
Source: https://servicestack.net/posts/jamstacks_hosting

<a href="https://jamstack.org">
    <svg class="w-14 h-14 float-left mt-2 mr-4" xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256"><path fill="#F0047F" d="M128 0C57.221 0 0 57.221 0 128c0 70.778 57.221 128 128 128c70.778 0 128-57.222 128-128V0H128Z"/><path fill="#FFF" d="M121.04 134.96v93.312c-49.663-2.837-89.64-42.345-93.215-91.81l-.097-1.502h93.312Zm90.962 0c-2.6 49.664-38.816 89.64-84.159 93.215l-1.377.097V134.96h85.536Zm.112-91.074v85.648h-85.648V43.886h85.648Z"/></svg>
</a>

The modern [jamstack.org](https://jamstack.org) approach for developing websites is primarily concerned with adopting
the architecture yielding the best performance and superior UX by minimizing the time to first byte from serving
pre-built static assets from CDN edge caches.

## Cheaper Hosting

<a href="https://jamstack.org">
    <svg class="w-14 h-14 float-left mt-2 mr-4" xmlns="http://www.w3.org/2000/svg" width="64" height="64" viewBox="0 0 64 64"><path fill="#699635" d="M61 20.9C54.8 15.3 48.6 9.6 42.4 4C33.3 17.8 17.9 30.1 3.3 39c-.4 1.9-.8 3.7-1.3 5.5C8.4 50.3 14.8 56.2 21.2 62C36.3 53.7 52.3 41.9 62 28.5c-.3-2.5-.7-5-1-7.6"/><path fill="#83bf4f" d="M22.4 54.6C16.1 48.8 9.8 43 3.5 37.3c15.7-8.9 28.9-21 38.6-35.3C48.4 7.8 54.7 13.5 61 19.3c-9.7 14.3-22.9 26.4-38.6 35.3"/><g fill="#699635"><path d="M20.8 50.8c-4.2-3.8-8.4-7.7-12.6-11.5c.3-.2.5-.3.8-.5c.8-.5 1-1.4.3-2.1l-.6-.6C21.4 28.3 32.4 18.3 41 6.7l.6.6c.7.6 1.7.5 2.3-.2c.2-.2.3-.5.5-.7c4.2 3.8 8.4 7.7 12.6 11.5c-.2.2-.3.5-.5.7c-.6.8-.5 1.9.2 2.6l.6.6C48.7 33.3 37.7 43.3 25 51.1l-.6-.6c-.7-.6-1.9-.7-2.8-.2c-.3.2-.5.3-.8.5m-9.9-11.4c3.4 3.1 6.8 6.2 10.3 9.4c1.3-.4 2.8-.3 4 .3C36.8 41.7 46.9 32.5 55 21.8c-.7-1.1-.8-2.5-.3-3.7c-3.4-3.1-6.8-6.2-10.3-9.4c-.9.8-2.2.9-3.3.3C33 19.8 22.9 29 11.3 36.4c.6 1 .5 2.2-.4 3"/><path d="M20 37.3c1.1-.4 2.3-.3 3.1.4c-.5.4-1.1.8-1.6 1.2c-.2-.2-.7-.2-.9 0c-.3.2-.4.5-.1.7c.2.2.7.2 1 0c1.2-.8 2.9-.9 3.8 0c.7.7.8 1.7.1 2.5c.2.2.4.3.6.5c-.5.3-1 .7-1.5 1c-.2-.2-.4-.4-.6-.5c-1.1.3-2.3.2-3-.5c.6-.4 1.1-.7 1.7-1.1c.2.2.7.2 1 0c.3-.2.3-.5.1-.7c-.2-.2-.6-.2-.9 0c-.7.5-1.6.7-2.4.6c-.6-.1-1.1-.3-1.5-.6c-.7-.7-.7-1.6-.1-2.4c-.2-.1-.3-.3-.5-.5c.5-.3 1-.7 1.5-1c-.1.1.1.2.2.4m23.2-21.4c.9-.6 2-.6 2.7.1l-1.2 1.5c-.2-.2-.6-.2-.8.1c-.2.3-.2.7 0 .9s.6.2.8-.1c.9-1.1 2.4-1.3 3.3-.5c.7.7.9 1.8.5 2.8c.2.2.4.3.6.5c-.4.5-.7.9-1.1 1.4c-.2-.2-.4-.4-.6-.5c-.9.5-1.9.5-2.6-.2c.4-.5.9-1 1.3-1.5c.2.2.6.2.8-.1s.2-.7 0-.9c-.2-.2-.6-.2-.8.1c-.5.6-1.3 1-2 1c-.5 0-1-.2-1.4-.5c-.7-.7-.9-1.7-.5-2.7c-.2-.1-.3-.3-.5-.5c.4-.4.8-.9 1.1-1.4c0 .2.2.3.4.5m-3 19.6c-3.2 2.9-8.6 2.8-11.7 0c-3.1-2.9-2.9-7.4.3-10.3c3.2-2.9 8.2-3.2 11.3-.3c3.2 2.8 3.3 7.7.1 10.6"/></g><path fill="#ffdd7d" d="m21.8 24.5l18.9 17.3c2.2-1.8 4.2-3.7 6.2-5.7L28.1 18.8c-2 1.9-4.1 3.8-6.3 5.7"/><path fill="#dbb471" d="M40.8 49.4c2.1-1.6 4.2-3.3 6.2-5V36c-2 2-4.1 3.9-6.2 5.7v7.7"/><path id="emojioneMoneyWithWings0" fill="#8d9998" d="M12.1 24.7c1.1-.4 2-1 2.9-1.7c.9-.7 1.7-1.4 2.6-2.2c.9-.7 1.8-1.5 2.8-2c1-.6 2.3-.9 3.4-.7c-1.1.2-2.1.7-3 1.4c-.9.6-1.7 1.4-2.6 2.1c-.9.7-1.8 1.5-2.8 2.1c-1 .6-2.2 1-3.3 1"/><path fill="#e8e8e8" d="M39.8 33.1s4.9 1.4 16.3-1.6s6 29.8-14.8 29.8c-2.3 0-2.4-2.9 4-8.1c0 0-9 2.1-5.1-4.6c0 0-6.4 1.1-3.5-4c0 0 1.3-3.8.1-8.1c.1 0 .8-2.2 3-3.4"/><g fill="#d1d1d1"><path d="M61.5 34.7c.2 8.8-6.8 23.4-20.7 23.4h-.2c-1.3 2.1-.7 3.3.7 3.3c15.8-.1 22.7-18.9 20.2-26.7"/><path d="M54.3 39.5c-3 10.8-15 11.7-15 11.7c-.5 2.3.5 3.3 1.8 3.3c4.4.1 12.6-7.2 13.2-15m-8.4 0c-7.1 10-9.6 6.3-9.6 6.3c-1.8 4 2.9 4 3.7 3.1c2.3-2.5 6-7.7 5.9-9.4"/></g><path fill="#8d9998" d="M40.5 58.4c1.2-1.6 2.7-2.9 4.2-4.2c1.5-1.3 3-2.6 4.5-4c1.4-1.4 2.8-3 3.8-4.8c1-1.8 1.5-3.9 1.3-5.9c-.2 2-.9 3.9-2 5.5s-2.4 3.1-3.8 4.5s-2.9 2.7-4.3 4.2c-1.5 1.3-2.8 2.9-3.7 4.7m-1.2-7.2c.4-1.1 1-2 1.7-2.9c.7-.9 1.4-1.7 2.2-2.6c.7-.9 1.5-1.8 2-2.8c.6-1 .9-2.3.7-3.4c-.2 1.1-.7 2.1-1.4 3c-.6.9-1.4 1.7-2.1 2.6c-.7.9-1.5 1.8-2.1 2.8c-.6 1-1 2.2-1 3.3"/><path fill="#e8e8e8" d="M30.2 24.2s-1.4-4.9 1.6-16.3S2 1.9 2 22.7c0 2.3 2.9 2.4 8.1-4c0 0-2.1 9 4.6 5.1c0 0-1.1 6.4 4 3.5c0 0 3.8-1.3 8.1-.1c0-.1 2.2-.8 3.4-3"/><g fill="#d1d1d1"><path d="M28.6 2.5C19.8 2.3 5.2 9.3 5.2 23.2v.2c-2.1 1.2-3.2.7-3.2-.7C2 6.9 20.8 0 28.6 2.5"/><path d="M23.8 9.7c-10.8 3-11.7 15-11.7 15c-2.3.5-3.3-.5-3.3-1.8c-.1-4.4 7.2-12.6 15-13.2m0 8.4c-10 7.1-6.3 9.6-6.3 9.6c-4 1.8-4-2.9-3.1-3.7c2.5-2.3 7.7-6 9.4-5.9"/></g><path fill="#8d9998" d="M4.9 23.5c1.6-1.2 2.9-2.7 4.2-4.2c1.3-1.5 2.6-3 4-4.5c1.4-1.4 3-2.8 4.8-3.8c1.8-1 3.9-1.5 5.9-1.3c-2 .2-3.9.9-5.5 2s-3.1 2.4-4.5 3.8s-2.7 2.9-4.2 4.3c-1.4 1.5-2.9 2.8-4.7 3.7"/><use href="#emojioneMoneyWithWings0"/></svg>
</a>

A consequence of designing your UI decoupled from your back-end server is that it also becomes considerably
cheaper to host as its static files can be hosted by any web server and is a task highly optimized by CDNs
who are able to provide generous free & low cost hosting options.

##  [/ui](https://github.com/NetCoreTemplates/vue-ssg/tree/main/ui)

This template takes advantage of its decoupled architecture and uses [GitHub Actions to deploy](/posts/deploy)
a copy of its static UI generated assets and hosted on:

### GitHub Pages CDN

### [*.jamstacks.net](https://jamstacks.net)

This is an optional deployment step which publishes a copy of your .NET App's `/wwwroot` folder to this templates
[gh-pages]https://github.com/NetCoreTemplates/vue-ssg/tree/gh-pages) branch where it's automatically served from
[GitHub Pages CDN](https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages) at **no cost**.

It's an optional but recommended optimization as it allows the initial download from your website to be served
directly from CDN edge caches.

## [/api](https://github.com/NetCoreTemplates/vue-ssg/tree/main/api)

The .NET 6 `/api` backend server is required for this App's dynamic functions including the Hello API on the home page
and its [built-in Authentication](https://docs.servicestack.net/auth).

The C# project still contains the complete App and can be hosted independently with the entire App served
directly from its deployed ASP.NET Core server at:

### Digital Ocean

### [vue-ssg.jamstacks.net](https://vue-ssg.jamstacks.net)

But when accessed from the CDN [vue-ssg.jamstacks.net](https://vue-ssg.jamstacks.net) that contains a
copy of its static `/wwwroot` UI assets, only its back-end JSON APIs are used to power its dynamic features.

## Total Cost

<a href="https://www.digitalocean.com/pricing">
    <svg class="w-24 h-24 float-left mt-0 mr-8" xmlns="http://www.w3.org/2000/svg" width="256" height="192" viewBox="0 0 256 192"><path fill="#0080FF" d="M127.806 103.432v24.705c41.874 0 74.478-40.453 60.78-84.332C182.6 24.63 167.363 9.393 148.188 3.407c-43.88-13.698-84.333 18.906-84.333 60.78h24.762c.003 0 .006-.006.006-.006c.004-26.28 26.01-46.596 53.639-36.57c10.236 3.714 18.4 11.877 22.117 22.112c10.027 27.612-10.26 53.609-36.516 53.646V78.744l-.004-.002h-24.686c-.002 0-.004.004-.004.004v24.686h24.637Zm-24.642 18.974H84.197l-.004-.005v-18.969h18.976v18.97s-.002.004-.005.004Zm-18.958-18.974h-15.9c-.007 0-.012-.004-.012-.004V87.535s.005-.012.012-.012h15.888c.007 0 .012.005.012.005v15.904Zm-62.11 49.766c-3.219-2.234-7.225-3.366-11.906-3.366H0v32.329h10.19c4.67 0 8.677-1.195 11.91-3.55c1.76-1.249 3.142-2.994 4.11-5.184c.962-2.18 1.45-4.75 1.45-7.64c0-2.857-.488-5.395-1.452-7.543c-.966-2.16-2.35-3.857-4.112-5.046Zm-16.144 2.098H9.15c3.559 0 6.488.7 8.71 2.083c2.465 1.487 3.716 4.315 3.716 8.407c0 4.215-1.254 7.171-3.726 8.79h-.002c-2.127 1.406-5.039 2.12-8.654 2.12H5.952v-21.4Zm25.881 26.865h5.73v-22.77h-5.73v22.77Zm2.92-32.727c-.98 0-1.823.347-2.503 1.027c-.687.655-1.036 1.486-1.036 2.468c0 .98.347 1.823 1.032 2.507c.684.686 1.528 1.033 2.507 1.033c.979 0 1.822-.347 2.508-1.033c.686-.685 1.032-1.529 1.032-2.507c0-.981-.348-1.811-1.032-2.463a3.43 3.43 0 0 0-2.508-1.032Zm23.618 11.86c-1.728-1.538-3.657-2.434-5.743-2.434c-3.163 0-5.792 1.094-7.813 3.25c-2.046 2.138-3.085 4.89-3.085 8.18c0 3.213 1.022 5.956 3.042 8.156c2.035 2.128 4.679 3.206 7.856 3.206c2.209 0 4.108-.615 5.655-1.83v.525c0 1.889-.506 3.359-1.503 4.368c-.997 1.008-2.379 1.519-4.107 1.519c-2.645 0-4.303-1.04-6.321-3.767l-3.902 3.749l.105.148c.843 1.182 2.134 2.34 3.838 3.44c1.704 1.095 3.846 1.652 6.368 1.652c3.402 0 6.153-1.049 8.18-3.116c2.037-2.079 3.07-4.862 3.07-8.27v-20.68h-5.64v1.903Zm-1.503 13.447c-1.001 1.138-2.287 1.689-3.93 1.689c-1.643 0-2.922-.551-3.909-1.687c-.981-1.13-1.48-2.63-1.48-4.454c0-1.854.499-3.371 1.481-4.51c.973-1.126 2.288-1.698 3.908-1.698c1.641 0 2.928.556 3.93 1.701c.997 1.138 1.503 2.654 1.503 4.507c0 1.824-.506 3.323-1.503 4.452Zm12.047 7.42h5.73v-22.77h-5.73v22.77Zm2.92-32.727c-.979 0-1.823.347-2.503 1.027c-.687.655-1.036 1.486-1.036 2.468c0 .98.347 1.823 1.032 2.507c.684.686 1.528 1.033 2.508 1.033a3.43 3.43 0 0 0 2.508-1.033c.685-.685 1.032-1.529 1.032-2.507c0-.981-.35-1.811-1.032-2.463a3.433 3.433 0 0 0-2.508-1.032Zm15.369 3.805h-5.64v6.152h-3.277v5.222h3.276v9.458c0 2.96.591 5.078 1.758 6.294c1.17 1.222 3.247 1.841 6.174 1.841c.931 0 1.868-.031 2.784-.09l.258-.018v-5.218l-1.964.103c-1.365 0-2.275-.24-2.708-.711c-.44-.48-.661-1.494-.661-3.013v-8.646h5.333v-5.222h-5.333v-6.152Zm24.998 7.855c-1.737-1.482-4.137-2.234-7.134-2.234c-1.907 0-3.69.418-5.302 1.236c-1.492.757-2.957 2.01-3.888 3.65l.058.07l3.669 3.512c1.511-2.41 3.192-3.247 5.42-3.247c1.197 0 2.19.322 2.954.955c.76.629 1.13 1.429 1.13 2.442v1.106a14.494 14.494 0 0 0-4.217-.65c-2.852 0-5.172.67-6.895 1.993c-1.746 1.34-2.63 3.255-2.63 5.696c0 2.139.745 3.877 2.222 5.169c1.49 1.246 3.35 1.879 5.533 1.879c2.181 0 4.222-.88 6.075-2.385v1.875h5.641v-14.638c0-2.771-.887-4.934-2.636-6.429Zm-10.173 12.366c.65-.453 1.568-.682 2.73-.682c1.379 0 2.841.277 4.35.824v2.234c-1.246 1.157-2.91 1.745-4.947 1.745c-.992 0-1.763-.22-2.293-.657c-.52-.426-.773-.969-.773-1.656c0-.781.305-1.373.933-1.808Zm17.374 8.701h5.73v-32.33h-5.73v32.33Zm26.52.487c-9.19 0-16.667-7.476-16.667-16.666c0-9.19 7.476-16.667 16.666-16.667c9.19 0 16.666 7.477 16.666 16.667c0 9.19-7.476 16.666-16.666 16.666Zm0-27.46c-5.952 0-10.793 4.842-10.793 10.794c0 5.951 4.841 10.792 10.792 10.792s10.793-4.84 10.793-10.792s-4.842-10.793-10.793-10.793Zm37.096 18.831c-1.025 1.149-2.073 2.145-2.881 2.662v.001c-.792.508-1.792.767-2.971.767c-1.689 0-3.046-.615-4.152-1.882c-1.1-1.26-1.657-2.878-1.657-4.814c0-1.934.55-3.55 1.636-4.801c1.091-1.26 2.441-1.87 4.128-1.87c1.846 0 3.792 1.147 5.459 3.119l3.787-3.632c-2.47-3.214-5.619-4.709-9.378-4.709c-3.146 0-5.873 1.146-8.105 3.405c-2.22 2.242-3.346 5.099-3.346 8.488c0 3.39 1.125 6.254 3.343 8.512c2.22 2.26 4.948 3.406 8.108 3.406c4.148 0 7.497-1.791 9.754-5.072l-3.725-3.58Zm23.516-11.447c-.812-1.132-1.918-2.043-3.29-2.71c-1.366-.665-2.96-1.002-4.734-1.002c-3.198 0-5.816 1.18-7.78 3.51c-1.908 2.31-2.874 5.183-2.874 8.539c0 3.45 1.06 6.307 3.152 8.489c2.08 2.171 4.865 3.273 8.276 3.273c3.864 0 7.035-1.563 9.424-4.645l.129-.166l-3.738-3.592c-.346.418-.837.892-1.286 1.314c-.565.534-1.097.947-1.665 1.23c-.857.426-1.817.637-2.892.637c-1.587 0-2.902-.467-3.907-1.388c-.94-.862-1.49-2.024-1.636-3.459h15.18l.052-2.091c0-1.48-.202-2.906-.599-4.24a12.37 12.37 0 0 0-1.812-3.699Zm-12.523 5.361c.276-1.09.778-1.996 1.497-2.7c.776-.765 1.787-1.152 3.003-1.152c1.39 0 2.465.395 3.196 1.177c.678.722 1.06 1.621 1.14 2.675h-8.836Zm34.528-6.839c-1.737-1.482-4.138-2.234-7.134-2.234c-1.907 0-3.69.418-5.302 1.236c-1.493.757-2.958 2.01-3.888 3.65l.058.07l3.669 3.512c1.511-2.41 3.192-3.247 5.42-3.247c1.197 0 2.19.322 2.954.955c.76.629 1.13 1.429 1.13 2.442v1.106a14.5 14.5 0 0 0-4.218-.65c-2.85 0-5.172.67-6.895 1.993c-1.745 1.34-2.63 3.255-2.63 5.696c0 2.139.746 3.877 2.223 5.169c1.489 1.246 3.35 1.879 5.532 1.879s4.223-.88 6.075-2.385v1.875h5.642v-14.638c0-2.771-.887-4.934-2.636-6.429Zm-10.173 12.366c.65-.453 1.567-.682 2.73-.682c1.379 0 2.841.277 4.35.824v2.234c-1.247 1.157-2.91 1.745-4.948 1.745c-.991 0-1.762-.22-2.292-.657c-.52-.426-.773-.969-.773-1.656c0-.781.305-1.373.933-1.808Zm34.851-11.919c-1.596-1.778-3.838-2.681-6.667-2.681c-2.273 0-4.119.653-5.5 1.94v-1.41h-5.619v22.771h5.73v-12.558c0-1.725.41-3.095 1.22-4.073c.808-.974 1.918-1.449 3.395-1.449c1.298 0 2.281.424 3.007 1.295c.729.875 1.098 2.08 1.098 3.586v13.199h5.73v-13.199c0-3.154-.805-5.65-2.394-7.42Z"/></svg>
</a>

Since hosting on GitHub Pages CDN is free, the only cost is for hosting this App's .NET Server which is being hosted
from a basic [$10 /mo](https://www.digitalocean.com/pricing) droplet which is currently hosting **25** .NET Docker
Apps and demos of [starting project templates](https://servicestack.net/start) which works out to be just **$0.40 /mo**!

## Jamstack Benefits

Jamstack is quickly becoming the preferred architecture for the development of modern web apps with
[benefits](https://jamstack.org/why-jamstack/) that extend beyond performance to improved:

- **Security** from a reduced attack surface from hosting read-only static resources and requiring fewer App Servers
- **Scale** with non-essential load removed from App Servers to CDN's architecture capable of incredible scale & load capacity
- **Maintainability** resulting from reduced hosting complexity and the clean decoupling of UI and server logic
- **Portability** with your static UI assets being easily capable from being deployed and generically hosted from any CDN or web server
- **Developer Experience** with the major JavaScript frameworks at the forefront of amazing DX are embracing Jamstack in their dev model, libraries & tooling

Best of all the Jamstack approach fits perfectly with ServiceStack's recommended
[API First Development](https://docs.servicestack.net/api-first-development) model which encourages development of
reusable message-based APIs where the same System APIs can be reused from all Web, Mobile & Desktop Apps
from multiple HTTP, MQ or gRPC endpoints.


# Deployment with GitHub Actions
Source: https://servicestack.net/posts/deploy

# ServiceStack GitHub Action Deployments

The [release.yml](https://github.com/NetCoreTemplates/razor/blob/main/.github/workflows/release.yml) 
in this template enables GitHub Actions CI deployment to a dedicated server with SSH access.

## Overview
`release.yml` is designed to work with a ServiceStack app deploying directly to a single server via SSH. A docker image is built and stored on GitHub's `ghcr.io` docker registry when a GitHub Release is created.

GitHub Actions specified in `release.yml` then copy files remotely via scp and use `docker-compose` to run the app remotely via SSH.

## What's the process of `release.yml`?

![](https://raw.githubusercontent.com/ServiceStack/docs/master/docs/images/mix/release-ghr-vanilla-diagram.png)

## Deployment server setup

To get this working, a server needs to be setup with the following:

- SSH access
- docker
- docker-compose
- ports 443 and 80 for web access of your hosted application

This can be your own server or any cloud hosted server like Digital Ocean, AWS, Azure etc. We use [Hetzner Cloud](http://cloud.hetzner.com/)
to deploy all ServiceStack's [GitHub Project Templates]( https://github.com/NetCoreTemplates/) as it was the 
[best value US cloud provider](https://servicestack.net/blog/finding-best-us-value-cloud-provider) we've found.

When setting up your server, you'll want to use a dedicated SSH key for access to be used by GitHub Actions. GitHub Actions will need the *private* SSH key within a GitHub Secret to authenticate. This can be done via ssh-keygen and copying the public key to the authorized clients on the server.

To let your server handle multiple ServiceStack applications and automate the generation and management of TLS certificates, an additional docker-compose file is provided in this template, `nginx-proxy-compose.yml`. This docker-compose file is ready to run and can be copied to the deployment server.

For example, once copied to remote `~/nginx-proxy-compose.yml`, the following command can be run on the remote server.

```
docker-compose -f ~/nginx-proxy-compose.yml up -d
```

This will run an nginx reverse proxy along with a companion container that will watch for additional containers in the same docker network and attempt to initialize them with valid TLS certificates.

### GitHub Actions secrets

The `release.yml` uses the following secrets.

| Required Secrets | Description |
| -- | -- |
| `DEPLOY_HOST` | Hostname used to SSH deploy .NET App to, this can either be an IP address or subdomain with A record pointing to the server |
| `DEPLOY_USERNAME` | Username to log in with via SSH e.g, **ubuntu**, **ec2-user**, **root** |
| `DEPLOY_KEY` | SSH private key used to remotely access deploy .NET App |
| `LETSENCRYPT_EMAIL` | Email required for Let's Encrypt automated TLS certificates |

These secrets can use the [GitHub CLI](https://cli.github.com/manual/gh_secret_set) for ease of creation. Eg, using the GitHub CLI the following can be set.

```bash
gh secret set DEPLOY_HOST -b"<DEPLOY_HOST>"
gh secret set DEPLOY_USERNAME -b"<DEPLOY_USERNAME>"
gh secret set DEPLOY_KEY < key.pem # DEPLOY_KEY
gh secret set LETSENCRYPT_EMAIL -b"<LETSENCRYPT_EMAIL>"
```

These secrets are used to populate variables within GitHub Actions and other configuration files.

## Deployments

A published version of your .NET App created with the standard dotnet publish tool:

```yaml
dotnet publish -c Release
```

is used to build a production build of your .NET App inside the standard `Dockerfile` for dockerizing .NET Applications.

Additional custom deployment tasks can be added to your project's package.json **postinstall** script which also gets run at deployment. 

If preferred additional MS Build tasks can be run by passing in custom parameters in the publish command, e.g:

```yaml
dotnet publish -c Release /p:APP_TASKS=prerender
```

Which your `MyApp.csproj` can detect with a target that checks for it:

```xml
<!-- Prerender tasks run in release.yml -->
<Target Name="AppTasks" AfterTargets="Build" Condition="$(APP_TASKS) != ''">
    <CallTarget Targets="Prerender" Condition="$(APP_TASKS.Contains('prerender'))" />
</Target>
<Target Name="Prerender">
    <Message Text="Prerender..." />
</Target>
```

## Pushing updates and rollbacks

By default, deployments occur on commit to your main branch. A new Docker image for your ServiceStack API is produced, pushed to GHCR.io and hosted on your Linux server with Docker Compose.

The template also will run the release process on the creation of a GitHub Release making it easier to switch to manual production releases.

Additionally, the `release.yml` workflow can be run manually specifying a version. This enables production rollbacks based on previously tagged releases.
A release must have already been created for the rollback build to work, it doesn't create a new Docker build based on previous code state, only redeploys as existing Docker image.


# In pursuit of the best value US cloud provider
Source: https://servicestack.net/posts/hetzner-cloud

At <a href="/">ServiceStack</a>, we have been using AWS for hosting for over 10 years. It has served us well, but it suffers from complex pricing and possibility of bill shock due to its fractured pay-as-you-go design.

Thankfully, more and more companies are providing simpler offerings for hosting needs, and AWS themselves launched [Lightsail](https://aws.amazon.com/lightsail) as their answer to market demands for simple hosting options that package everything you need for basic hosting.

These simpler hosting options tend to bundle several things together as one fixed monthly price. A VM with a specific compute and memory allocation, as well as data transfer, and storage.

## Looking at different US offerings

Something we wanted to do was to host our [live demo applications](https://github.com/ServiceStackApps/LiveDemos) on a US based host. We were using [Hetzner dedicated servers](https://www.hetzner.com/dedicated-rootserver) in the past for non-latency sensitive use cases like our build server and [Gist.Cafe (our interactive playground for multiple platforms)](https://gist.cafe) but we also wanted our demo applications to be snappy for US users.

[DigitalOcean](https://www.digitalocean.com/pricing) provides ["Droplets"](https://www.digitalocean.com/pricing/droplets) with this fixed pricing model with a nice and simple interface. Their pricing was quite good and we realized we could run all 20+ of our demo applications on a single droplet for $40/month.

For deployment, [we also like to keep things as simple as we can, whilst keeping portability](https://docs.servicestack.net/do-github-action-mix-deployment). Since all our projects are public and on GitHub, we use [GitHub Actions](https://docs.servicestack.net/do-github-action-mix-deployment#github-repository-setup) heavily along with a pattern that deploys our applications using Docker Compose via SSH.
Each application runs in its own container behind an [NGINX proxy](https://docs.servicestack.net/do-github-action-mix-deployment#get-nginx-reverse-proxy-and-letsencrypt-companion-running) with a side car that handles renewing LetsEncrypt certificates. Below is an example of this pattern with Blazor and Litestream.

:::youtube fY50dWszpw4
Using Blazor WASM with Litestream
:::

A nice side effect of this approach is moving servers is relatively painless. We change the DNS entry for the application to point to our new server, update the GitHub Action Secrets if needed and run our Release workflow.

A minute or so later, the application is back running again. Since their were 20+ of these repositories we took advantage of the [GitHub Organization Secrets](https://cli.github.com/manual/gh_secret_set) so we only needed to update values in one place, and [running the workflows again](https://cli.github.com/manual/gh_workflow_run) can also be done programmatically through the GitHub CLI.

## DigitalOcean Price Increase

In June of 2022, we got a notification that [prices for droplets would be increasing](https://www.digitalocean.com/try/new-pricing), and for our droplet it would be going from **$40 to $48**. While this is a small amount of money, it prompted us to have a wider look into this market.

Something we try to do at ServiceStack is to not only provide a comprehensive .NET Framework for building API first systems, but also seek out great value hosting options we can recommend in this ever change space which we're happy to share, like this blog post, that might be useful to our users and others.

Not everyone builds massively distributed systems, and as hardware performance increases, and platforms like [.NET are becoming even more optimized](https://devblogs.microsoft.com/dotnet/performance-improvements-in-aspnet-core-6), a setup with just a server or two can manage larger loads and use cases.

Our research and evaluations ended up right back at [Hetzner but this time with their Cloud offering](https://www.hetzner.com/cloud). For less than **$15 USD** per month, you can get a **4 vCPU, 8GB RAM, 160GB storage and 20TB** of data transfer **hosted in the US**.

We found this was by far the cheapest offering for a simple fixed monthly hosting, and looked to compare how well it performed against the more traditional cloud hosting setups.

## Litestream and SQLite

Our demo applications use [SQLite](https://www.sqlite.org) as a simple way to host the database storage and application together, taking advantage of SQLite's embedded nature.
We were also testing out [Litestream](https://litestream.io) as a possible solution to the lack of data backups and safety when using SQLite for more production like workloads.

<div class="mx-auto mt-4 mb-4">
  <a href="https://litestream.io">
      <div class="inline-flex justify-center w-full">
        <img src="/img/posts/hetzner-cloud/litestream.svg" alt="">
      </div>
      <div class="text-gray-500 text-center">litestream.io</div>
  </a>
</div>

Litestream runs as a separate process and watches your SQLite file for changes and replicates them to storage options like AWS S3, Azure Blob storage and SFTP.
[We created several templates to make this easier](https://docs.servicestack.net/ormlite/litestream) and provide a way to bake in automated disaster recovery using Litestream when used with GitHub Actions and our SSH with Docker Compose deployment.

With some basic load testing, we noticed that SQLite performed pretty well without any effort on our part, and decided we should see how this compares to the commonly suggested hosting patterns provided by the large cloud providers of AWS and Azure.

We used the recommended "Production" setups provided by AWS RDS and Azure SQL Database wizards along with 2 vCPU application server to provide the basis on our comparison.
The reason we chose to use the suggested defaults from these providers was to illustrate the power of defaults when offered by market leaders. When compared to a simple SQLite setup, and providers that offer fixed monthly pricing like Hetzner and DigitalOcean, which is often enough to small companies selling Business to Business (B2B) solutions, AWS and Azure recommended "Production" environments can look extremely over priced.

One of the main reasons managed database solutions are chosen is the fact that they take care of automated backups and restore if things go wrong. There are other nice features that definitely have a lot of value, but managed disaster recovery is probably the most commonly cited one I've come across for why services like RDS are chosen during early development.

Litestream provides this kind of data safety and disaster recovery functionality by targeting cost effective and robust storage solutions like AWS S3 and other cloud provided object stores, and making the backup process close to real-time, and accessible via their CLI.
And the embedded nature of SQLite removes the uncertainty of the process of upgrading your database.

## The Test

To get a clearer idea how each of these hosting options perform with a fairly modest workload, we used a [Gatling](https://gatling.io) test to simulate a user logging into our sample Bookings application, browsing around and creating a booking.

These series of steps had 2 write requests and 8 read, separated by 2 seconds per step. We then setup a Gatling simulation that ramped up adding new users to our system from 5 per second to 15 per second, to add a growing number of users over 10 minutes, then sustained over another 10 minutes.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/aws-gatling-result.png" alt="">
    </div>
<div class="text-gray-500 text-center">AWS Gatling Result.</div>
</div>

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/azure-gatling-result.png" alt="">
    </div>
<div class="text-gray-500 text-center">Azure Gatling Result.</div>
</div>

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/hetzner-gatling-result.png" alt="">
    </div>
<div class="text-gray-500 text-center">Hetzner Gatling Result.</div>
</div>

All 3 setups could handle this rate of requests without issue, and though the "Recommended" AWS and Azure setups would have more headroom, the price difference is far too large to ignore, especially as the difference is paid every month.
The requests throughput of that this test illustrated ~100rps can suit many many use cases, and SQLite is [really only limited by its single writer design](https://www.sqlite.org/whentouse.html#:~:text=An%20SQLite%20database%20is%20limited,to%20something%20less%20than%20this.). We did previous tests of upto 250rps on the same Hetzner Cloud instance with SQLite, but this was starting to reach the maximum throughput, again purely to do with the single writer limitation.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/images/litestream/litestream-costs.svg" alt="">
    </div>
<div class="text-gray-500 text-center">Previous test result price comparison without AWS using Provisioned IOPS.</div>
</div>

This level of throughput is enough to service many kinds of businesses with a drastically more simple system to manage, with large cost savings. Also, with the use of an ORM like [OrmLite](https://docs.servicestack.net/ormlite), switching to another database provider can be migrated if and when the traditional offerings like Postgres are needed.

## The Setups
<style>
    table {
        width: 100%;
        margin-top: 4em;
        margin-bottom: 4em;
    }
</style>

The original setup for tests we did in June didn't default to provisioned IOPs for AWS, but when repeating the tests AWS costs blow out due to this feature being enabled by default. 

Without provisioned IOPs, it drops to around **$132/month** as an estimated cost. The **$300/month** default feature for a "Production" database is very hard for AWS to justify, and I think more of a sign of their poor performing GP2 storage option. Although this will only impact very "chatty" types of applications that need higher IOPs throughput, the difference in performance from RDS vs providers like DigitalOcean and Hetzner can be quite stark.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/aws-rds-with-provisioned-iops.png" alt="">
    </div>
<div class="text-gray-500 text-center">AWS RDS now defaults to provisioned IOPs for a Production setup, drastically increasing costs.</div>
</div>

|              | AWS (DB)          | AWS (App) | Azure (DB) | Azure (App) | DigitalOcean | Hetzner Cloud |
|--------------|-------------------|-----------|------------|-------------|--------------|---------------|
| vCPU         | 2                 | 2         | 4          | 2           | 4            | 4             |
| Memory  (GB) | 8                 | 4         | 10         | 8           | 8            | 8             |
| Storage (GB) | 100 (provisioned) | 16        | 32         | 30          | 160          | 160           |
| Cost         | $442              | $34       | $373       | $70         | $48          | $15           |

The above specs were provided as "Production" defaults when using a single database instance. Azure SQL Database defaults to costing $373, during the load test, the database CPU hit ~25%.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/azure-db-cpu-during-test.png" alt="">
    </div>
<div class="text-gray-500 text-center">Azure SQL database without tuning performs poorly for cost, likely due to lack of indexes</div>
</div>


|           | AWS (DB) | AWS (App) | Azure (DB) | Azure (App) | Hetzner Cloud |
|-----------|----------|-----------|------------|-------------|---------------|
| Max CPU % | 8        | 35        | 25         | 45          | 40            |


This is without any tuning on any of the databases, so while you like more performance out of the recommended setups, it is still clear SQLite performs well by default, and it is well worth considering not only Hetzner Cloud for value for money, but if your use can only needs a single host with SQLite.

## Hetzner Cloud

While we were primarily looking for one of the lowest cost options with simplified pricing, Hetzner Cloud pleasantly surprised us with a few features the larger providers could learn from.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/hetzner-cloud-buy.png" alt="">
    </div>
<div class="text-gray-500 text-center">Hetzner Cloud Pricing.</div>
</div>

### Creating a new instance is fast 
Most of the time if will be ready to remote to before you can open your terminal. Not sure if this is due to some kind of pre-creation process on Hetzner part during the creation screen, but everything is very responsive.
In my testing from the time the "Create" button was clicked, my SSH commands would succeed within **20 seconds**.

### Live Graphs
Another part of the responsiveness is their "Live" graphs for monitoring. It is surprisingly low latency and an extremely stark difference between AWS charging extra for "Detailed" monitoring on EC2 instances. The graphs update every 3-5 seconds in the browser and look to be over a few seconds behind real-time.

<div class="mx-auto mt-4 mb-4">
    <div class="inline-flex justify-center w-full">
      <img src="/img/posts/hetzner-cloud/hetzner-cloud-live-graphs.gif" alt="">
    </div>
<div class="text-gray-500 text-center">Live monitoring updates every 3-5 seconds.</div>
</div>

CloudWatch is a major value add for AWS, and Hetzner's offering is very very basic in comparison, but it is nice to see live updating stats right in your web browser, and something hopefully the other providers can also offer in the future.

### Price
This is the biggest draw card by a long way. The AWS and Azure "recommended" setups are extremely expensive for the hardware and performance they offer. Yes they are mature cloud offerings with a large array of features, but their **pricing scales with hardware resources**.
Products like **Provisioned IOPs** are extremely expensive, and when other cloud providers are offering far more performant and competitive storage with their instances, it can feel like AWS is using it's market share and their defaults to upsell very expensive products.

### Transfer costs
It's been long known that one of the ways large cloud providers keep customers in their network is by charging [excessively large and complex data egress costs](https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures). Something attractive about simplified pricing from Hetzner Cloud (and DigitalOcean to a lesser degree) is the included data transfer of 20TB a month.

Not only is AWS data transfer pricing extremely complicated (inter region vs cross region vs CloudFront vs Transit Gateway and so on), but if your application was sending a lot of data to clients, that same **20TB** you get for free with a **$15 server**, would cost **$1,791 just for data** when coming from AWS. Azure pricing also confusing, and in some ways more expensive.

## Defaults are powerful
Both AWS and Azure "recommended" defaults are there not because the software selected (SQL Server and Postgres) need that amount of resources just to operate, but more as an upsell.
Lots of projects and applications absolutely do not need features like "Provisioned IOPs", despite GP2 storage of AWS being incredibly slow.

Performing disk speed check using the Linux utility `fio` an AWS EC2 instance with 100GB GP2 storage can do ~2250 IOPS and 9MB/s read, and ~750 IOPs at 3MB/s write.
In contrast, Digital Ocean $48 instance, this is not even paying the extra $8/month for the faster storage can do 35.2k IOPS at 144MB/s read, and 11.8k IOPS at 48MB/s write.

Hetzner again is the stand out, with the $15 instance tests resulting in 50.8k IOPS at 207MB/s read, and 16.9k IOPS at 69MB/s write.

|               | Read IOPS | Write IOPs | Read MBs  | Write MBs |
|---------------|-----------|------------|-----------|-----------|
| AWS           | 2.3k      | 0.8k       | 9.2 MB/s  | 3.1 MB/s  |
| Azure         | 3.0k      | 1.0k       | 12.5 MB/s | 4.2 MB/s  |
| DigitalOcean  | 35.2k     | 11.8k      | 144 MB/s  | 48.2 MB/s |
| Hetzner Cloud | 50.5k     | 16.9k      | 207 MB/s  | 69.2 MB/s |


All tests used the following `fio` command.

```shell
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test \
--filename=test --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75
```

## SQLite

Part of the resurgence in popularity of using SQLite is not only the simplicity of a single server, but also as hardware is getting faster, issues surrounding limitations of a single writer are becoming less of an issue for a wider number of use cases.

Litestream's elegant solution for streaming backups to cheap replica storage is definitely adding to that popularity as well since it was a sticking point for a lot of use cases that need that simple data redundancy functionality.

Other solutions for Postgres like `pgbackrest` are similar, but the ease of use is another big part of what makes SQLite and Litestream a great combination.
One command to watch and replicate, another to restore, and it runs completely independent of your application using the SQLite file.

## Hetzner Cloud is hard to beat on price

We're going to keep testing Hetzner Cloud with new applications and use cases going into the future. While they are a very new player in the crowded Cloud Provider market, and their offerings are much more limited, the pricing is a breath of fresh air from the large three providers.

More competition in this space is a great thing, and for those that can use solutions like SQLite for their projects, checking out some of the smaller players like DigitalOcean and Hetzner Cloud is well worth your time.
The early signs from Hetzner Cloud is they not only have an amazing value product, but the features they do have improve on the equivalents from likes of AWS and Azure, which is hopefully a sign of things to come from them.


# Real-time search with Typesense
Source: https://servicestack.net/posts/typesense

We have [recently migrated](/blog/jekyll-to-vitepress) the [ServiceStack Docs](https://docs.servicestack.net) website from using Jekyll for static site generation (SSG) to using 
[VitePress](https://vitepress.vuejs.org) which enables us to use Vite with Vue 3 components and have an insanely fast hot reload while we update our documentation.

VitePress is very well suited to documentation sites, and it is one of the primary use cases for VitePress at the time of writing. 
The default theme even has optional [integration with Algolia DocSearch](https://vitepress.vuejs.org/config/algolia-search). 
However, the Algolia DocSeach product didn't seem to offer the service for commercial products even as a paid service and their per request pricing model made it harder to determine what our costs would be in the long run for using their search service for our documentation.

<a class="flex flex-col items-center my-8" href="https://typesense.org"><svg fill="none" height="56" viewBox="0 0 250 56" width="250" xmlns="http://www.w3.org/2000/svg"><g fill="#1035bc"><path d="m15.0736 15.8466c.1105.5522.1657 1.0859.1657 1.6013 0 .4785-.0552.9938-.1657 1.546l-7.01225-.0552v18.5521c0 1.546.71779 2.319 2.15335 2.319h4.1963c.2577.6258.3865 1.2516.3865 1.8773 0 .6258-.0368 1.0123-.1104 1.1595-1.6932.2209-3.4417.3313-5.24538.3313-3.57055 0-5.35583-1.5276-5.35583-4.5828v-19.6564l-3.920246.0552c-.1104293-.5522-.165644-1.0675-.165644-1.546 0-.5154.0552147-1.0491.165644-1.6013l3.920246.0552v-5.7975c0-.99387.14724-1.69325.44172-2.09816.29448-.44172.86503-.66258 1.71165-.66258h1.4908l.33129.33129v8.28225z"/><path d="m41.7915 16.1227-7.5644 25.8957c-1.3988 4.7485-2.8896 8.0982-4.4724 10.0491s-3.9571 2.9264-7.1227 2.9264c-1.6196 0-3.1104-.2393-4.4724-.7178-.1104-1.0307.1841-2.0246.8834-2.9816 1.1411.4049 2.3559.6073 3.6442.6073 1.9509 0 3.4417-.6625 4.4724-1.9877 1.0307-1.3251 1.9693-3.3865 2.816-6.184l.1656-.5522c-.9571-.0736-1.6932-.2945-2.2086-.6626-.4785-.3681-.8834-1.049-1.2147-2.0429l-7.7301-24.2945c1.1411-.4785 1.9509-.7178 2.4295-.7178 1.0675 0 1.7853.6442 2.1534 1.9325l4.3619 13.8589c.1473.4418.9939 3.3129 2.5399 8.6135.0736.2577.2577.3865.5521.3865l6.7362-24.4049c.4786-.1472 1.1043-.2208 1.8773-.2208.8099 0 1.4908.1104 2.043.3313z"/><path d="m52.4009 41.135v10.9325c0 .9938-.1472 1.6932-.4417 2.0981-.2945.4418-.8834.6626-1.7669.6626h-1.4908l-.3312-.3313v-38.4846l.3312-.3313h1.4356c.8835 0 1.4724.2392 1.7669.7178.3313.4417.4969 1.1779.4969 2.2086v.276c2.2086-2.4662 4.8405-3.6994 7.8957-3.6994 3.1289 0 5.4847 1.27 7.0675 3.8099 1.5828 2.503 2.3743 5.9816 2.3743 10.4356 0 2.1717-.2945 4.1226-.8835 5.8527-.5521 1.7301-1.3067 3.2025-2.2638 4.4172-.9202 1.1779-1.9877 2.0981-3.2024 2.7607-1.2148.6258-2.4663.9387-3.7546.9387-2.5399 0-4.951-.7546-7.2332-2.2638zm0-17.9448v14.1902c2.2454 1.6564 4.362 2.4846 6.3497 2.4846 1.9878 0 3.6258-.8834 4.9141-2.6503 1.2884-1.7668 1.9326-4.4356 1.9326-8.0061 0-1.7669-.1657-3.2945-.497-4.5828-.2945-1.3252-.6994-2.4111-1.2147-3.2577-.5153-.8834-1.1227-1.5276-1.8221-1.9325-.6626-.4417-1.3804-.6626-2.1534-.6626-1.4724 0-2.8711.3865-4.1963 1.1595-1.3251.773-2.4294 1.8589-3.3129 3.2577z"/><path d="m97.6973 30.6442h-17.1166c.1841 6.2576 2.5583 9.3865 7.1227 9.3865 2.5031 0 5.1718-.773 8.0061-2.319.8099.7361 1.3068 1.6748 1.4909 2.8159-3.0184 2.0614-6.405 3.092-10.1596 3.092-1.9141 0-3.5521-.3497-4.9141-1.049-1.3619-.7362-2.4846-1.7301-3.3681-2.9816-.8466-1.2884-1.4724-2.7976-1.8773-4.5277-.4049-1.73-.6073-3.6257-.6073-5.6871 0-2.0981.2392-4.0122.7178-5.7423.5153-1.7301 1.2515-3.2209 2.2085-4.4724.9571-1.2515 2.0982-2.227 3.4234-2.9264 1.3619-.6994 2.9079-1.0491 4.638-1.0491 1.6932 0 3.2025.3129 4.5276.9387 1.362.589 2.4847 1.4172 3.3681 2.4847.9203 1.0306 1.6196 2.2822 2.0982 3.7546.4785 1.4355.7178 2.9816.7178 4.638 0 .6626-.0369 1.3067-.1105 1.9325-.0368.589-.092 1.1595-.1656 1.7117zm-17.1166-3.1473h13.2516v-.7178c0-2.5398-.5338-4.5828-1.6013-6.1288s-2.6687-2.319-4.8037-2.319c-2.0981 0-3.7362.8282-4.9141 2.4847-1.1411 1.6564-1.7852 3.8834-1.9325 6.6809z"/><path d="m103.381 41.0245c.036-.8098.257-1.6932.662-2.6503.442-.9938.939-1.7668 1.491-2.319 2.908 1.5828 5.466 2.3743 7.675 2.3743 1.214 0 2.19-.2393 2.926-.7178.773-.4786 1.16-1.1227 1.16-1.9326 0-1.2883-.994-2.319-2.982-3.092l-3.092-1.1595c-4.638-1.6932-6.957-4.3988-6.957-8.1166 0-1.3251.239-2.503.718-3.5337.515-1.0675 1.214-1.9693 2.098-2.7055.92-.773 2.006-1.362 3.258-1.7669 1.251-.4049 2.65-.6074 4.196-.6074.699 0 1.472.0553 2.319.1657.883.1104 1.767.2761 2.65.4969.884.1841 1.73.4049 2.54.6626s1.509.5337 2.098.8282c0 .9203-.184 1.8773-.552 2.8712s-.865 1.73-1.491 2.2086c-2.908-1.2884-5.429-1.9325-7.564-1.9325-.957 0-1.712.2392-2.264.7178-.552.4417-.828 1.0306-.828 1.7668 0 1.1411.92 2.043 2.761 2.7055l3.368 1.2148c2.429.8466 4.233 2.0061 5.411 3.4785s1.767 3.184 1.767 5.135c0 2.6135-.976 4.7116-2.927 6.2944-1.951 1.5461-4.748 2.3191-8.392 2.3191-3.571 0-6.921-.9019-10.049-2.7056z"/><path d="m151.772 31.5828h-15.239c.111 2.0246.571 3.6258 1.38 4.8037.847 1.1411 2.301 1.7117 4.362 1.7117 2.135 0 4.583-.6258 7.344-1.8773 1.067 1.1042 1.748 2.5582 2.043 4.3619-2.945 2.0982-6.479 3.1473-10.601 3.1473-3.902 0-6.865-1.1964-8.89-3.589-1.988-2.4294-2.981-6.0184-2.981-10.7669 0-2.2086.257-4.1963.773-5.9632.515-1.8036 1.269-3.3312 2.263-4.5828.994-1.2883 2.209-2.2822 3.644-2.9816 1.436-.6994 3.074-1.0491 4.915-1.0491 1.877 0 3.533.2945 4.969.8835 1.436.5521 2.65 1.3619 3.644 2.4294.994 1.0307 1.73 2.2638 2.209 3.6994.515 1.4356.773 3 .773 4.6933 0 .9202-.056 1.8036-.166 2.6503-.11.8098-.258 1.6196-.442 2.4294zm-10.656-11.5951c-2.871 0-4.417 2.1718-4.638 6.5154h9.165v-.6626c0-1.7669-.368-3.1841-1.104-4.2515-.736-1.0675-1.877-1.6013-3.423-1.6013z"/><path d="m182.033 24.5153v12.0368c0 2.3559.386 4.1043 1.159 5.2454-1.178 1.0307-2.595 1.5461-4.251 1.5461-1.583 0-2.669-.3497-3.258-1.0491-.589-.7362-.884-1.8773-.884-3.4233v-12.8651c0-1.6564-.202-2.8159-.607-3.4785s-1.159-.9939-2.264-.9939c-1.951 0-3.773.8835-5.466 2.6504v18.773c-.552.1104-1.141.184-1.767.2208-.589.0368-1.196.0552-1.822.0552s-1.251-.0184-1.877-.0552c-.589-.0368-1.16-.1104-1.712-.2208v-27.3313l.331-.3865h2.761c2.062 0 3.35 1.1043 3.865 3.3128 2.687-2.319 5.356-3.4785 8.006-3.4785 2.651 0 4.602.8651 5.853 2.5951 1.288 1.6933 1.933 3.9755 1.933 6.8466z"/><path d="m187.825 41.0245c.036-.8098.257-1.6932.662-2.6503.442-.9938.939-1.7668 1.491-2.319 2.908 1.5828 5.466 2.3743 7.675 2.3743 1.214 0 2.19-.2393 2.926-.7178.773-.4786 1.16-1.1227 1.16-1.9326 0-1.2883-.994-2.319-2.982-3.092l-3.092-1.1595c-4.638-1.6932-6.957-4.3988-6.957-8.1166 0-1.3251.239-2.503.718-3.5337.515-1.0675 1.214-1.9693 2.098-2.7055.92-.773 2.006-1.362 3.258-1.7669 1.251-.4049 2.65-.6074 4.196-.6074.699 0 1.472.0553 2.319.1657.883.1104 1.767.2761 2.65.4969.884.1841 1.73.4049 2.54.6626s1.509.5337 2.098.8282c0 .9203-.184 1.8773-.552 2.8712s-.865 1.73-1.491 2.2086c-2.908-1.2884-5.429-1.9325-7.564-1.9325-.957 0-1.712.2392-2.264.7178-.552.4417-.828 1.0306-.828 1.7668 0 1.1411.92 2.043 2.761 2.7055l3.368 1.2148c2.429.8466 4.233 2.0061 5.411 3.4785s1.767 3.184 1.767 5.135c0 2.6135-.976 4.7116-2.927 6.2944-1.951 1.5461-4.748 2.3191-8.392 2.3191-3.571 0-6.921-.9019-10.049-2.7056z"/><path d="m236.216 31.5828h-15.239c.111 2.0246.571 3.6258 1.38 4.8037.847 1.1411 2.301 1.7117 4.362 1.7117 2.135 0 4.583-.6258 7.344-1.8773 1.067 1.1042 1.748 2.5582 2.043 4.3619-2.945 2.0982-6.479 3.1473-10.601 3.1473-3.902 0-6.865-1.1964-8.89-3.589-1.988-2.4294-2.981-6.0184-2.981-10.7669 0-2.2086.257-4.1963.773-5.9632.515-1.8036 1.269-3.3312 2.263-4.5828.994-1.2883 2.209-2.2822 3.645-2.9816 1.435-.6994 3.073-1.0491 4.914-1.0491 1.877 0 3.533.2945 4.969.8835 1.436.5521 2.65 1.3619 3.644 2.4294.994 1.0307 1.73 2.2638 2.209 3.6994.515 1.4356.773 3 .773 4.6933 0 .9202-.055 1.8036-.166 2.6503-.11.8098-.258 1.6196-.442 2.4294zm-10.656-11.5951c-2.871 0-4.417 2.1718-4.638 6.5154h9.166v-.6626c0-1.7669-.369-3.1841-1.105-4.2515-.736-1.0675-1.877-1.6013-3.423-1.6013z"/><path d="m244.777 50.6871v-50.521455c.552-.1104299 1.178-.165645 1.878-.165645.736 0 1.417.0552151 2.042.165645v50.521455c-.625.1104-1.306.1657-2.042.1657-.7 0-1.326-.0553-1.878-.1657z"/></g></svg></a>

We found [Typesense](https://typesense.org) as an appealing alternative which offers [simple cost-effective cloud hosting](https://cloud.typesense.org) 
but even better, they also have an easy to use open source option for self-hosting or evaluation. 
We were so pleased with its effortless adoption, simplicity-focus and end-user UX that it quickly became our preferred way to 
[navigate our docs](https://docs.servicestack.net). So that more people can find out about Typesense's amazing OSS Search
product we've documented our approach used for creating and deploying an index of our site using GitHub Actions.

Documentation search is a common use case which Typesense caters for with their [typesense-docsearch-scraper](https://github.com/typesense/typesense-docsearch-scraper). This is a utility designed to easily scrape a documentation site and post the results to a Typesense server to create a fast searchable index.

## Self hosting option

Since we already have several AWS instances hosting our example applications, we opted to start with a self hosting on AWS Elastic Container Service (ECS) since Typesense is already packaged into [an easy to use Docker image](https://hub.docker.com/r/typesense/typesense/).

Trying it locally, we used the following commands to spin up a local Typesense server ready to scrape out docs site.

```shell
mkdir /tmp/typesense-data
docker run -p 8108:8108 -v/tmp/data:/data typesense/typesense:0.21.0 \
    --data-dir /data --api-key=<temp-admin-api-key> --enable-cors
```

To check that the server is running, we can open a browser at `/health` and we get back 200 OK with `ok: true`.

The Typesense server has a [REST API](https://typesense.org/docs/0.21.0/api) which can be used to manage the indexes you create. The cloud offering comes with a web dashboard to manage your data which is a definite advantage over the self hosting, but for now we were still trying it out.

## Populating our index

Now that our local server is running, we can scrape our docs site using the [typesense-docsearch-scraper](https://github.com/typesense/typesense-docsearch-scraper). This needs some configuration since the scraper needs to know:

- Where is the Typesense server.
- How to authenticate with the Typesense server.
- Where is the docs website.
- Rules for the scraper to follow extracting information from the docs website.

These [pieces of configuration come from 2 sources](https://github.com/ServiceStack/docs/tree/master/search-server/typesense-scraper). A [`.env` file](https://github.com/ServiceStack/docs/blob/master/search-server/typesense-scraper/typesense-scraper.env) related to the Typesense server information and [a `.json` file](https://github.com/ServiceStack/docs/blob/master/search-server/typesense-scraper/typesense-scraper-config.json) related to what site will be getting scraped.

With our Typesense running locally on port 8108, we configure the .env file with the following information.

```
TYPESENSE_API_KEY=${TYPESENSE_API_KEY}
TYPESENSE_HOST=localhost
TYPESENSE_PORT=8108
TYPESENSE_PROTOCOL=http
```

Next, we have the `.json` config for the scraper. The [typesense-docsearch-scraper gives an example of this config in their repository](https://github.com/typesense/typesense-docsearch-scraper/blob/master/configs/public/typesense_docs.json) for what this config should look like.

Altering the default selectors to match the HTML for our docs site, we ended up with a configuration that looked like this.

```json
{
  "index_name": "typesense_docs",
  "allowed_domains": ["docs.servicestack.net"],
  "start_urls": [
    {
      "url": "https://docs.servicestack.net/"
    }
  ],
  "selectors": {
    "default": {
      "lvl0": ".page h1",
      "lvl1": ".content h2",
      "lvl2": ".content h3",
      "lvl3": ".content h4",
      "lvl4": ".content h5",
      "text": ".content p, .content ul li, .content table tbody tr"
    }
  },
  "scrape_start_urls": false,
  "strip_chars": " .,;:#"
}
```

Now we have both the configuration files ready to use, we can run the scraper itself. The scraper is also available using the docker image `typesense/docsearch-scraper` and we can pass our configuration to this process using the following command.

```shell
docker run -it --env-file typesense-scraper.env \
    -e "CONFIG=$(cat typesense-scraper-config.json | jq -r tostring)" \
    typesense/docsearch-scraper
```

Here we are using `-i` so we can reference our local `--env-file` and use `cat` and `jq` to populate the `CONFIG` environment variable using our `.json` config file.

## Docker networking

Here we run into a bit of a issue, since the scraper itself is running in Docker via WSL, `localhost` isn't resolving to our host machine to find the Typesense server also running in Docker.
Instead we need to point the scraper to the Typesense server using the Docker local IP address space of 172.17.0.0/16 for it to resolve without additional configuration.

We can see in the output of the Typesense server that it is running using `172.17.0.2`. We can swap the `localhost` with this IP address and communication is flowing.

```
DEBUG:typesense.api_call:Making post /collections/typesense_docs_1635392168/documents/import
DEBUG:typesense.api_call:Try 1 to node 172.17.0.2:8108 -- healthy? True
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 172.17.0.2:8108
DEBUG:urllib3.connectionpool:http://172.17.0.2:8108 "POST /collections/typesense_docs_1635392168/documents/import HTTP/1.1" 200 None
DEBUG:typesense.api_call:172.17.0.2:8108 is healthy. Status code: 200
> DocSearch: https://docs.servicestack.net/azure 22 records)
DEBUG:typesense.api_call:Making post /collections/typesense_docs_1635392168/documents/import
DEBUG:typesense.api_call:Try 1 to node 172.17.0.2:8108 -- healthy? True
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 172.17.0.2:8108
DEBUG:urllib3.connectionpool:http://172.17.0.2:8108 "POST /collections/typesense_docs_1635392168/documents/import HTTP/1.1" 200 None
```

The scraper crawls the docs site following all the links in the same domain to get a full picture of all the content of our docs site.
This takes a minute or so, and in the end we can see in the Typesense sever output that we now have "committed_index: 443".

```
_index: 443, applying_index: 0, pending_index: 0, disk_index: 443, pending_queue_size: 0, local_sequence: 44671
I20211028 03:39:40.402626   328 raft_server.h:58] Peer refresh succeeded!
```

## Searching content

So now we have a Typesense server with an index full of content, we want to be able to search it on our docs site.
Querying our index using straight `cURL`, we can see the query itself only needs to known 3 pieces of information at a minimum.

 - Collection name, eg `typesense_docs`
 - Query term, `?q=test`
 - What to query, `&query_by=content`

```shell
curl -H 'x-typesense-api-key: <apikey>' \
    'http://localhost:8108/collections/typesense_docs/documents/search?q=test&query_by=content'
```

The collection name and `query_by` come from how our scraper were configured. The scraper was posting data to the 
`typesense_docs` collection and populating various fields, eg `content`.

Which as it returns JSON can be easily queried in JavaScript using **fetch**:

```js
fetch('http://localhost:8108/collections/typesense_docs/documents/search?q='
    + encodeURIComponent(query) + '&query_by=content', {
    headers: {
        // Search only API key for Typesense.
        'x-typesense-api-key': 'TYPESENSE_SEARCH_ONLY_API_KEY'
    }
})
```

In the above we have also used a different name for the API key token, this is important since the `--api-key` specified to the running Typesense server is the admin API key. You don't want to expose this to a browser client since they will have the ability to create,update and delete your collections or documents.

Instead we want to generate a "Search only" API key that is safe to share on a browser client. This can be done using the Admin API key and the following REST API call to the Typesense server.

```bash
curl 'http://localhost:8108/keys' -X POST \
  -H "X-TYPESENSE-API-KEY: ${TYPESENSE_API_KEY}" \
  -H 'Content-Type: application/json' \
  -d '{"description": "Search only","actions": ["documents:search"],"collections":["*"]}'
```

Now we can share this generated key safely to be used with any of our browser clients.

## Keeping the index updated

Another problem that becomes apparent is that subsequent usages of the scraper increases the size of our index since it currently doesn't detect and update existing documents.
It wasn't clear if this is possible to configure or manage from the current scraper (ideally by using URL paths as the unique identifier), so we needed a way to achieve the following goals.

- Update the search index automatically soon after docs have been changed
- Don't let the index grow too big causing manual intervention
- Have high uptime for our search so users can always search our docs

Typesense server itself performs extremely well, so a full update from the scraper doesn't generate an amount of load that is of much a concern.
This is also partly because the scraper seems to be sequentially crawling pages so it can only generate so many updates on single thread.
However, every additional scrape will use additional disk space and memory for the running server causing us to periodically reset the index and repopulate, causing downtime.

One option is to switch to a new collection everytime we update the docs sites and delete the old collection. This requires additional orchestration between client and the server, and to avoid down time the following order of operations would be needed.

- Docs are updated
- Publish updated docs 
- Create new collection, store new name + old name
- Scrape updated docs
- Update client with new collection name
- Delete old collection

This dance would require multiple commits/actions in GitHub (we use GitHub Actions), and also be time sensitive since it will be non-deterministic as to how long it will take to scrape, update, and deploy our changes.

Additional operational burden is something we want to avoid since it an on going cost on developer time that would otherwise be spent improving ServiceStack offerings for our customers.

## Read-only Docker container

Something to keep in mind when making architecture decisions is looking at the specifics of what is involved when it comes to the *flow of data* of your systems.

You can ask yourself questions like:

- What data is updated
- When/How often is data updated
- Who updates the data

The answers to these questions can lead to choices that can exploit either the frequency, and/or availability of your data to make it easier to manage.
A common example of this is when deciding how to cache information in your systems. 
Some data is write heavy, making it a poor choice for cache while other data might rarely change, be read heavy and the update process might be completely predictable making it a great candidate for caching.

If update frequency of the data is completely in your control and/or deterministic, you have a lot more choices when it comes to how to manage that data.

In the case of Typesense, when it starts up, it reads from its `data` directory from disk to populate the index in memory and since our index is small and only updates when our documentation is updated, we can simplify the management of the index data by **baking it straight into a docker image**.

Making our hosted Typesense server read-only, we can build the index and our own Docker image, with the index data in it, as a part of our CI process.

This has several key advantages.

- Disaster recovery doesn't need any additional data management.
- Shipping an updated index is a normal ECS deployment.
- Zero down time deployments.
- Index is of a fixed size once deployed.

To make things even more simplified, the incremental improvement of our documentation means that the difference between search index between updates is very small.
This means if our search index is updated even a day after the actual documentation, the vast majority of our documentation is still accurately searchable by our users.

Search on our documentation site is a very light workload for Typesense. Running as an ECS service on a 2 vCPU instance, the service struggled to get close to 1% with constant typeahead searching.

![](/img/posts/typesense/typesense-cpu-utilization.png)

And since our docs site index is so small, the memory footprint is also tiny and stable at ~50MB or ~10% of the the service's soft memory limit.

![](/img/posts/typesense/typesense-memory-utilization.png)

This means we will be able to host this using a single EC2 instance among various other or the ServiceStack hosted example applications and use the same [deployment patterns we've shared in our GitHub Actions templates](https://docs.servicestack.net/mix-github-actions-aws-ecs).

[![](https://raw.githubusercontent.com/ServiceStack/docs/master/docs/images/mix/cloudcraft-host-digram-release-ecr-aws.png)](https://docs.servicestack.net/mix-github-actions-aws-ecs)

So while this approach of shipping an index along with the Docker image isn't practical for large or 'living' indexes, many opensource documentation sites would likely be able to reuse this simplified approach.

## GitHub Actions Process

Since the ServiceStack docs site is hosted using GitHub Pages and we already use GitHub Actions to publish updates to our docs, using GitHub Actions was the natural place for this automation.

To create our own Docker image for our search server we need to perform the following tasks on our CI process.

- Run a local Typesense server on the CI via Docker
- Scrape our hosted docs populating the local Typesense server
- Copy the `data` folder of our local Typesense server during `docker build`

The whole process in GitHub Actions looks like this.

```shell
mkdir -p ${GITHUB_WORKSPACE}/typesense-data
cp ./search-server/typesense-server/Dockerfile ${GITHUB_WORKSPACE}/typesense-data/Dockerfile
cp ./search-server/typesense-scraper/typesense-scraper-config.json typesense-scraper-config.json
envsubst < "./search-server/typesense-scraper/typesense-scraper.env" > "typesense-scraper-updated.env"
docker run -d -p 8108:8108 -v ${GITHUB_WORKSPACE}/typesense-data/data:/data \
    typesense/typesense:0.21.0 --data-dir /data --api-key=${TYPESENSE_API_KEY} --enable-cors &
# wait for typesense initialization
sleep 5
docker run -i --env-file typesense-scraper-updated.env \
    -e "CONFIG=$(cat typesense-scraper-config.json | jq -r tostring)" typesense/docsearch-scraper
```

Our `Dockerfile` then takes this data from the `data` folder during build.

```Dockerfile
FROM typesense/typesense:0.21.0

COPY ./data /data
```

One additional problem we had was related to the search only API key generation. As expected when generating API keys, we don't want the process to generate reused API keys, but to avoid needing to update our search client between updates, we actually want to use the same search only API key everytime we generate a new server.

This can be achieved by specifying `value` in the `POST` command sent to the local Typesense server.

```bash
curl 'http://172.17.0.2:8108/keys' -X POST \
  -H "X-TYPESENSE-API-KEY: ${TYPESENSE_API_KEY}" \
  -H 'Content-Type: application/json' \
  -d '{"value":<search-api-key>,"description":"Search only","actions":["documents:search"],"collections":["*"]}'
```

Once our custom Docker image has been built, we deploy it to AWS Elastic Container Repository (ECR), register a new `task-defintion.json` with ECS pointing to our new image, and finally update the running ECS Service to use the new task definition.

To make things more hands off and reduce any possible issues from GitHub Pages CDN caching, updates to our search index are done on a daily basis using GitHub Action `schedule`.
Once a day, the process checks if the latest commit in the repository is less than 1 day old. If it is,we ship an updated search index, otherwise we actually cancel the GitHub Action process early to save on CI minutes.

The whole GitHub Action can be seen in our [ServiceStack/docs repository](https://github.com/ServiceStack/docs/blob/master/.github/workflows/search-index-update.yml) if you are interested or are setting up your own process the same way.

## Search UI Dialog

Now that our docs are indexed the only thing left to do is display the results. We set out to create a comparable UX to
algolia's doc search which we've implemented in custom Vue3 components and have Open sourced in 
[this gist](https://gist.github.com/gistlyn/d215e9ff31abd9adce719a663a4bd8af) in hope it will serve useful in adopting 
typesearch for your own purposes.

As VitePress is a SSG framework we need to wrap them in a [ClientOnly component](https://vitepress.vuejs.org/guide/global-component.html#clientonly)
to ensure they're only rendered on the client:

```html
<ClientOnly>
    <KeyboardEvents @keydown="onKeyDown" />
    <TypeSenseDialog :open="openSearch" @hide="hideSearch" />
</ClientOnly>
```

Where the logic to capture the window global shortcut keys is wrapped in a hidden 
[KeyboardEvents.vue](https://gist.github.com/gistlyn/d215e9ff31abd9adce719a663a4bd8af#file-keyboardevents-vue):

```html
<template>
  <div class="hidden"></div>
</template>
  
<script>
  export default {
    created() {
      const component = this;
      this.handler = function (e) {
        component.$emit('keydown', e);
      }
      window.addEventListener('keydown', this.handler);
    },
    beforeDestroy() {
      window.removeEventListener('keydown', this.handler);
    }
  }
</script>
```

Which is handled in our custom [Layout.vue](https://gist.github.com/gistlyn/d215e9ff31abd9adce719a663a4bd8af#file-layout-vue)
VitePress theme to detect when the `esc` and `/` or `CTRL+K` keys are pressed to hide/open the dialog: 

```ts
const onKeyDown = (e:KeyboardEvent) => {
    if (e.code === 'Escape') {
        hideSearch();
    }
    else if ((e.target as HTMLElement).tagName != 'INPUT') {
        if (e.code == 'Slash' || (e.ctrlKey && e.code == 'KeyK')) {
            showSearch();
            e.preventDefault();
        }
    }
};
```

The actual search dialog component is encapsulated in 
[TypeSenseDialog.vue](https://gist.github.com/gistlyn/d215e9ff31abd9adce719a663a4bd8af#file-typesensedialog-vue)
(utilizing tailwind classes, scoped styles and inline SVGs so is easily portable), 
the integral part being the API search query to our typesense instance:

```js
fetch('https://search.docs.servicestack.net/collections/typesense_docs/documents/search?q='
  + encodeURIComponent(query.value)
  + '&query_by=content,hierarchy.lvl0,hierarchy.lvl1,hierarchy.lvl2,hierarchy.lvl3&group_by=hierarchy.lvl0', {
    headers: {
      // Search only API key for Typesense.
      'x-typesense-api-key': 'TYPESENSE_SEARCH_ONLY_API_KEY'
    }
})
```

Which instructs Typesense to search through each documents content and h1-3 headings, grouping results by its page title.
Refer to the [Typesense API Search Reference](https://typesense.org/docs/0.21.0/api/documents.html#search) to learn how
to further fine-tune search results for your use-case.

## Search Results

![](/img/posts/typesense/typesense-dart.gif)

The results are **excellent**, [see for yourself](https://docs.servicestack.net) by using the search at the top right or using Ctrl+K shortcut key on our docs site. 
It handles typos really well, it is very quick and has become the fastest way to navigate our extensive documentation.

We have been super impressed with the search experience that Typesense enabled, the engineers behind the Typesense product have created something with a better developer experience than even paid SaaS options and provided it with a clear value proposition.

## ServiceStack a Sponsor of Typesense

We were so impressed by the amazing results, that as a way to show our thanks we've become sponsors of 
[Typesense on GitHub sponsors](https://github.com/sponsors/typesense) 🎉 🎉 

We know how challenging it can be to try make open source software sustainable that if you find yourself using 
amazing open source products like Typesense which you want to continue to see flourish, we encourage finding 
out how you or your organization can support them, either via direct contributions or by helping spread the
word with public posts (like this) sharing your experiences as their continued success will encourage further
investments that ultimately benefits the project and all its users.


# Migrating from Jekyll to VitePress
Source: https://servicestack.net/posts/jekyll-to-vitepress

<style>
table {
    margin-bottom: 2em;
}
th, td {
    padding: 1em 1.5em;
}
tbody tr:nth-child(odd) {
    --tw-bg-opacity: 1;
    background-color: rgba(243, 244, 246, var(--tw-bg-opacity));
}
</style>

Since Jekyll support has been officially sunset, we decided to migrate our docs site to VitePress. VitePress is a new static site generator based on Vite and Vue 3. It is a cut down version of VuePress but using the blisteringly fast Vite build tool.

This has given us the ability to update our docs locally with instant on save update to view the final result quickly while we edit/create docs.

<div class="my-8 ml-20 flex justify-center">
    <img style="max-height:150px" src="/img/posts/jekyll-to-vitepress/vite.svg" alt="Vite logo">
    <img style="max-height:150px" src="/img/posts/jekyll-to-vitepress/vuepress.png" alt="VitePress logo">
</div>


We wanted to share our experience with this process in the hope it might help others that are performing the same migration, document common errors and help other developer to get an idea about what is involved before you might undertake such a task.

## Jekyll vs VitePress

Below is not an exhaustive list of features but more focusing on pros and cons when it comes to comparing these two options for use as the static site generator for a documentation site.

| Features           | Jekyll            | VitePress                      |
|:-------------------|:------------------|:-------------------------------|
| Native Language    | Ruby              | JavaScript                     |
| Template Syntax    | Liquid            | Vue                            |
| Update Time        | 6-30 seconds      | 30-500 ms                      |
| Themes             | ✅ - mature market | ✅ - limited                    |
| Extensible         | ✅                 | ✅ (only themes + vite plugins) |
| Client framework   | None              | Vue 3                          |
| Maintained         | No longer         | New project                    |
| 1.0+ release       | ✅                 | ➖                              |
| Permalink          | ✅                 | ➖ (depends on filename)        |
| Markdown support   | ✅                 | ✅                              |
| HTML support       | ✅                 | ✅ (must use Vue component)     |
| Sitemap generation | ✅                 | ➖                              |
| Tags               | ✅                 | ➖                              |
| Clean URLs         | ✅                 | ➖                              |

This list might look bad for VitePress, but it comes down to a young library that is still in active development. The default theme for VitePress is also centered around technical documentation making it quick to get up and running looking good for this use case.

## Killer feature - Performance

The stand out feature and one of the most compelling reason for us to undertake this migration was not the default theme (although that helped) but the user experience when editing documentation locally.

![](/img/posts/jekyll-to-vitepress/vitepress-update-large.gif)

In contrast, editing this blog post for our main site which is still currently using Jekyll, it takes between 6-8 seconds for a small single page change. Previously our docs page with over 300 pages could take over a minute depending on the type of change.

```
Regenerating: 1 file(s) changed at 2021-10-29 16:17:00
                    _blog/posts/2021-10-29-jekyll-migration.md
                    ...done in 6.3882767 seconds.
```

Having a statically generated site that is hard to preview or has a slow iteration cycle can be extremely frustrating to work with so we were looking for something that a pleasure to work with and well suited to documentation.

## Common Migration Problems

When we started this task to migrate, we want to first have a proof of concept of migrating all the content to the same URLs.
This required us to achieve a few things before we could get stuck into fixing content/syntax related changes.

- Page URLs must be the same.
- Project must be able to run locally and deploy.
- Side menu links must be present.

These were our minimum requirements before we wanted to commit to making all the changes required to migrate.

## File name vs `slug`

The first one surfaced a design difference straight away. While Jekyll created HTML output based on frontmatter `slug` property, VitePress only works off the MarkDown file name.

We would commonly create a MarkDown file called one thing but change our mind on the URL and change the frontmatter `permalink` to something else. This broke all our existing paths, so we needed a way to parse the markdown files, and produce copies with the updated name using the `slug` value in the frontmatter.

Since we were still evaluating this process, we created a quick C# script that would take in a file, extract the `slug` value and copy a file with that name back out into a separate directory.

```csharp
static void Main(string[] args)
{
    var filename = args[0];
    var fileLines = File.ReadAllLines(filename).ToList();
    if (!Directory.Exists("updated"))
    {
        Directory.CreateDirectory("updated");
    }
    foreach (var line in fileLines)
    {
        if (line.StartsWith("slug:"))
        {
            var newName = line.Split(":")[1].Trim();
            File.WriteAllLines("./updated/" + newName + ".md", fileLines);
        }
    }
}
```

Not very elegant, but it did the job. This was published as a `single file` executable targeting linux so it could be easily used with tools like `find` with `--exec`.

```shell
find *.md -maxdepth 1 -type f -exec ./RenameMd {} \;
```

All round pretty hacky, but this was also while we were still evaluating VitePress, so it was considered a throw away script.

> This was run in each directory as needed, if `slug` or `permalink` is controlling your nested pathing, this problem will be more complex to handle.

This was run for our main folder of docs as well as our `releases` folder and we have successfully renamed files.

## Broken links are build failures

VitePress is more strict with issues than Jekyll. This is actually a good thing, especially as your site content grows. VitePress will fail building your site if in your markdown to link to a relative link that it can't see is a file.

This comes from the above design decision of not aliasing files to output paths. Markdown links like `[My link](/my-cool-page)` needs to be able to see `my-cool-page.md`. This means if you move or rename a file, it will break if something else links to it. Jekyll got around this by allowing the use of `permalink` and `slug` which is great for flexibility, but means at build time it can't be sure (without a lot more work) if the relative path won't be valid.

There are drawbacks to this though. If you host multiple resources under the same root path as your VitePress site and you want to reference this, I'm not sure you will be able to. You might have to resort to absolute URLs to link out to resources like this. And since VitePress doesn't alias any paths, it means your hosting environment will need to do this.

## Syntax issues

Jekyll is very forgiving when it comes to content that is passed around as straight html and put in various places using Liquid. For example if you have the following HTML in an `include` for Jekyll.

```html
<p>This solution is <50 lines of code</p>
```

Jekyll will just copy it and not bother you about the invalid HTML issues of having a `less-than (<)` in the middle of a `<p>` element. VitePress won't however, and you'll need to correctly use `&lt;` and `&gt;` encoded symbols appropriately.

## Include HTML

Another issue is the difference of how to reuse content. In Jekyll, you would use `{% include my/path/to/file.html %}`. This will likely show up in errors like `[vite:vue] Duplicate attribute`.

Instead in VitePress, an include of straight HTML will require migrating that content to a Vue component.

For example, if we have content file `catchphrase.html` like the following.

```html
<div>
    <h4>Catchphrase</h4>
    <p>It's.. what I do..</p>
</div>
```

We would need to wrap this in a Vue component like `catchphrase.vue`:

```html
<template>
    <div>
        <h4>Catchphrase</h4>
        <p>It's.. what I do..</p>
    </div>
</template>
<script>
    export default {
        name: "catchphrase"
    }
</script>

<style scoped>

</style>
```

Then it would need to be imported. This can be declared globally in the vitepress theme config or adhoc in the consuming Markdown file itself.

```markdown
<script setup>
import catchphrase from './catchphrase.vue';
</script>

<catchphrase />
```

The `<catchphrase />` is where it is injected into the output. For HTML so simple, this could be instead converted to Markdown and used the same way.

```markdown
## Catchphrase
it's.. what I do..
```

And then used:

```markdown
<script setup>
import catchphrase from './catchphrase.md';
</script>

<catchphrase />
```

## Jekyll markdownify redundant
Something similar is done in Jekyll, but with the use of Liquid filters.

```markdown
{% capture projects %}
{% include web-new-netfx.md %}
{% endcapture %} 
{{ projects | markdownify }}
```

This use of `capture` and passing the content to be converted is done by default when importing.

```markdown
<script setup>
import netfxtable from './.vitepress/includes/web-new-netfx.md';
</script>

<netfxtable />
```

If the module is declared global, then only the `<netfxtable />` is needed anywhere in your site to embed the content.

## Templating syntax the same but different

When moving from Jekyll to VitePress, I came across errors like `Cannot read property 'X' of undefined`. It was referring to some example code in a page we had that looked something like this.

```markdown
Content text here with templating code example below.

    Value: {{X.prop}}

More explanation here.
```

This error came about because we didn't religiously fence our code examples. Jekyll let us get away with this and actually produced the visuals we wanted without trying to render value in the handlebars `{{ }}` syntax.

VitePress only ignores these if they are in a code fence using the triple tilda syntax OR if the content is within a `:::v-pre` block.

## Replacing `raw` and `endraw`

Since some of our documentation used handlebar syntax in example code, we needed a way for Jekyll to ignore these statements and just present our code.
`raw` and `endraw` were used which were usually wrapping code blocks. VitePress doesn't have a problem with this syntax which means the `{% raw %}` statements were included in our page which we didn't want.

This was a matter of finding where these were used in all our documents and replace them with `:::v-pre` blocks as needed.

## Sidebar

This was a different situation. In Jekyll, we had created a `SideBar.md` file to help us render a left hand side menu of contents for the docs site. 
In VitePress's default theme, we could provide a JSON representation and the client would then dynamically populate it further using different levels of headings.

To do this, we used a simple NodeJs script that used the `markdown-it` library to parse the MarkDown itself and produce the expected JSON.

Loading the file and extracting the elements we needed was quite straight forward.

```js
let fs = require('fs')
let MarkdownIt = require('markdown-it');
let md = new MarkdownIt();

let content = fs.readFileSync('SideBar.md','utf8')
let res = md.parse(content).filter((element, index) => {
    return (element.level == '3' || element.level == '5') && element.type == 'inline';
});
```

Now we had `res` that contained all the information we would need, we can iterate the array, transforming the data as we go.

```js
let sidebarObjs = {
    '/': []
};
var lastHeading = null;
var lastHeadingIndex = -1;
for(let i = 0; i < res.length; i++) {
    let item = res[i];
    if(item.level == '3') {
        lastHeading = item.content;
        sidebarObjs['/'].push({
            text: lastHeading,
            children: []
        })
        lastHeadingIndex++;
        continue;
    }
    let text = item.children[1].content;
    let link = item.children[0].attrs[0][1];
    sidebarObjs['/'][lastHeadingIndex].children.push({
        text: text,
        link: link
    })
}

fs.writeFileSync('SideBar_format.json',JSON.stringify(sidebarObjs),'utf-8')
```

Once working, we later split this into multiple menus and condensed what we already had to make the menu more manageable.

## Problems we worked around

That covers the bulk of the changes we made that prevented us from running, building and deploying our application, however there were some short comings that we had to work around outside of VitePress itself.
There were 2 main sticking points for which we had to come up with a creative work around.

- Client router always appending `.html` (breaking clean URLs)
- Failing to route to paths with addition dots (`.`) in the URL

Hosting clean URLs can be done from the server side, so we used AWS CloudWatch functions to rewrite the request to append the `.html` in the backend if it was not provided.
However, VitePress still generated all page links with `.html`, creating multiple paths for the single document which can cause problems with other processes such as search indexing.

Since our documentation is something we edit quite frequently, we didn't want to be stuck in limbo due to these two problems while we workout and propose how VitePress itself might allow for such a setup.
Our solution **is NOT recommended** since it will be quite brittle which we have accepted. We found the 2 locations in the VitePress source that were causing this problem during static site generation. 
Thankfully, the logic is simple and we can remove 2 lines of code during the `npm install` phase out of our `node_modules` directory so that our CI builds would be consistent.

```js
const fs = require('fs');
const glob = require('glob');
let js = 'node_modules/vitepress/dist/client/app/router.js';
fs.writeFileSync(js, fs.readFileSync(js, 'utf8').replace("url.pathname += '.html';", ''))

glob('node_modules/vitepress/dist/node/serve-*.js',{},(err,files) =>{
    let file = files[0];
    fs.writeFileSync(file,fs.readFileSync(file,'utf8').replace("cleanUrl += \".html\";",''))
})

console.log('Completed post install process...')
```
> NOT RECOMMENDED

If you have standard `.html` paths on your existing site, you won't need the above. It is just a temporary workaround for getting clean URLs, which is again, *not recommended*.

## Verdict

While we have made the jump to VitePress, it is still young and under heavy development.
There is a good chance that some of the behaviour and lacking features will make this not a viable option for migrating off Jekyll.

And we knew this going in as it is very clearly outlined on the front page of their docs:

![](/img/posts/jekyll-to-vitepress/vitepress-warning.png)

However, while there are still outstanding issues, the developer experience of Vue 3 combined with Vite and an SSG theme 
aimed at producing documentation is extremely compelling.
The work Even You and the VitePress community are doing is something to look out for as currently we believe it offers 
one of the best content heavy site development experiences currently possible.

## ServiceStack a Sponsor of Vue's Evan You

As maintainers of several [Vue & .NET Project Templates](https://docs.servicestack.net/templates-vue), we're big fans of 
Evan's work creating Vue, Vite and his overall stewardship of their surrounding ecosystems which greatly benefits from his 
master design skills and fine attention to detail in both library and UI design who has a talent in creating inherently 
simple technologies that progressively scales up to handle the complexest of Apps.

We believe in and are excited for the future of Vue and Vite that to show our support ServiceStack is now a sponsor of
[Evan You on GitHub sponsors](https://github.com/sponsors/yyx990803) 🎉 🎉


# ServiceStack License Agreement
Source: https://servicestack.net/terms

IMPORTANT! READ CAREFULLY: THIS IS A LEGAL AGREEMENT. BY DOWNLOADING, INSTALLING, COPYING, SAVING ON YOUR COMPUTER, OR OTHERWISE USING THE SOFTWARE, YOU (LICENSEE, AS DEFINED BELOW) ARE BECOMING A PARTY TO THIS AGREEMENT AND YOU ARE CONSENTING TO BE BOUND BY ALL THE TERMS AND CONDITIONS OF THIS AGREEMENT.

IF YOU DO NOT AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT, YOU SHOULD NOT DOWNLOAD, INSTALL OR USE THE SOFTWARE. BY CLICKING TO ACCEPT, YOU HEREBY AGREE TO THE TERMS OF THIS AGREEMENT.

### PURPOSE

ServiceStack, Inc. makes available software known as ServiceStack, including the ServiceStack releases, binaries, packages, websites, documentation and supporting content (collectively, the "Software") any third party software programs that are owned and licensed by parties other than Licensor and that either integrated with or made part of ServiceStack (collectively, "Third Party Software") under the terms of this ServiceStack License Agreement. If You do not agree to these terms, You may not access or use the Software.

### PARTIES

(a) "Licensor" means ServiceStack Inc, having its principal place of business at 2035 Sunset Lake Road, Newark, Delaware, USA.

(b) "Licensee" means the legal entity specified in the License Certificate, exercising rights under, and complying with all of the terms of, this Agreement who has not previously violated the terms of this Licence with respect to the Software, or who has received express permission from the Licensor to exercise rights under this Licence despite a previous violation.

### DEFINITIONS

The following terms have the meanings assigned to the below:

"Derivative Software" means the Binary code Software and/or Binary code that results from your compilation of modified or unmodified source code Software.

"Authorized User" means a single employee, independent contractor or other temporary worker authorized by Licensee to use Software while performing duties within the scope of their employment or assignment who has not previously violated the terms of this Licence with respect to the Software, or who has received express permission from the Licensor to exercise rights under this Licence despite a previous violation.

"License Certificate" means evidence of a license provided by Licensor to Licensee in electronic or printed form.

"License Key" means a unique key-code that enables Licensee to use Software by a single Authorized User. Only Licensor and/or its representatives are permitted to produce License Keys for Software.

"Competing Product" means your own software product that incorporates Derivative Software and substantially duplicates the capabilities or competes with the Software or Derivative Software.

"Allowed Project" means your own software that is not a Competing Product.

"ServiceStack Public Services" means any public remote services or Internet websites hosted or operated by ServiceStack, Inc. including websites or services identified using the top-level servicestack.net domain.

### USE OF THE SOFTWARE

Use of the Software and Documentation is subject to the restrictions contained in this Agreement. ServiceStack, Inc. grants you a non-exclusive, limited, non-transferable, freely revocable license to use the Software. ServiceStack, Inc. reserves all rights not expressly granted herein in the Software and the Content and may terminate this license at any time for any reason or no reason.

You agree that ServiceStack, Inc. or third parties own all legal right, title and interest in and to the Software, including any Intellectual Property Rights that subsist in the Software. "Intellectual Property Rights" means any and all rights under patent law, copyright law, trade secret law, trademark law, and any and all other proprietary rights. ServiceStack, Inc. reserves all rights not expressly granted to you. Nothing in this Agreement gives you a right to use the ServiceStack name or any of the ServiceStack trademarks, logos, domain names, and other distinctive brand features. Any feedback, comments, or suggestions you may provide regarding ServiceStack or the Software are entirely voluntary and we will be free to use such feedback, comments or suggestions as we see fit and without any obligation to you.

You may not use the Software for any purpose not expressly permitted by this License Agreement. Except to the extent required by applicable third party licenses, you may not: (a) use, modify, and compile source code Software for the purpose of creating Competing Products, or (b) copy, modify, adapt, redistribute, decompile, reverse engineer, disassemble reproduce and distribute Derivative Software as part of a Competing Product, and (c) share License Keys to Unauthorized Users.

Use, reproduction and distribution of components of the Software licensed under an open source software license are governed solely by the terms of that open source software license and not this License Agreement.

You agree that you will not remove, obscure, or alter any proprietary rights notices (including copyright and trademark notices) that may be affixed to or contained within the Software.

You agree that you are solely responsible for (and that ServiceStack, Inc. has no responsibility to you or to any third party for) any data, content, or resources that you create, transmit or display through the Software, and for the consequences of your actions (including any loss or damage which ServiceStack may suffer) by doing so.

With the exception of Open Source License Keys, you must protect the license key as Confidential Information of ServiceStack, Inc and you agree that you will not reproduce, publish, transmit, display, distribute or disclose your License Key.

We may, without prior notice, change the Software; stop providing the Software or features of the Software, to you, or to users generally; or we may create usage limits for the Software.

The Software contains features that removes technical restrictions and automatically disables the Software upon exceeding the free usage limits. Licensee may not disable, destroy, circumvent or remove these feature of the Software, and any attempt to do so will be in violation of this Agreement and will terminate Licensee's rights to use the Software.

ServiceStack, Inc. may investigate and/or suspend your license to use the Software and account if you violate any of the above policies. ServiceStack, Inc. reserves the right to immediately terminate your license to use the Software and account without further notice in the event that, in its judgment, you violate this Agreement.

### INDIVIDUAL FREE USAGE

Subject to the terms of this Agreement, a Licensee is granted a right to use the Software in Allowed Projects where they are the sole copyright owner and author within the scope
permitted by the "Individual License Key" granted from the Licensor's website at [servicestack.net/free](https://servicestack.net/free).

### OPEN SOURCE FREE USAGE

Subject to the terms of this Agreement, a Licensee is granted a right to use the Software in Allowed Projects whose entire source code is released under the Open Source licenses
identified by its [SPDX identifier](https://spdx.org/licenses/): MIT, GPL-3.0, Apache-2.0, GPL-2.0, BSD-3-Clause, LGPL-2.1-or-later, MS-PL, BSD-2-Clause, Zlib, EPL-1.0 and is
publicly available within the sites and scope permitted by the "Open Source License Key" granted from the Licensor's website at [servicestack.net/free](https://servicestack.net/free).

### RESTRICTED FREE USAGE

Subject to the terms of this Agreement, Licensee is granted a right to use the Software for small projects and evaluation purposes without charge provided that use of the Software is within the "Free Usage Quota" published on Licensor's website at [servicestack.net](https://servicestack.net).

Upon exceeding the Free Usage Quota limits in production, Licensee must obtain License Key for continued use of the Software or cease using the Software.

Libraries without Free Usage Quotas can be used in any Allowed Project subject to the terms of this Agreement.

### LICENSE FEES AND PAYMENTS

Licensee agrees to the terms and conditions of Software purchase published on Licensor's website at [servicestack.net](https://servicestack.net). Licensee will pay to Licensor the license fee and other charges (if applicable) as set forth in the invoice or other purchase documentation. Licensor may charge Licensee interest for any payment that is more than thirty (30) days past due at the rate of one and one-half percent (1.5%) per month or the highest amount allowed by law, whichever is lower.

### YOUR CONTENT

“Content” means any information, text, graphics, or other materials uploaded, downloaded or appearing on ServiceStack Public Services. You retain ownership of all Content you submit, post, display, or otherwise make available on ServiceStack Public Services.

By submitting, posting or displaying Content on or through ServiceStack Public Services, you grant, and you represent and warrant that you have a right to grant, to ServiceStack, Inc. a worldwide, non-exclusive, perpetual, royalty-free license (with the right to sublicense) to use, reproduce, adapt, modify, publish, transmit, perform, display, distribute, and make derivative works of such Content in any and all media or distribution methods (now known or later developed). You agree that this license includes the right for ServiceStack, Inc. to make your Content available to others for the publication, distribution, syndication, or broadcast of such Content on other media and services.

Your Content will be able to be viewed by other users of ServiceStack Public Services and through third party services and ServiceStack Public Services. You should only provide Content that you are comfortable sharing with others under the terms of this Agreement. All Content, whether publicly posted or privately transmitted, is the sole responsibility of the person who originated such Content. We may not monitor or control the Content posted via ServiceStack Public Services. We do not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Content or communications posted via ServiceStack Public Services nor does ServiceStack endorse any opinions expressed via ServiceStack Public Services. Any use or reliance on any Content or materials posted via ServiceStack Public Services or obtained by you through ServiceStack Public Services is at your own risk. You understand that by using ServiceStack Public Services, you may be exposed to Content that might be offensive, harmful, inaccurate or otherwise inappropriate, or postings that have been mislabeled or are otherwise deceptive. Under no circumstances will ServiceStack be liable in any way for any Content, including, but not limited to, any errors or omissions in any Content, or any loss or damage of any kind incurred as a result of the use of any Content posted, emailed, transmitted or otherwise made available via ServiceStack Public Services or broadcast elsewhere.

You are responsible for your use of ServiceStack Public Services, for any Content you provide, and for any consequences thereof, including the use of your Content by other users and third party partners. You understand that your Content will be published on ServiceStack Public Services and may be republished, and if you do not have the right to submit Content for such use, it may subject you to liability. ServiceStack, Inc. will not be responsible or liable for any use of your Content by ServiceStack in accordance with this Agreement. You represent and warrant that you have all the rights, power and authority necessary to grant the rights granted herein to any Content that you submit and that ServiceStack’s use of your Content in accordance with this Agreement will not violate any law or infringe the rights of any third party.

We reserve the right at all times (but will not have an obligation) to remove or refuse to distribute any Content on ServiceStack Public Services and to terminate your access. We also reserve the right to access, read, preserve, and disclose any information as we reasonably believe is necessary to (i) satisfy any applicable law, regulation, legal process or governmental request, (ii) enforce this Agreement, including investigation of potential violations hereof, (iii) detect, prevent, or otherwise address fraud, security or technical issues, (iv) respond to you support requests, or (v) protect the rights, property or safety of ServiceStack, Inc. its users and the public.

### DISCLAIMER OF WARRANTIES

YOU EXPRESSLY UNDERSTAND AND AGREE THAT YOUR USE OF THE SOFTWARE IS AT YOUR SOLE RISK AND THAT THE SOFTWARE IS PROVIDED "AS IS" AND "AS AVAILABLE" WITHOUT WARRANTY OF ANY KIND FROM ServiceStack, Inc.

YOUR USE OF THE SOFTWARE AND ANY MATERIAL DOWNLOADED OR OTHERWISE OBTAINED THROUGH THE USE OF THE SOFTWARE IS AT YOUR OWN DISCRETION AND RISK AND YOU ARE SOLELY RESPONSIBLE FOR ANY DAMAGE TO YOUR COMPUTER SYSTEM OR OTHER DEVICE OR LOSS OF DATA THAT RESULTS FROM SUCH USE.

ServiceStack, Inc. FURTHER EXPRESSLY DISCLAIMS ALL WARRANTIES AND CONDITIONS OF ANY KIND, WHETHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO THE IMPLIED WARRANTIES AND CONDITIONS OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.

### LIMITATION OF LIABILITY

YOU EXPRESSLY UNDERSTAND AND AGREE THAT ServiceStack, Inc. ITS SUBSIDIARIES AND AFFILIATES, AND ITS LICENSORS SHALL NOT BE LIABLE TO YOU UNDER ANY THEORY OF LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR EXEMPLARY DAMAGES THAT MAY BE INCURRED BY YOU, INCLUDING ANY LOSS OF DATA, WHETHER OR NOT SERVICESTACK OR ITS REPRESENTATIVES HAVE BEEN ADVISED OF OR SHOULD HAVE BEEN AWARE OF THE POSSIBILITY OF ANY SUCH LOSSES ARISING.

Third-Party Links The Software may contain links to third-party websites or resources. You acknowledge and agree that we are not responsible or liable for: (i) the availability or accuracy of such websites or resources; or (ii) the content, products, or services on or available from such websites or resources. Links to such websites or resources do not imply any endorsement by ServiceStack of such websites or resources or the content, products, or services available from such websites or resources. You acknowledge sole responsibility for and assume all risk arising from your use of any such websites or resources.

### INDEMNIFICATION

Your access to and use of the Software, third-party Products or any Content is at your own risk. You understand and agree that the Software is provided to you on an “AS IS” and “AS AVAILABLE” basis. Without limiting the foregoing, COMPANY AND ITS PARTNERS DISCLAIM ANY WARRANTIES, EXPRESS OR IMPLIED, OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT.

To the maximum extent permitted by law, you agree to defend, indemnify and hold harmless ServiceStack, Inc. its affiliates and their respective directors, officers, employees and agents from and against any and all claims, actions, suits or proceedings, as well as any and all losses, liabilities, damages, costs and expenses (including reasonable attorneys fees) arising out of or accruing from (a) your use of the Software, (b) any application you develop on the Software that infringes any copyright, trademark, trade secret, trade dress, patent or other intellectual property right of any person or defames any person or violates their rights of publicity or privacy, and (c) any non-compliance by you with this License Agreement.

### GENERAL LEGAL TERMS

This License Agreement constitutes the whole legal agreement between you and ServiceStack, Inc. and governs your use of the Software (excluding any services which ServiceStack may provide to you under a separate written agreement), and completely replaces any prior agreements between you and ServiceStack in relation to the Software.

You agree that if ServiceStack, Inc. does not exercise or enforce any legal right or remedy which is contained in this License Agreement (or which ServiceStack, Inc. has the benefit of under any applicable law), this will not be taken to be a formal waiver of ServiceStack's rights and that those rights or remedies will still be available to ServiceStack, Inc.

If any court of law, having the jurisdiction to decide on this matter, rules that any provision of this License Agreement is invalid, then that provision will be removed from this License Agreement without affecting the rest of this License Agreement. The remaining provisions of this License Agreement will continue to be valid and enforceable.

You acknowledge and agree that each member of the group of companies of which ServiceStack, Inc. is the parent shall be third party beneficiaries to this License Agreement and that such other companies shall be entitled to directly enforce, and rely upon, any provision of this License Agreement that confers a benefit on (or rights in favor of) them. Other than this, no other person or company shall be third party beneficiaries to this License Agreement.

EXPORT RESTRICTIONS. THE SOFTWARE IS SUBJECT TO UNITED STATES EXPORT LAWS AND REGULATIONS. YOU MUST COMPLY WITH ALL DOMESTIC AND INTERNATIONAL EXPORT LAWS AND REGULATIONS THAT APPLY TO THE SOFTWARE. THESE LAWS INCLUDE RESTRICTIONS ON DESTINATIONS, END USERS AND END USE.

The rights granted in this License Agreement may not be assigned or transferred by either you or ServiceStack, Inc. without the prior written approval of the other party. Neither you nor ServiceStack shall be permitted to delegate their responsibilities or obligations under this License Agreement without the prior written approval of the other party. Notwithstanding the foregoing, either party shall have the right to assign this Agreement in connection with the merger, reorganization or acquisition of such party or the sale of all or substantially all of its assets related to this Agreement, without such consent.

This License Agreement, and your relationship with ServiceStack, Inc. under this License Agreement, shall be governed by the laws of the State of Delaware without regard to its conflict of laws provisions. You and ServiceStack, Inc. agree to submit to the exclusive jurisdiction of the courts located within the county of New Castle County, Delaware to resolve any legal matter arising from this License Agreement. Notwithstanding this, you agree that ServiceStack, Inc. shall still be allowed to apply for injunctive remedies (or an equivalent type of urgent legal relief) in any jurisdiction.

ServiceStack may provide notices to you via email, written or hard copy notice, or through posting on the Software, as determined by ServiceStack, Inc. in our sole discretion. ServiceStack, Inc. reserves the right to determine the form and means of providing notice to you, provided that you may opt out of certain means of notice as described in this Agreement.

We may revise this Agreement from time to time. The most current version will always be on this page. If in our sole discretion the revision is material, we will notify you as provided in this Agreement at least thirty days before the revisions become effective. By continuing to access or use the Software after those revisions become effective, you agree to be bound by the revised Agreement.

The Software is operated and provided by ServiceStack, Inc., 2035 Sunset Lake Road, Newark, Delaware, USA 19702-2600. If you have questions about this Agreement, please contact us at [team@servicestack.net](mailto:team@servicestack.net).

January 1, 2022


# Quick Shortcuts to ServiceStack Sites
Source: https://servicestack.net/links

### [servicestack.net/discord](https://servicestack.net/discord)
Join **#ServiceStack** Discord Channel

### [servicestack.net/discuss](https://servicestack.net/discuss)
ServiceStack GitHub Discussions

 - [/ask](https://servicestack.net/ask) - Q & A Community Support
 - [/ideas](https://servicestack.net/ideas) - Ideas & Feature Requests
 - [/showcase](https://servicestack.net/showcase) - Share your Apps with the community


# Community Rules
Source: https://servicestack.net/community-rules

servicestack.net is where anyone is welcome to learn about ServiceStack and .NET development.
We want to keep it a welcome place, so we have created this ruleset to help guide the content posted on servicestack.net.

If you see a post or comment that breaks the rules, we welcome you to report it to the our moderators.

These rules apply to all community aspects on servicestack.net: all parts of a public post (title, description, tags, visual content), comments, links, and messages.
Moderators consider context and intent while enforcing the community rules.

- No nudity or sexually explicit content.
- Provocative, inflammatory, unsettling, or suggestive content should be marked as Mature.
- No hate speech, abuse, or harassment.
- No content that condones illegal or violent activity.
- No gore or shock content.
- No posting personal information.

### Good Sharing Practices

Considering these tips when sharing with the servicestack.net community will help ensure you're contributing great content.

#### 1. Value
- Good sharing means posting content which brings value to the community. Content which opens up a discussion, shares something new and unique, or has a deeper story to tell beyond the image itself is content that generally brings value. Ask yourself first: is this something I would be interested in seeing if someone else posted it?
#### 2. Transparency
- We expect that the original poster (OP) will be explicit about if and how they are connected to the content they are posting. Trying to hide that relationship, or not explaining it well to others, is a common feature of bad sharing.
#### 3. Respect
- Good sharing means knowing when the community has spoken through upvotes and downvotes and respecting that. You should avoid constantly reposting content to User Submitted that gets downvoted. This kind of spamming annoys the community, and it won't make your posts any more popular.
  Repeated violations of the good sharing practices after warning may result in account ban.


If content breaks these community rules, it will be removed and the original poster warned about the removal.
Warnings will expire. If multiple submissions break the rules in a short time frame, warnings will accumulate, which could lead to a 24-hour suspension, and further, a ban.

If you aren't sure if your post fits the community rules, please don't post it.
Just because you've seen a rule-breaking image posted somewhere else on servicestack.net doesn't mean it's okay for you to repost it.


# Privacy Policy for ServiceStack, Inc
Source: https://servicestack.net/privacy

At ServiceStack Website, accessible from https://servicestack.net, one of our main priorities is the privacy of our visitors. This Privacy Policy document contains types of information that is collected and recorded by ServiceStack Website and how we use it.

If you have additional questions or require more information about our Privacy Policy, do not hesitate to contact us.

This Privacy Policy applies only to our online activities and is valid for visitors to our website with regards to the information that they shared and/or collect in ServiceStack Website. This policy is not applicable to any information collected offline or via channels other than this website. 

### Consent
By using our website, you hereby consent to our Privacy Policy and agree to its terms.

### Information we collect
The personal information that you are asked to provide, and the reasons why you are asked to provide it, will be made clear to you at the point we ask you to provide your personal information.

If you contact us directly, we may receive additional information about you such as your name, email address, phone number, the contents of the message and/or attachments you may send us, and any other information you may choose to provide.

When you register for an Account, we may ask for your contact information, including items such as name, company name, address, email address, and telephone number.

### How we use your information
We use the information we collect in various ways, including to:

- Provide, operate, and maintain our website
- Improve, personalize, and expand our website
- Understand and analyze how you use our website
- Develop new products, services, features, and functionality
- Find and prevent fraud
- Send you emails, directly, for customer service, to provide you with updates and other information relating to the website, and for marketing and promotional purposes

### Newsletters
We offer electronic newsletters which you may voluntarily subscribe to. You may choose to stop receiving our newsletter or marketing emails 
by following the unsubscribe instructions included in these emails or by contacting us. However, you will continue to receive essential emails.

### Sharing your information with other entities
We will not share your personal information with third parties, except as necessary for our legitimate professional and business needs,
to carry out your requests, and/or as required or permitted by law or professional standards.

### Log Files
ServiceStack Website follows a standard procedure of using log files. These files log visitors when they visit websites. All hosting companies do this and a part of hosting services' analytics. The information collected by log files include internet protocol (IP) addresses, browser type, Internet Service Provider (ISP), date and time stamp, referring/exit pages, and possibly the number of clicks. These are not linked to any information that is personally identifiable. The purpose of the information is for analyzing trends, administering the site, tracking users' movement on the website, and gathering demographic information.

### Cookies and Web Beacons
Like any other website, ServiceStack Website uses 'cookies'. These cookies are used to store information including visitors' preferences, and the pages on the website that the visitor accessed or visited. The information is used to optimize the users' experience by customizing our web page content based on visitors' browser type and/or other information.

For more general information on cookies, please read ["What Are Cookies"](https://www.privacypolicyonline.com/what-are-cookies/).

### Advertising Partners Privacy Policies
You may consult this list to find the Privacy Policy for each of the advertising partners of ServiceStack Website.

Third-party ad servers or ad networks uses technologies like cookies, JavaScript, or Web Beacons that are used in their respective advertisements and links that appear on ServiceStack Website, which are sent directly to users' browser. They automatically receive your IP address when this occurs. These technologies are used to measure the effectiveness of their advertising campaigns and/or to personalize the advertising content that you see on websites that you visit.

Note that ServiceStack Website has no access to or control over these cookies that are used by third-party advertisers.

### Third Party Privacy Policies
ServiceStack Website's Privacy Policy does not apply to other advertisers or websites. 
Thus, we are advising you to consult the respective Privacy Policies of these third-party ad servers for more detailed information. 
It may include their practices and instructions about how to opt-out of certain options.

You can choose to disable cookies through your individual browser options. To know more detailed information about cookie management with specific web browsers, it can be found at the browsers' respective websites.

### CCPA Privacy Rights (Do Not Sell My Personal Information)
Under the CCPA, among other rights, California consumers have the right to:

Request that a business that collects a consumer's personal data disclose the categories and specific pieces of personal data that a business has collected about consumers.

Request that a business delete any personal data about the consumer that a business has collected.

Request that a business that sells a consumer's personal data, not sell the consumer's personal data.

If you make a request, we have one month to respond to you. If you would like to exercise any of these rights, please contact us.

### GDPR Data Protection Rights
We would like to make sure you are fully aware of all of your data protection rights. Every user is entitled to the following:

The right to access – You have the right to request copies of your personal data. We may charge you a small fee for this service.

The right to rectification – You have the right to request that we correct any information you believe is inaccurate. You also have the right to request that we complete the information you believe is incomplete.

The right to erasure – You have the right to request that we erase your personal data, under certain conditions.

The right to restrict processing – You have the right to request that we restrict the processing of your personal data, under certain conditions.

The right to object to processing – You have the right to object to our processing of your personal data, under certain conditions.

The right to data portability – You have the right to request that we transfer the data that we have collected to another organization, or directly to you, under certain conditions.

If you make a request, we have one month to respond to you. If you would like to exercise any of these rights, please contact us.

### Children's Information
Another part of our priority is adding protection for children while using the internet. We encourage parents and guardians to observe, participate in, and/or monitor and guide their online activity.

ServiceStack Website does not knowingly collect any Personal Identifiable Information from children under the age of 13. If you think that your child provided this kind of information on our website, we strongly encourage you to contact us immediately and we will do our best efforts to promptly remove such information from our records.


# License Registration
Source: https://servicestack.net/register

The ServiceStack license key allows un-restricted access to ServiceStack packages and is available in your 
**My Account** Section after purchasing a [commercial license](https://servicestack.net/pricing).

There are multiple ways of registering your license key which all need to be added to your top-level host project:

### a) Add it to the projects appsettings.json or Web.Config

Easiest way to register your license key is to add the **servicestack license** appSetting.
For ASP.NET Core Apps add it to **appsettings.json**:

```json
{
    "servicestack": {
        "license": "{licenseKeyText}"
    }
}
```

Non ServiceStack .NET Core **AppHost** Apps (i.e. just using Redis or OrmLite) will also need to explicitly register the license key from IConfiguration:

```csharp
Licensing.RegisterLicense(Configuration.GetValue<string>("servicestack:license"));
```

For .NET Framework Applications add it to the **Web.config** or App.config's `<appSettings/>` config section:

```xml
<appSettings>
    <add key="servicestack:license" value="{licenseKeyText}" />
</appSettings>
```

### b) Add it in code before your Application Starts Up

By calling `Licensing.RegisterLicense()` before your application starts up, E.g. For ASP.NET, place it in the Global.asax `Application_Start` event:

```csharp
protected void Application_Start(object sender, EventArgs e)
{
    Licensing.RegisterLicense(licenseKeyText);
    new AppHost().Init();
}
```

Otherwise for a self-hosting Console Application, place it before initializing the AppHost, as shown above.

### c) Add the System Environment Variable

To simplify license key registration when developing multiple ServiceStack solutions you can register the License Key once in the 
**SERVICESTACK_LICENSE** Environment Variable on each pc using ServiceStack libraries:

| Variable | Value |
|:-|:-|
| SERVICESTACK_LICENSE | `{licenseKeyText}` |

You'll need to restart IIS or VS.NET for them to pickup any new Environment Variables.

### d) Copy license key text into an external text file

Similar to above, we can register the license from an external plain-text file containing the license key text, e.g:

```csharp
protected void Application_Start(object sender, EventArgs e)
{
    Licensing.RegisterLicenseFromFileIfExists("~/license.txt".MapHostAbsolutePath());
    new AppHost().Init();
}
```

For Self-Hosting set BuildAction to **Copy if Newer** and use `"~/license.txt".MapAbsolutePath()` method.

The license key is white-space insensitive so can be broken up over multiple lines.


# About
Source: https://servicestack.net/creatorkit/about

[![](/img/pages/creatorkit/creatorkit-brand.svg)](/creatorkit/)

[CreatorKit](/creatorkit/) is a simple, customizable alternative solution to using Mailchimp for accepting and managing website
newsletter subscriptions and other mailing lists, sending rich emails with customizable email layouts and templates to your
Customers and subscribers using your preferred SMTP provider of choice.

It also provides a private alternative to using Disqus to enhance websites with threading and commenting on your preferred
blog posts and website pages you want to be able to collaborate with your community on.

### Enhance static websites

We're developing CreatorKit as an ideal companion for JAMStack or statically generated branded websites like
[Razor SSG](https://razor-ssg.web-templates.io/posts/razor-ssg)
enabling you to seamlessly integrate features such as newsletter subscriptions, email management, comments, voting,
and moderation into your existing websites without the complexity of a custom solution, that's ideally suited for Websites
who want to keep all Mailing Lists Contacts and Authenticated User Comments in a different site, isolated from your
existing Customer Accounts and internal Systems.

With CreatorKit, you can enjoy the convenience of managing your blog's comments, votes, and subscriptions directly
from your own hosted [CreatorKit Portal](https://creatorkit.netcore.io/portal/) without needing to rely on complex content 
management systems to manage your blog's interactions with your readers.

Additionally, CreatorKit makes it easy to send emails and templates to different mailing lists, making it the perfect
tool for managing your email campaigns. Whether you're a blogger, marketer, or entrepreneur, CreatorKit is a great
solution for maximizing your blog's functionality and engagement.

## Features

The CreatorKit Portal offers a complete management UI to manage mailing lists, email newsletter and marketing campaigns,
thread management and moderation workflow.

### Email Management

[![](/img/pages/creatorkit/portal-messages.png)](/creatorkit/portal-messages)

### Optimized Email UI's with Live Previews

[![](/img/pages/creatorkit/portal-messages-simple.png)](/creatorkit/portal-messages#email-ui)

### Custom HTML Templates

[![](/img/pages/creatorkit/portal-messages-custom.png)](/creatorkit/portal-messages#sending-custom-html-emails)

### HTML Email Templates

[![](/img/pages/creatorkit/portal-messages-markdown.png)](/creatorkit/portal-messages#sending-html-markdown-emails)

### Mailing List Email Runs

[![](/img/pages/creatorkit/portal-mailrun-custom.png)](/creatorkit/portal-mailruns)

### Newsletter Generation

[![](/img/pages/creatorkit/portal-mailrun-newsletter.png)](/creatorkit/portal-mailruns#generating-newsletters)

### Comment Moderation

[![](/img/pages/creatorkit/portal-report.png)](/creatorkit/portal-posts)

### Use for FREE

CreatorKit is a FREE customizable .NET App included with [ServiceStack](https://servicestack.net) which is
[Free for Individuals and Open Source projects](https://servicestack.net/free) or for organizations that continue to
host their forked CreatorKit projects on GitHub or GitLab. As a stand-alone hosted product there should be
minimal need for any customizations with initial [Mailining Lists, Subscribers](/creatorkit/install#before-you-run),
[App Settings](/creatorkit/install#whats-included) and branding information maintained in
customizable [CSV](/creatorkit/install#before-you-run) and [text files](/creatorkit/customize).

To get started follow the [installation instructions](/creatorkit/install) to download and configure it with your
organization's website settings.

## Future

As we're using CreatorKit ourselves to power all dynamic Mailing List and Comment System features on
[https://servicestack.net](servicestack.net), we'll be continuing to develop it with useful features to
empower static websites with more generic email templates and potential to expand it with commerce features, inc.
Stripe integration, products & subscriptions, ordering system, invoicing, quotes, PDF generation, etc.

Follow [@ServiceStack](https://twitter.com/ServiceStack), Watch or Star [NetCoreApps/CreatorKit](https://github.com/NetCoreApps/CreatorKit)
or Join our CreatorKit-powered Monthly Newsletter to follow and keep up to date with new features:

<div class="not-prose">
    <div class="mt-8 mx-auto max-w-md" data-mail="JoinMailingList" data-props="{ submitLabel:'Join our newsletter' }"></div>
</div>

As a design goal [CreatorKit's components](/creatorkit/components) will be easily embeddable into any external website,
where it will be integrated into the [Razor SSG](/posts/razor-ssg) project template to serve as a working demonstration
and reference implementation. As such it's a great option if you're looking to create a Fast, FREE, CDN hostable,
[simple, modern](/posts/javascript) statically generated website created with Razor & Markdown
like [ServiceStack/servicestack.net](https://github.com/ServiceStack/servicestack.net).

### Feedback welcome

If you'd like to prioritize features you'd like to see first or propose new, generically useful features for
static websites, please let us know in [servicestack.net/ideas](https://servicestack.net/ideas).


# Install
Source: https://servicestack.net/creatorkit/install

CreatorKit is a customizable .NET companion App that you would run alongside your Website which provides the backend for
mailing list subscriptions, User repository and comment features which can be added to your website with CreatorKit's
tailwind components which are loaded from and communicate back directly to your CreatorKit .NET App instance:

<div class="my-12 flex justify-center w-full">
    <div class="flex flex-wrap">
        <svg class="w-40 h-40 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M13 23V13h10v10H13Zm1.5-1.5h7v-.8q-.625-.775-1.525-1.238T18 19q-1.075 0-1.975.463T14.5 20.7v.8ZM18 18q.625 0 1.063-.438T19.5 16.5q0-.625-.438-1.063T18 15q-.625 0-1.063.438T16.5 16.5q0 .625.438 1.063T18 18Zm-8.75 4l-.4-3.2q-.325-.125-.613-.3t-.562-.375L4.7 19.375l-2.75-4.75l2.575-1.95Q4.5 12.5 4.5 12.337v-.674q0-.163.025-.338L1.95 9.375l2.75-4.75l2.975 1.25q.275-.2.575-.375t.6-.3l.4-3.2h5.5l.4 3.2q.325.125.613.3t.562.375l2.975-1.25l2.75 4.75L19.925 11H15.4q-.35-1.075-1.25-1.788t-2.1-.712q-1.45 0-2.475 1.025T8.55 12q0 1.2.675 2.1T11 15.35V22H9.25Z"/></svg>
        <svg class="w-40 h-40 mx-8 mt-10 inline-block" style="rotate:200deg" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M16.01 11H4v2h12.01v3L20 12l-3.99-4v3z"/></svg>
        <div class="flex flex-col">
            <svg class="w-40 h-40 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"><path d="M3.338 17A9.996 9.996 0 0 0 12 22a9.996 9.996 0 0 0 8.662-5M3.338 7A9.996 9.996 0 0 1 12 2a9.996 9.996 0 0 1 8.662 5"/><path d="M13 21.95s1.408-1.853 2.295-4.95M13 2.05S14.408 3.902 15.295 7M11 21.95S9.592 20.098 8.705 17M11 2.05S9.592 3.902 8.705 7M9 10l1.5 5l1.5-5l1.5 5l1.5-5M1 10l1.5 5L4 10l1.5 5L7 10m10 0l1.5 5l1.5-5l1.5 5l1.5-5"/></g></svg>
            <svg class="w-32 h-32 text-sky-400" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M12 6c-2.67 0-4.33 1.33-5 4c1-1.33 2.17-1.83 3.5-1.5c.76.19 1.31.74 1.91 1.35c.98 1 2.09 2.15 4.59 2.15c2.67 0 4.33-1.33 5-4c-1 1.33-2.17 1.83-3.5 1.5c-.76-.19-1.3-.74-1.91-1.35C15.61 7.15 14.5 6 12 6m-5 6c-2.67 0-4.33 1.33-5 4c1-1.33 2.17-1.83 3.5-1.5c.76.19 1.3.74 1.91 1.35C8.39 16.85 9.5 18 12 18c2.67 0 4.33-1.33 5-4c-1 1.33-2.17 1.83-3.5 1.5c-.76-.19-1.3-.74-1.91-1.35C10.61 13.15 9.5 12 7 12Z"/></svg>
        </div>
    </div>
</div>

## Get CreatorKit

To better be able to keep up-to-date with future CreatorKit improvements we recommend
[forking CreatorKit](https://github.com/NetCoreApps/CreatorKit/fork) so you can easily apply future changes
to your customized forks:

<div class="not-prose my-12 text-center">
    <a href="https://github.com/NetCoreApps/CreatorKit" title="Fork CreatorKit" class="rounded-md bg-white px-3.5 py-2.5 text-2xl font-semibold text-gray-900 shadow-sm ring-1 ring-inset ring-gray-300 hover:bg-gray-50">
        <svg class="w-10 h-10 inline-block" xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256"><path fill="currentColor" d="M224 64a32 32 0 1 0-40 31v9a16 16 0 0 1-16 16H88a16 16 0 0 1-16-16v-9a32 32 0 1 0-16 0v9a32 32 0 0 0 32 32h32v25a32 32 0 1 0 16 0v-25h32a32 32 0 0 0 32-32v-9a32.06 32.06 0 0 0 24-31ZM48 64a16 16 0 1 1 16 16a16 16 0 0 1-16-16Zm96 128a16 16 0 1 1-16-16a16 16 0 0 1 16 16Zm48-112a16 16 0 1 1 16-16a16 16 0 0 1-16 16Z"/></svg>
        <span class="pr-1">CreatorKit</span>
    </a>
</div>

Or if you're happy to take CreatorKit's current feature-set as it is, download the .zip to launch a local instance of
CreatorKit:

<div class="not-prose mt-12 flex justify-center">
<a class="hover:no-underline" href="https://github.com/NetCoreApps/CreatorKit/archive/refs/heads/main.zip">
    <div class="bg-white dark:bg-gray-800 px-4 py-4 mr-4 mb-4 rounded-lg shadow-lg text-center items-center justify-center hover:shadow-2xl dark:border-2 dark:border-pink-600 dark:hover:border-blue-600 dark:border-2 dark:border-pink-600 dark:hover:border-blue-600" style="min-width: 150px;">
        <div class="text-center font-extrabold flex items-center justify-center mb-2">
            <div class="text-4xl text-blue-400 my-3">
                <svg class="w-12 h-12 text-indigo-600" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="currentColor" d="M13 23V13h10v10H13Zm1.5-1.5h7v-.8q-.625-.775-1.525-1.238T18 19q-1.075 0-1.975.463T14.5 20.7v.8ZM18 18q.625 0 1.063-.438T19.5 16.5q0-.625-.438-1.063T18 15q-.625 0-1.063.438T16.5 16.5q0 .625.438 1.063T18 18Zm-8.75 4l-.4-3.2q-.325-.125-.613-.3t-.562-.375L4.7 19.375l-2.75-4.75l2.575-1.95Q4.5 12.5 4.5 12.337v-.674q0-.163.025-.338L1.95 9.375l2.75-4.75l2.975 1.25q.275-.2.575-.375t.6-.3l.4-3.2h5.5l.4 3.2q.325.125.613.3t.562.375l2.975-1.25l2.75 4.75L19.925 11H15.4q-.35-1.075-1.25-1.788t-2.1-.712q-1.45 0-2.475 1.025T8.55 12q0 1.2.675 2.1T11 15.35V22H9.25Z"/></svg>
            </div>
        </div>
        <div class="archive-name px-4 pb-2 text-blue-600 dark:text-indigo-400">CreatorKit.zip</div>
    </div>
</a>
</div>

## Extending CreatorKit

To minimize disruption when upgrading to future versions of CreatorKit we recommend adding any new Services to 
[CreatorKit.Extensions](https://github.com/NetCoreApps/CreatorKit/tree/main/CreatorKit.Extensions) and their DTOs
in [CreatorKit.Extensions.ServiceModel](https://github.com/NetCoreApps/CreatorKit/tree/main/CreatorKit.Extensions.ServiceModel):

```files
/CreatorKit
  /CreatorKit.Extensions
    CustomEmailRunServices.cs
    CustomEmailServices.cs
    CustomRendererServices.cs
  /CreatorKit.Extensions.ServiceModel
    MarkdownEmail.cs
    NewsletterMailRun.cs
    RenderNewsletter.cs
```

These folders will be limited to optional extras which can added to or removed as needed where it will be isolated from 
the core set of functionality maintained in the other CreatorKit's folders. 

Any custom AppHost or IOC dependencies your Services require can be added to 
[Configure.Extensions.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/Configure.Extensions.cs).

### Before you Run

We need to initialize CreatorKit's database which we can populate with our preferred App Users, Mailing Lists and Subscribers
by modifying the CSV files in `/Migrations/seed`:

```files
/Migrations
  /seed
    mailinglists.csv
    subscribers.csv
    users.csv
  Migration1000.cs
  Migration1001.cs
```

## Mailing Lists

You can define all Mailing Lists you wish to send and contacts can subscribe to in **mailinglists.csv**:

#### mailinglists.csv

```csv
Name,Description
None,None
TestGroup,Test Group
MonthlyNewsletter,Monthly Newsletter
BlogPostReleases,New Blog Posts
VideoReleases,New Videos
ProductReleases,New Product Releases
YearlyUpdates,Yearly Updates
```

When the database is first created this list will be used to generate the
[MailingList.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.ServiceModel/Types/MailingList.cs) Enum, e.g:

```csharp
[Flags]
public enum MailingList
{
    None = 0,
    [Description("Test Group")]
    TestGroup         = 1 << 0,     //1
    [Description("Monthly Newsletter")]
    MonthlyNewsletter = 1 << 1,     //2
    [Description("New Blog Posts")]
    BlogPostReleases  = 1 << 2,     //4
    [Description("New Videos")]
    VideoReleases     = 1 << 3,     //8
    [Description("New Product Releases")]
    ProductReleases   = 1 << 4,     //16
    [Description("Yearly Updates")]
    YearlyUpdates     = 1 << 5,     //32
}
```

This is a `[Flags]` enum with each value increasing by a power of 2 allowing a single integer value to capture
all the mailing lists contacts are subscribed to.

#### subscribers.csv

Add any mailing subscribers you wish to be included by default, it's a good idea to include all Website developer emails
here so they can test sending emails to themselves:

```csv
Email,FirstName,LastName,MailingLists
test@subscriber.com,Test,Subscriber,3
```

[Mailing Lists](creatorkit/customize#mailing-lists) is a flag enums where the integer values is a sub of all Mailing Lists
you want them subscribed to, e.g. use `3` to  subscribe to both the `TestGroup (1)` and `MonthlyNewsletter (2)` Mailing Lists.

#### users.csv

Add any App Users you want your CreatorKit App to include by default, at a minimum you'll need an `Admin` user which is
required to access the Portal to be able to use CreatorKit:

```csv
Id,Email,FirstName,LastName,Roles
1,admin@email.com,Admin,User,"[Admin]"
2,test@user.com,Test,User,
```

Once your happy with your seed data run the included [OrmLite DB Migrations](https://docs.servicestack.net/ormlite/db-migrations) with:

<copy-line text="npm run migrate"></copy-line>

Which will create the CreatorKit SQLite databases with your seed Users and Mailing List subscribers included.

Should you need to recreate the database, you can delete the `App_Data/*.sqlite` databases then rerun
`npm run migrate` to recreate the databases with your updated `*.csv` seed data.

### What's included

The full .NET Source code is included with CreatorKit enabling unlimited customizations. It's a stand-alone download
which doesn't require any external dependencies to run initially, although some features require configuration:

#### SMTP Server

You'll need to configure an SMTP Server to enable sending Emails by adding it to your **appsettings.json**, e.g:

```json
{
  "smtp": {
    "UserName" : "SmtpUsername",
    "Password" : "SmtpPassword",
    "Host" : "smtp.example.org",
    "Port" : 587,
    "From" : "noreply@example.org",
    "FromName" : "My Organization",
    "Bcc": "optional.backup@example.org"
  }
}
```

If you don't have an existing SMTP Server we recommend using [Amazon SES](https://aws.amazon.com/ses/) as a cost effective
way to avoid managing your own SMTP Servers.

#### OAuth Providers

By default CreatorKit is configured to allow Sign In's for authenticated post comments from Facebook, Google, Microsoft
OAuth Providers during development on its `https://localhost:5002`.

You'll need to configure OAuth Apps for your production host in order to support OAuth Sign Ins at deployment:

- Create App for Facebook at https://developers.facebook.com/apps
- Create App for Google at https://console.developers.google.com/apis/credentials
- Create App for Microsoft at https://apps.dev.microsoft.com

You can Add/Remove to this from the list of [supported OAuth Providers](https://docs.servicestack.net/auth#oauth-providers).

### RDBMS

CreatorKit by default is configured to use an embedded SQLite database which can be optionally configured to replicate
backups to AWS S3 or Cloudflare R2 using [Litestream](https://docs.servicestack.net/ormlite/litestream).

This is setup to be used with Cloudflare R2 by default which can be configured from the [.deploy/litestream-template.yml](https://github.com/NetCoreApps/CreatorKit/blob/main/.deploy/litestream-template.yml) file:

```yml
access-key-id: ${R2_ACCESS_KEY_ID}
secret-access-key: ${R2_SECRET_ACCESS_KEY}

dbs:
  - path: /data/db.sqlite
    replicas:
      - type: s3
        bucket: ${R2_BUCKET}
        path: db.sqlite
        region: auto
        endpoint: ${R2_ENDPOINT}
```

By adding the matching GitHub Action Secrets to your repository, this file will be populated and deployed to your own Linux server via SSH.
This provides a realtime backup to your R2 bucket for minimal cost, enabling point in time recovery of data if you run into issues.

Alternatively [Configure.Db.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/Configure.Db.cs) can
be changed to use preferred [RDBMS supported by OrmLite](https://docs.servicestack.net/ormlite/installation).

### App Settings

The **PublicBaseUrl** and **BaseUrl** properties in `appsettings.json` should be updated with the URL where your
CreatorKit instance is deployed to and replace **WebsiteBaseUrl** with the website you want to use CreatorKit emails
to be addressed from:

```json
{
  "AppData": {
    "PublicBaseUrl":  "https://creatorkit.netcore.io",
    "BaseUrl":        "https://creatorkit.netcore.io",
    "WebsiteBaseUrl": "https://razor-ssg.web-templates.io"
  }
}
```

### CORS

Any additional Website URLs that utilize CreatorKit's components should be included in the CORS **allowOriginWhitelist**
to allow CORS requests from that website:

```json
{
  "CorsFeature": {
    "allowOriginWhitelist": [
      "http://localhost:5000",
      "http://localhost:8080"
    ]
  }
}
```

### Customize

After configuring CreatorKit to run with your preferred Environment, you'll want to customize it to your Organization
or Personal Brand:


# Customize
Source: https://servicestack.net/creatorkit/customize

The `/emails` folder contains all email templates and layouts made available to CreatorKit:

```files
/emails
  /layouts
    basic.html
    empty.html
    marketing.html
  /partials
    button-centered.html
    divider.html
    image-centered.html
    section.html
    title.html
  /vars
    info.txt
    urls.txt   
  empty.html
  newsletter-welcome.html
  newsletter.html
  verify-email.html
```

Which uses the [#Script](https://sharpscript.net) .NET Templating language to render Emails from Templates, where:

 - `/layouts` contains different kinds of email layouts
 - `/partials` contains all reusable [Partials](https://sharpscript.net/docs/partials) made available to your templates
 
The remaining `*.html` contains different type of emails you want to send, e.g. **empty.html** is a blank
template you can use to send custom Markdown email content with the your preferred email layout.

## Template Variables

All Branding Information referenced in the templates are maintained in the `/vars` folder:

```files
/vars
  info.txt
  urls.txt
```

At a minimum you'll want to replace all **info.txt** variables from ServiceStack's with your Organization's information:

#### info.txt

```txt
Company           ServiceStack
CompanyOfficial   ServiceStack, Inc.
Domain            servicestack.net
MailingAddress    470 Schooleys Mt Road #636, Hackettstown, NJ 07840-4096
MailPreferences   Mail Preferences
Unsubscribe       Unsubscribe
Contact           Contact
Privacy           Privacy policy
OurAddress        Our mailing address:
MailReason        You received this email because you are subscribed to ServiceStack news and announcements.
SignOffTeam       The ServiceStack Team
NewsletterFmt     ServiceStack Newsletter, {0}
SocialUrls        Website,Twitter,YouTube
SocialImages      website_24x24,twitter_24x24,youtube_24x24
```

Variables inside your email templates can be referenced using handlebars syntax, e.g: 

`{{info.Company}}`

The **urls.txt** contains all URLs embedded in emails that you'll want to replace with URLs on your website, with 
`/mail-preferences` and `/signup-confirmed` being integration pages covered in [Integrations](./integrations).

#### urls.txt

```txt
BaseUrl           {{BaseUrl}}
PublicBaseUrl     {{PublicBaseUrl}}
WebsiteBaseUrl    {{WebsiteBaseUrl}}
Website           {{WebsiteBaseUrl}}
MailPreferences   {{WebsiteBaseUrl}}/mail-preferences
Unsubscribe       {{WebsiteBaseUrl}}/mail-preferences
Privacy           {{WebsiteBaseUrl}}/privacy
Contact           {{WebsiteBaseUrl}}/#contact
SignupConfirmed   {{WebsiteBaseUrl}}/signup-confirmed
Twitter           https://twitter.com/ServiceStack
YouTube           https://www.youtube.com/channel/UC0kXKGVU4NHcwNdDdRiAJSA
```

 - **BaseUrl** - Base URL of your Website that uses CreatorKit
 - **AppBaseUrl** - Base URL of the current CreatorKit instance
 - **PublicAppBaseUrl** - Base URL of a public CreatorKit instance

The **PublicAppBaseUrl** is used to reference public images hosted on your deployed CreatorKit instance since most email
clients wont render images hosted on `https://localhost`.

### Usage

You're free to add to these existing collections or create new variable collections which  are accessible from 
`{{info.*}}` and `{{urls.*}}` in your templates that's also available via dropdown in the Markdown Editor Variables
dropdown:

![](/img/pages/creatorkit/markdown-vars.png)

In addition, a `{{images.*}}` variable collection is also populated from all images in the `/img/mail` folder, e.g:

```files
/img
  /mail
    blog_48x48@2x.png
    chat_48x48@2x.png
    email_100x100@2x.png
    logo_72x72@2x.png
    logofull_350x60@2x.png
    mail_48x48@2x.png
    speaker_48x48@2x.png
    twitter_24x24@2x.png
    video_48x48@2x.png
    website_24x24@2x.png
    welcome_650x487.jpg
    youtube_24x24@2x.png
    youtube_48x48@2x.png
```

That's prefixed with the `{{PublicAppBaseUrl}}` allowing them to be referenced directly in your `*.html` Email templates. e.g:  

```html
<img src="{{images.welcome_650x487.jpg}}">
```

Or from your Markdown Emails using Markdown Image syntax:

```markdown
![]({{images.welcome_650x487.jpg}})
```


# Components
Source: https://servicestack.net/creatorkit/components

After launching your customized CreatorKit instance, you can start integrating its features into your existing websites, 
or if you're also in need of a fast, beautiful website we highly recommend the [Razor SSG](https://razor-ssg.web-templates.io/posts/razor-ssg)
template which is already configured to include CreatorKit's components.

The components are included using a declarative progressive markup so that it doesn't affect the behavior of the website
if the CreatorKit is down or unresponsive.

## Enabling CreatorKit Components

To utilize CreatorKit's Components in your website you'll need to initialize the components you want to use by embedding
this script at the bottom of your page, e.g. in [Footer.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Shared/Footer.cshtml):

```html
<script type="module">
@{ var components = HostContext.DebugMode 
     ? "https://localhost:5003/mjs/components" 
     : "https://creatorkit.netcore.io/mjs/components"; }

import { mail } from '@components/mail.mjs'
import { post } from '@components/post.mjs'

mail('[data-mail]')
post('[data-post]')
</script>
```

Where `mail()` will scan the document for declarative `data-mail` for any Mailing List components to create, likewise `post()`
does the same for any Thread/Post components.

The `@components` URL lets you load Components from your `localhost:5001` instance during development and your public CreatorKit
instance in production which you'll need to replace `creatorkit.netcore.io` to use. 

## Post Voting and Comments

You can enable voting for individual posts or pages with and Thread comments by including the `PostComments` component with:

```html
<div data-post="PostComments""></div>
```

Which when loaded will render a thread like icon where users can up vote posts or pages and either Sign In/Sign Up
buttons for unauthenticated users or a comment box for Signed in Users:

<div data-post="PostComments" data-props="{ commentLink: null }" class="not-prose text-base mb-12"></div>

#### PostComments Properties

The available PostComments properties for customizing its behavior include:

```ts
defineProps<{
    hide?: "threadLikes"|"threadLikes"[]
    commentLink?: { href: string, label: string }
}>()
```

### Component Properties

Any component properties can be either declared inline using `data-props`, e.g:

```html
<div data-post="PostComments" data-props="{
    hide: 'threadLikes', 
    commentLink: { 
        href: '/community-rules',
        label: 'read the community rules'
    } 
}"></div>
```

<div data-post="PostComments" data-props="{
    hide: 'threadLikes', 
    commentLink: { 
        href: '/community-rules',
        label: 'read the community rules'
    } 
}" class="not-prose text-base mb-20"></div>

Where it will hide the Thread Like icon and include a link to your `/community-rules` page inside each comment box.

Alternatively properties can instead be populated in the `mail()` and `post()` initialize functions: 

```html
<script type="module">
@{ var components = HostContext.DebugMode
     ? "https://localhost:5001/mjs/components"
     : "https://creatorkit.netcore.io/mjs/components"; }

mail('[data-mail]', { 
    mailingLists:['MonthlyNewsletter'] 
})

post('[data-post]', {
    commentLink: { 
        href: '/community-rules',
        label: 'read the community rules'
    } 
})
</script>
```

## Mailing List Components

### JoinMailingList

The `JoinMailingList` component can be added anywhere you want to accept Mailing List subscriptions on your website, e.g:

```html
<div data-mail="JoinMailingList" data-props="{ submitLabel:'Join our newsletter' }"></div>
```

<div class="my-20 flex justify-center">
    <div data-mail="JoinMailingList" data-props="{ submitLabel:'Join our newsletter' }"></div>
</div>

Which you can style as needed as this template wraps in a 
[Newsletter.cshtml](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/Pages/Shared/Newsletter.cshtml)
Tailwind component that's displayed on the [Home Page](/).

#### JoinMailingList Properties

Which allows for the following customizations:

```ts
defineProps<{
    //= MonthlyNewsletter
    mailingLists?: "TestGroup"     | "MonthlyNewsletter" | "BlogPostReleases" |
                   "VideoReleases" | "ProductReleases"   | "YearlyUpdates" 
    placeholder?: string    //= Enter your email
    submitLabel?: string    //= Subscribe
    thanksHeading?: string  //= Thanks for signing up!
    thanksMessage?: string  //= To complete sign up, look for the verification...
    thanksIcon?: { svg?:string, uri?:string, alt?:string, cls?:string }
}>
```

### MailPreferences

The `MailPreferences` component manages a users Mailing List subscriptions which you can be linked in your Email footers
for users wishing to manage or unsubscribe from mailing list emails. 

It can be include in any HTML or Markdown page as Razor SSG does in its 
[mail-preferences.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/mail-preferences.md):

```html
<div data-mail="MailPreferences"></div>
```

Where if it's unable to locate the user will ask the user for their email:

<div class="my-20" data-mail="MailPreferences"></div>

Alternatively the page can jump directly to a contacts Mailing Lists by including a `?ref` query string parameter
of the Contact's External Ref, e.g: `/mail-preferences?ref={{ExternalRef}}`

You can also add `&unsubscribe=1` to optimize the page for users wishing to Unsubscribe where it will also display
an **Unsubscribe** button to subscribe to all mailing lists.

#### MailPreferences Properties

Most of the copy used in the `MailPreferences` component can be overridden with:

```ts
defineProps<{
    emailPrompt?: string            //= Enter your email to manage your email...
    submitEmailLabel?: string       //= Submit
    updatedHeading?: string         //= Updated!
    updatedMessage?: string         //= Your email preferences have been saved.
    unsubscribePrompt?: string      //= Unsubscribe from all future email...
    unsubscribeHeading?: string     //= Updated!
    unsubscribeMessage?: string     //= You've been unsubscribed from all email...
    submitLabel?: string            //= Save Changes
    submitUnsubscribeLabel?: string //= Unsubscribe
}>()
```

## Tailwind Styles

CreatorKit's components are styled with tailwind classes which will also need to be included in your website. 
For Tailwind projects we recommend copying a concatenation of all Components from 
[/CreatorKit/wwwroot/tailwind/all.components.txt](https://raw.githubusercontent.com/NetCoreApps/CreatorKit/main/CreatorKit/wwwroot/tailwind/all.components.txt) 
and include it in your project where the tailwind CLI can find it so any classes used are included in your 
App's Tailwind **.css** bundle.

In Razor SSG projects this is already being copied in its [postinstall.js](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/postinstall.js) 

If you're not using Tailwind, websites will need to reference your CreatorKit's instance Tailwind .css bundle instead, e.g:

```html
<link rel="stylesheet" href="https://creatorkit.netcore.io/css/app.css">
```


# Integrations
Source: https://servicestack.net/creatorkit/integrations

We recommend your website have pages for the following `info.txt` collection variables:

```txt
MailPreferences   {{WebsiteBaseUrl}}/mail-preferences
Unsubscribe       {{WebsiteBaseUrl}}/mail-preferences
Privacy           {{WebsiteBaseUrl}}/privacy
Contact           {{WebsiteBaseUrl}}/#contact
SignupConfirmed   {{WebsiteBaseUrl}}/signup-confirmed
```

You're also free to change the URLs in `info.txt` to reference existing pages on your website where they exist.

The `info.SignupConfirmed` URL is redirected to after a contact verifies their email address.

## Example

For reference here are example pages Razor SSG uses for this URLs:

| Page                                  | Source Code                                                                                                             |
|---------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| [/signup-confirmed](signup-confirmed) | [/_pages/signup-confirmed.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/signup-confirmed.md) |
| [/mail-preferences](mail-preferences) | [/_pages/mail-preferences.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/mail-preferences.md) |
| [/privacy](privacy)                   | [/_pages/privacy.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/privacy.md)                   |
| [/community-rules](community-rules)   | [/_pages/community-rules.md](https://github.com/NetCoreTemplates/razor-ssg/blob/main/MyApp/_pages/community-rules.md)   |


# Overview
Source: https://servicestack.net/creatorkit/portal-overview

All information captured by CreatorKit's components can be managed from your CreatorKit's instance portal at:

<div class="not-prose">
    <h3 class="text-4xl text-center text-indigo-800 font-semibold pb-3"><span class="text-gray-300">https://localhost:5003</span>/portal/</h3>
</div>

Signing in with an Admin User will take you to the dashboard showing your Website activity: 

![](/img/pages/creatorkit/portal.png)

## Mailing List Admin

The first menu section is for managing your contact mailing lists including creating and sending emails and email campaigns
to mailing list contacts,

### Contacts

Mailing List Contacts can either be added via the [JoinMailingList](creatorkit/components#joinmailinglist) component
on your website or using the Contacts Admin UI:

![](/img/pages/creatorkit/portal-contacts.png)

### Archive

When you want to clear your workspace of sent emails you can archive them which moves them to a separate Database ensuring
the current working database is always snappy and clear of clutter. 

![](/img/pages/creatorkit/portal-archive.png)

## Posts Admin

The **Manage Posts** section is for managing and moderating your website's post comments with
most menu items manages data in different Tables using [AutoQueryGrid](https://docs.servicestack.net/vue/autoquerygrid)
and custom [AutoForm](https://docs.servicestack.net/vue/autoform) components.


# Messages
Source: https://servicestack.net/creatorkit/portal-messages

### Sending Single plain-text Emails

**Messages** lets you craft and send emails to a single contact which can be sent immediately or saved as a draft so
you can review the HTML rendered email and send later.

![](/img/pages/creatorkit/portal-messages.png)

It also lists all available emails that can be sent which are any APIs that inherit the `CreateEmailBase` base class
which contains the minimum contact fields required in each email:

```csharp
public abstract class CreateEmailBase
{
    [ValidateNotEmpty]
    [Input(Type="EmailInput")]
    public string Email { get; set; }
    [ValidateNotEmpty]
    [FieldCss(Field = "col-span-6 lg:col-span-3")]
    public string FirstName { get; set; }
    [ValidateNotEmpty]
    [FieldCss(Field = "col-span-6 lg:col-span-3")]
    public string LastName { get; set; }
}
```

Plain text emails can be sent with the `SimpleTextEmail` API:

```csharp
[Renderer(typeof(RenderSimpleText))]
[Tag(Tag.Mail), ValidateIsAdmin]
[Description("Simple Text Email")]
public class SimpleTextEmail : CreateEmailBase, IPost, IReturn<MailMessage>
{
    [ValidateNotEmpty]
    [FieldCss(Field = "col-span-12")]
    public string Subject { get; set; }

    [ValidateNotEmpty]
    [Input(Type = "textarea"), FieldCss(Field = "col-span-12", Input = "h-36")]
    public string Body { get; set; }
    public bool? Draft { get; set; }
}
```

### Email UI

Which are rendered using the [Vue AutoForm component](https://docs.servicestack.net/vue/autoform) from the API
definition where the `SimpleTextEmail` Request DTO renders the new Email UI:

![](/img/pages/creatorkit/portal-messages-simple.png)

Which uses the custom `EmailInput` component to search for contacts and populates their Email, First and Last name fields.

The implementation for sending single emails are defined in
[EmailServices.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.ServiceInterface/EmailServices.cs)
which uses [EmailRenderer.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.ServiceInterface/EmailRenderer.cs)
to save and send non draft emails which follow the pattern below:

```csharp
public EmailRenderer Renderer { get; set; }

public async Task<object> Any(SimpleTextEmail request)
{
    var contact = await Db.GetOrCreateContact(request);
    var viewRequest = request.ConvertTo<RenderSimpleText>().FromContact(contact);
    var bodyText = (string) await Gateway.SendAsync(typeof(string), viewRequest);
    
    var email = await Renderer.CreateMessageAsync(Db, new MailMessage
    {
        Draft = request.Draft ?? false,
        Message = new EmailMessage
        {
            To = contact.ToMailTos(),
            Subject = request.Subject,
            Body = request.Body,
            BodyText = bodyText,
        },
    }.FromRequest(request));
    return email;
}
```

Live previews are generated and Emails rendered with renderer APIs that inherit `RenderEmailBase` e.g:

```csharp
[Tag(Tag.Mail), ValidateIsAdmin, ExcludeMetadata]
public class RenderSimpleText : RenderEmailBase, IGet, IReturn<string>
{
    public string Body { get; set; }
}
```

Which renders the Request DTO inside a [#Script](https://sharpscript.net) email context:

```csharp
public async Task<object> Any(RenderSimpleText request)
{
    var ctx = Renderer.CreateScriptContext();
    return await ctx.RenderScriptAsync(request.Body,request.ToObjectDictionary());
}
```

### Sending Custom HTML Emails

`CustomHtmlEmail` is a configurable API for sending HTML emails utilizing custom Email Layout and Templates
from populated dropdowns configured with available Templates in `/emails`:

```csharp
[Renderer(typeof(RenderCustomHtml))]
[Tag(Tag.Mail), ValidateIsAdmin]
[Icon(Svg = Icons.RichHtml)]
[Description("Custom HTML Email")]
public class CustomHtmlEmail : CreateEmailBase, IPost, IReturn<MailMessage>
{
    [ValidateNotEmpty]
    [Input(Type = "combobox", EvalAllowableValues = "AppData.EmailLayoutOptions")]
    public string Layout { get; set; }
    
    [ValidateNotEmpty]
    [Input(Type = "combobox", EvalAllowableValues = "AppData.EmailTemplateOptions")]
    public string Template { get; set; }
    
    [ValidateNotEmpty]
    [FieldCss(Field = "col-span-12")]
    public string Subject { get; set; }

    [Input(Type = "MarkdownEmailInput", Label = ""), FieldCss(Field = "col-span-12", Input = "h-56")]
    public string? Body { get; set; }
    public bool? Draft { get; set; }
}
```

![](/img/pages/creatorkit/portal-messages-custom.png)

#### Custom HTML Implementation

It follows the same pattern as other email implementations where it uses the `EmailRenderer` to create and send emails:

```csharp
public async Task<object> Any(CustomHtmlEmail request)
{
    var contact = await Db.GetOrCreateContact(request);
    var viewRequest = request.ConvertTo<RenderCustomHtml>().FromContact(contact);
    var bodyHtml = (string) await Gateway.SendAsync(typeof(string), viewRequest);

    var email = await Renderer.CreateMessageAsync(Db, new MailMessage
    {
        Draft = request.Draft ?? false,
        Message = new EmailMessage
        {
            To = contact.ToMailTos(),
            Subject = request.Subject,
            Body = request.Body,
            BodyHtml = bodyHtml,
        },
    }.FromRequest(viewRequest));
    return email;
}
```

Which uses the `RenderCustomHtml` to render the HTML and Live Previews which executes the populated Request DTO with
the Email **#Script** context configured to use the selected Email Layout and Template:

```csharp
public async Task<object> Any(RenderCustomHtml request)
{
    var context = Renderer.CreateMailContext(layout:request.Layout, page:request.Template);
    var evalBody = !string.IsNullOrEmpty(request.Body) 
        ? await context.RenderScriptAsync(request.Body, request.ToObjectDictionary())
        : string.Empty;

    return await Renderer.RenderToHtmlResultAsync(Db, context, request, 
        args:new() {
            ["body"] = evalBody,
        });
}
```

## CreatorKit.Extensions

Any additional services should be maintained in [CreatorKit.Extensions](https://github.com/NetCoreApps/CreatorKit/tree/main/CreatorKit.Extensions) 
project with any custom email implementations added to 
[CustomEmailServices.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.Extensions/CustomEmailServices.cs).

### Sending HTML Markdown Emails

[MarkdownEmail.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.Extensions.ServiceModel/MarkdownEmail.cs)
is an example of a more user-friendly custom HTML Email you may want to send, which is pre-configured to use the
[basic.html](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/emails/layouts/basic.html)
Layout and the
[empty.html](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/emails/empty.html)
Email Template to allow sending plain HTML Emails with a custom Markdown Email body:

```csharp
[Renderer(typeof(RenderCustomHtml), Layout = "basic", Template="empty")]
[Tag(Tag.Mail), ValidateIsAdmin]
[Icon(Svg = Icons.TextMarkup)]
[Description("Markdown Email")]
public class MarkdownEmail : CreateEmailBase, IPost, IReturn<MailMessage>
{
    [ValidateNotEmpty]
    [FieldCss(Field = "col-span-12")]
    public string Subject { get; set; }

    [ValidateNotEmpty]
    [Input(Type="MarkdownEmailInput",Label=""), FieldCss(Field="col-span-12",Input="h-56")]
    public string? Body { get; set; }
    public bool? Draft { get; set; }
}
```

As defined, this DTO renders the form utilizing a custom `MarkdownEmailInput` rich text editor which provides an optimal UX 
for authoring Markdown content with icons to assist with discovery of Markdown's different formatting syntax.

#### Template Variables

The editor also includes a dropdown to provide convenient access to your [Template Variables](creatorkit/customize#template-variables):

![](/img/pages/creatorkit/portal-messages-markdown.png)

The implementation of `MarkdownEmail` just sends a Custom HTML Email configured to use the **basic** Layout with the **empty** Email Template:

```csharp
public async Task<object> Any(MarkdownEmail request)
{
    var contact = await Db.GetOrCreateContact(request);
    var viewRequest = request.ConvertTo<RenderCustomHtml>().FromContact(contact);
    viewRequest.Layout = "basic";
    viewRequest.Template = "empty";
    var bodyHtml = (string) await Gateway.SendAsync(typeof(string), viewRequest);

    var email = await Renderer.CreateMessageAsync(Db, new MailMessage
    {
        Draft = request.Draft ?? false,
        Message = new EmailMessage
        {
            To = contact.ToMailTos(),
            Subject = request.Subject,
            Body = request.Body,
            BodyHtml = bodyHtml,
        },
    }.FromRequest(viewRequest));
    return email;
}
```


# Mail Runs
Source: https://servicestack.net/creatorkit/portal-mailruns

Mail Runs is where you would go to craft and send emails to an entire Mailing List group. It has the same Simple,
Markdown and Custom HTML Email UIs as [Messages](creatorkit/portal-messages) except instead of a single contact, 
it will generate and send individual emails to every contact in the specified **Mailing List**:

![](/img/pages/creatorkit/portal-mailrun-custom.png)

You'll also be able to send personalized emails with the contact's `{{Email}}`, `{{FirstName}}` and `{{LastName}}`
template variables.

### MailRun Implementation

All Mail Run APIs inherit `MailRunBase` which contains the Mailing List that the Mail Run should send emails to:

```csharp
public abstract class MailRunBase
{
    [ValidateNotEmpty]
    public MailingList MailingList { get; set; }
}
```

It has the equivalent Standard, Markdown and Custom HTML Emails that messages has, which instead inherits from `MailRunBase`,
e.g. here's the Request DTO definition that's used to render the above **Custom HTML Email** UI:

```csharp
[Renderer(typeof(RenderCustomHtml))]
[Tag(Tag.Mail), ValidateIsAdmin]
[Icon(Svg = Icons.RichHtml)]
[Description("Custom HTML Email")]
public class CustomHtmlMailRun : MailRunBase, IPost, IReturn<MailRunResponse>
{
    [ValidateNotEmpty]
    [Input(Type = "combobox", EvalAllowableValues = "AppData.EmailLayoutOptions")]
    public string Layout { get; set; }
    [ValidateNotEmpty]
    [Input(Type = "combobox", EvalAllowableValues = "AppData.EmailTemplateOptions")]
    public string Template { get; set; }
    [ValidateNotEmpty]
    public string Subject { get; set; }
    [ValidateNotEmpty]
    [Input(Type = "MarkdownEmailInput", Label = ""), FieldCss(Field = "col-span-12", Input = "h-56")]
    public string? Body { get; set; }
}
```

It's implementation differs slightly from the Messages
[Custom HTML Implementation](creatorkit/portal-messages#custom-html-implementation) as an email needs to be generated
and sent per contact and are instead generated and saved to the `MailMessageRun` table:

```csharp
public async Task<object> Any(CustomHtmlMailRun request)
{
    var response = CreateMailRunResponse();
    
    var mailRun = await Renderer.CreateMailRunAsync(Db, new MailRun {
        Layout = request.Layout,
        Template = request.Template,
    }, request);
    
    foreach (var sub in await Db.GetActiveSubscribersAsync(request.MailingList))
    {
        var viewRequest = request.ConvertTo<RenderCustomHtml>().FromContact(sub);
        var bodyHtml = (string) await Gateway.SendAsync(typeof(string), viewRequest);

        response.AddMessage(await Renderer.CreateMessageRunAsync(Db, new MailMessageRun
        {
            Message = new EmailMessage
            {
                To = sub.ToMailTos(),
                Subject = request.Subject,
                Body = request.Body,
                BodyHtml = bodyHtml,
            }
        }.FromRequest(viewRequest), mailRun, sub));
    }
    
    await Db.CompletedMailRunAsync(mailRun, response);
    return response;
}
```

### Verifying Mail Run Messages

Creating a Mail Run generates messages for each Contact in the Mailing List, but doesn't send them immediately, 
it instead opens the saved Mail Run so you have an opportunity to inspect the generated messages to decide whether
you want to send or delete the messages.

![](/img/pages/creatorkit/portal-mailrun-newsletter-send.png)

Click **View Messages** to inspect a sample of the generated messages from the saved Mail Run then either 
**Send Messages** if you want to send them out or **Delete** to delete the Mail Run and start again.

Whilst the Mail Run Messages are being sent out you can click Refresh to monitor progress. 

## CreatorKit.Extensions

Any additional services should be maintained in [CreatorKit.Extensions](https://github.com/NetCoreApps/CreatorKit/tree/main/CreatorKit.Extensions)
project with any custom Mail Run implementations added to
[CustomEmailRunServices.cs](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.Extensions/CustomEmailRunServices.cs).

## Generating Newsletters

The `NewsletterMailRun` API is an advanced Email Generation example for generating a Monthly Newsletter - that it automatically 
generates from new content added to [Razor SSG](https://razor-ssg.web-templates.io/posts/razor-ssg) Websites that it 
discovers from its pre-rendered API JSON metadata.

Even if you're not using Razor SSG website it should still serve as a good example for how to implement a Mail Run for
a custom mail campaign utilizing custom data sources.

The `NewsletterMailRun` API has 2 optional properties for the Year and Month you want to generate the Newsletter for:

```csharp
[Renderer(typeof(RenderNewsletter))]
[Tag(Tag.Emails)]
[ValidateIsAdmin]
[Description("Generate Newsletter")]
[Icon(Svg = Icons.Newsletter)]
public class NewsletterMailRun : MailRunBase, IPost, IReturn<MailRunResponse>
{
    public int? Month { get; set; }
    public int? Year { get; set; }
}
```

Which renders the **Generate Newsletter** UI: 

![](/img/pages/creatorkit/portal-mailrun-newsletter.png)

The implementation follows the standard Mail Run implementation, using the `EmailRenderer` to creating a `MailMessageRun`
for every contact in the mailing list. 

We can also see it will default to the current Month/Year if not provided and that it uses the 
[marketing.html](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/emails/layouts/marketing.html) Layout and
the [newsletter.html](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit/emails/newsletter.html)
Email template:

```csharp
public async Task<object> Any(NewsletterMailRun request)
{
    var response = CreateMailRunResponse();
    request.Year ??= DateTime.UtcNow.Year;
    request.Month ??= DateTime.UtcNow.Month;

    var viewRequest = request.ConvertTo<RenderNewsletter>();
    var fromDate = new DateTime(request.Year.Value, request.Month.Value, 1);
    var bodyHtml = (string) await Gateway.SendAsync(typeof(string), viewRequest);

    var mailRun = await Renderer.CreateMailRunAsync(Db, new MailRun {
        Layout = "marketing",
        Template = "newsletter",
    }, request);
    
    foreach (var sub in await Db.GetActiveSubscribersAsync(request.MailingList))
    {
        response.AddMessage(await Renderer.CreateMessageRunAsync(Db, new MailMessageRun
        {
            Message = new EmailMessage
            {
                To = sub.ToMailTos(),
                Subject = string.Format(AppData.Info.NewsletterFmt, 
                    $"{fromDate:MMMM} {fromDate:yyyy}"),
                BodyHtml = bodyHtml,
            }
        }.FromRequest(viewRequest), mailRun, sub));
    }

    await Db.CompletedMailRunAsync(mailRun, response);
    return response;
}
```

The **newsletter.html** Email Template uses the #Script templating language to render the different Newsletter sections, e.g:

```html
{{#if meta.Posts.Count > 0 }}
{{ 'divider' |> partial }}
{{ 'section' |> partial({ iconSrc:images.blog_48x48, title:'New Posts' }) }}
<tr>
    <td width="100%" align="left" valign="top">
        {{#each meta.Posts }}
            <h3><a href="{{ it.url }}" target="_blank">{{ it.title }} →</a></h3>
            <p>{{ it.summary }}</p>
        {{/each}}
    </td>
</tr>
{{/if}}
```

Which uses the `RenderNewsletter` API to render the Newsletter emails and live previews which in addition to the App's
template variables adds a `meta` property containing the Data Source for the contents in **newsletter.html**:

```csharp
public async Task<object> Any(RenderNewsletter request)
{
    var year = request.Year ?? DateTime.UtcNow.Year;
    var fromDate = new DateTime(year, request.Month ?? 1, 1);
    var meta = await MailData.SearchAsync(fromDate: fromDate,
        toDate: request.Month != null ? new DateTime(year, request.Month.Value, 1).AddMonths(1) : null);
    
    var context = Renderer.CreateMailContext(layout:"marketing", page:"newsletter", 
        args:new() {
            ["meta"] = meta
        });

    return await Renderer.RenderToHtmlResultAsync(Db, context, request, args: new() {
        ["title"] = $"{fromDate:MMMM} {fromDate:yyyy}"
    });
}
```

The implementation of [MailData](https://github.com/NetCoreApps/CreatorKit/blob/main/CreatorKit.ServiceInterface/MailData.cs)
gets its data from [/meta/2023/all.json](https://razor-ssg.web-templates.io/meta/2023/all.json) which is prerendered with 
all the new website content added in **2023** which is filtered further to only include content published within the 
selected date range:


```csharp
public class MailData
{
    public DateTime LastUpdated { get; set; }
    public AppData AppData { get; }

    public MailData(AppData appData)
    {
        AppData = appData;
    }

    public TimeSpan CacheDuration { get; set; } = TimeSpan.FromMinutes(10);
    public ConcurrentDictionary<int, SiteMeta> MetaCache { get; } = new();

    public async Task<SiteMeta> SearchAsync(DateTime? fromDate = null, DateTime? toDate = null)
    {
        var year = fromDate?.Year ?? DateTime.UtcNow.Year;
        var metaCache = MetaCache.TryGetValue(year, out var siteMeta) 
            && siteMeta.CreatedDate < DateTime.UtcNow.Add(CacheDuration)
            ? siteMeta
            : null;

        if (metaCache == null)
        {
            var metaJson = await AppData.BaseUrl.CombineWith($"/meta/{year}/all.json")
                .GetJsonFromUrlAsync();
            metaCache = metaJson.FromJson<SiteMeta>();
            metaCache.CreatedDate = DateTime.UtcNow;
            MetaCache[year] = metaCache;
        }

        var results = new SiteMeta
        {
            CreatedDate = metaCache.CreatedDate,
            Pages = WithinRange(metaCache.Pages, fromDate, toDate).ToList(),
            Posts = WithinRange(metaCache.Posts, fromDate, toDate).ToList(),
            WhatsNew = WithinRange(metaCache.WhatsNew, fromDate, toDate).ToList(),
            Videos = WithinRange(metaCache.Videos, fromDate, toDate).ToList(),
        };
        return results;
    }

    private static IEnumerable<MarkdownFile> WithinRange(
        IEnumerable<MarkdownFile> docs, DateTime? fromDate, DateTime? toDate)
    {
        if (fromDate != null)
            docs = docs.Where(x => x.Date >= fromDate);
        if (toDate != null)
            docs = docs.Where(x => x.Date < toDate);
        return docs;
    }
}
```

The results of the external API Request are also cached for a short duration to speed up Live Previews when crafting
emails.


# Posts
Source: https://servicestack.net/creatorkit/portal-posts

The **Manage Posts** section provides editable [AutoQuery Grid components](https://docs.servicestack.net/vue/autoquerygrid) 
to manage all the RDBMS tables used to implement CreatorKit's comment system like **Threads** which manages the `Thread` 
table which supports adding Thread comments to every unique URL:

![](/img/pages/creatorkit/portal-threads.png)

## Moderation

Most of the time will be spent either reading through and deleting bad comments and responding to reported comments 
which includes special behavior to filter reports to only show reports that have yet to be moderated.

The **Update Comment Report** includes different moderation options you can choose to perform based on the severity of
a comment ranging from flagging a comment which marks the comment as flagged and hides the comment content, deleting
the comment which also deletes any replies, temporarily banning the user for a day, a week, a month to permanently
banning the user until they're explicitly unbanned:

![](/img/pages/creatorkit/portal-report.png)

Thread Users are managed with the [User Admin Feature](https://docs.servicestack.net/admin-ui-users) UI who can be
banned for any duration up to **Ban Until Date** or permanently banned by Locking the User Account:

![](/img/pages/creatorkit/admin-users.png)


