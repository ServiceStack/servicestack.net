---
title: Introducing AI Server
summary: Introducing AI Server - an OSS Self Hosted Gateway for running LLM, Ollama, Media and Comfy UI APIs
tags: [ai-server,ai,gpt,service-reference,c#,js]
author: Demis Bellot
image: https://images.unsplash.com/photo-1642516303080-431f6681f864?crop=entropy&fit=crop&h=1000&w=2000
---

## AI Server now ready to serve!

We're excited to announce the first release of AI Server - a Free OSS self-hosted Docker private gateway to 
manage API access to multiple LLM APIs, Ollama endpoints, Media APIs, Comfy UI and FFmpeg Agents.

:::youtube Ojo80oFQte8
Introducing AI Server
:::

### Centralized Management

Designed as a one-stop solution to manage an organization's AI integrations for all their System Apps,
by utilizing developer friendly HTTP JSON APIs that supports any programming language or framework.

[![](/img/svgs/ai-server-overview.svg)](https://openai.servicestack.net)

### Distribute load across multiple Ollama, Open AI Gateway and Comfy UI Agents

It works as a private gateway to process LLM, AI and image transformations requests
that any of our Apps need where it dynamically load balances requests across our local GPU Servers, Cloud GPU instances
and API Gateways running multiple instances of Ollama, Open AI Chat, LLM Gateway, Comfy UI, Whisper
and ffmpeg providers.

In addition to maintaining a history of AI Requests, it also provides file storage
for its CDN-hostable AI generated assets and on-the-fly, cacheable image transformations.

### Native Typed Integrations

Uses [Add ServiceStack Reference](https://docs.servicestack.net/add-servicestack-reference) to enable
simple, native typed integrations for most popular Web, Mobile and Desktop languages including: 
C#, TypeScript, JavaScript, Python, Java, Kotlin, Dart, PHP, Swift, F# and VB.NET.

Each AI Feature supports multiple call styles for optimal integration of different usages:

- **Synchronous API** · Simplest API ideal for small workloads where the Response is returned in the same Request
- **Queued API** · Returns a reference to the queued job executing the AI Request which can be used to poll for the API Response
- **Reply to Web Callback** · Ideal for reliable App integrations where responses are posted back to a custom URL Endpoint

### Live Monitoring and Analytics

Monitor performance and statistics of all your App's AI Usage, real-time logging of executing APIs with auto archival 
of completed AI Requests into monthly rolling SQLite databases.

### Protected Access with API Keys

AI Server utilizes [Simple Auth with API Keys](https://docs.servicestack.net/auth/admin-apikeys)
letting Admins create and distribute API Keys to only allow authorized clients to access their
AI Server's APIs, which can be optionally further restricted to only
[allow access to specific APIs](https://docs.servicestack.net/auth/apikeys#creating-user-api-keys).

## Install

AI Server can be installed on macOS and Linux with Docker by running [install.sh](https://github.com/ServiceStack/ai-server/blob/main/install.sh):

1. Clone the AI Server repository from GitHub:

:::sh
`git clone https://github.com/ServiceStack/ai-server`
:::

2. Run the Installer

:::sh
`cd ai-server && cat install.sh | bash`
:::

The installer will detect common environment variables for the supported AI Providers like OpenAI, Google, Anthropic, 
and others, and prompt ask you if you want to add them to your AI Server configuration.

<ascii-cinema src="https://docs.servicestack.net/pages/ai-server/ai-server-install.cast"
  loop="true" poster="npt:00:21" theme="dracula" rows="12" />

### Optional - Install ComfyUI Agent

If your server also has a GPU you can ask the installer to also install the [ComfyUI Agent](/ai-server/comfy-extension):

<ascii-cinema src="https://docs.servicestack.net/pages/ai-server/agent-comfy-install.cast"
    loop="true" poster="npt:00:21" theme="dracula" rows="16" />

The ComfyUI Agent is a separate Docker agent for running [ComfyUI](https://www.comfy.org), 
[Whisper](https://github.com/openai/whisper) and [FFmpeg](https://www.ffmpeg.org) on servers with GPUs to handle 
AI Server's [Image](https://docs.servicestack.net/ai-server/transform/image) and 
[Video transformations](https://docs.servicestack.net/ai-server/transform/video) and Media Requests, including:

- [Text to Image](https://docs.servicestack.net/ai-server/text-to-image)
- [Image to Text](https://docs.servicestack.net/ai-server/image-to-text)
- [Image to Image](https://docs.servicestack.net/ai-server/image-to-image)
- [Image with Mask](https://docs.servicestack.net/ai-server/image-with-mask)
- [Image Upscale](https://docs.servicestack.net/ai-server/image-upscale)
- [Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text)
- [Text to Speech](https://docs.servicestack.net/ai-server/text-to-speech)

#### Comfy UI Agent Installer

To install the ComfyUI Agent on a separate server (with a GPU), you can clone and run the ComfyUI Agent installer 
on that server instead:

1. Clone the Comfy Agent

:::sh
`git clone https://github.com/ServiceStack/agent-comfy.git`
:::

2. Run the Installer

:::sh
`cd agent-comfy && cat install.sh | bash`
:::

## Running in Production

We've been developing and running AI Server for several months now, processing millions of LLM and Comfy UI Requests
to generate Open AI Chat Answers and Generated Images used to populate the
[pvq.app](https://pvq.app) and [blazordiffusion.com](https://blazordiffusion.com) websites.

Our production instance with more info about AI Server is available at:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
https://openai.servicestack.net
:::

[![](/img/posts/ai-server/ai-server-languages.png)](https://openai.servicestack.net)

## API Explorer

Whilst our production instance is protected by API Keys, you can still use it to explore available APIs in its API Explorer:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
[https://openai.servicestack.net/ui/](https://openai.servicestack.net/ui/OpenAiChatCompletion)
:::

## Documentation

The documentation for AI Server is being maintained at:

:::{.m-0 .text-center .text-2xl .font-semibold .text-indigo-600}
https://docs.servicestack.net/ai-server/
:::

## Built-in UIs

Built-in UIs allow users with API Keys access to custom UIs for different AI features

[![](/img/posts/ai-server/ai-server-builtin-uis.png)](https://openai.servicestack.net)

## Admin UIs

Use Admin UI to manage API Keys that can access AI Server APIs and Features

[![](/img/posts/ai-server/ai-server-admin-uis.png)](https://openai.servicestack.net)

## Features

The current release of AI Server supports a number of different modalities, including:

### Large Language Models
- [Open AI Chat](https://docs.servicestack.net/ai-server/chat)
    - Support for Ollama endpoints
    - Support for Open Router, Anthropic, Open AI, Mistral AI, Google and Groq API Gateways

### Comfy UI Agent / Replicate / DALL-E 3
 
- [Text to Image](https://docs.servicestack.net/ai-server/text-to-image)

### Comfy UI Agent
 
- [Image to Image](https://docs.servicestack.net/ai-server/image-to-image)
    - [Image Upscaling](https://docs.servicestack.net/ai-server/image-upscale)
    - [Image with Mask](https://docs.servicestack.net/ai-server/image-with-mask)
- [Image to Text](https://docs.servicestack.net/ai-server/image-to-text)
- [Text to Speech](https://docs.servicestack.net/ai-server/text-to-speech)
- [Speech to Text](https://docs.servicestack.net/ai-server/speech-to-text)
 
### FFmpeg

- [Image Transformations](https://docs.servicestack.net/ai-server/transform/image)
  - **Crop Image** - Crop an image to a specific size
  - **Convert Image** - Convert an image to a different format
  - **Scale Image** - Scale an image to a different resolution
  - **Watermark Image** - Add a watermark to an image

- [Video Transformations](https://docs.servicestack.net/ai-server/transform/video)
  - **Crop Video** - Crop a video to a specific size
  - **Convert Video** - Convert a video to a different format
  - **Scale Video** - Scale a video to a different resolution
  - **Watermark Video** - Add a watermark to a video
  - **Trim Video** - Trim a video to a specific length

### Managed File Storage

- Blob Storage - isolated and restricted by API Key

## AI Server API Examples

To simplify integrations with AI Server each API Request can be called with 3 different call styles to better
support different use-cases and integration patterns.

### Synchronous Open AI Chat Example

The **Synchronous API** is the simplest API ideal for small workloads where the Response is returned in the same Request:

```csharp
var client = new JsonApiClient(baseUrl);
client.BearerToken = apiKey;

var response = client.Post(new OpenAiChatCompletion {
    Model = "mixtral:8x22b",
    Messages = [
        new() {
            Role = "user",
            Content = "What's the capital of France?"
        }
    ],
    MaxTokens = 50
});

var answer = response.Choices[0].Message.Content;
```

### Synchronous Media Generation Request Example

Other AI Requests can be called synchronously in the same way where its API is named after the modality
it implements, e.g. you'd instead call `TextToImage` to generate an Image from a Text description:

```csharp
var response = client.Post(new TextToImage
    PositivePrompt = "A serene landscape with mountains and a lake",
    Model = "flux-schnell",
    Width = 1024,
    Height = 1024,
    BatchSize = 1
});

File.WriteAllBytes(saveToPath, response.Results[0].Url.GetBytesFromUrl());
```

### Queued Open AI Chat Example

The **Queued API** immediately Returns a reference to the queued job executing the AI Request:

```csharp
var response = client.Post(new QueueOpenAiChatCompletion
{
    Request = new()
    {
        Model = "gpt-4-turbo",
        Messages = [
            new() { Role = "system", Content = "You are a helpful AI assistant." },
            new() { Role = "user", Content = "How do LLMs work?" }
        ],
        MaxTokens = 50
    }
});
```

Which can be used to poll for the API Response of any Job by calling `GetOpenAiChatStatusResponse`
and checking when its state has finished running to get the completed `OpenAiChatResponse`:

```csharp
GetOpenAiChatStatusResponse status = new();
while (status.JobState is BackgroundJobState.Started or BackgroundJobState.Queued)
{
    status = await client.GetAsync(new GetOpenAiChatStatus { RefId = response.RefId });
    await Task.Delay(1000);
}

var answer = status.Result.Choices[0].Message.Content;
```

### Queued Media Artifact Generation Request Example

Most other AI Server Requests are Artifact generation requests which would instead call 
`GetArtifactGenerationStatus` to get the artifacts response of a queued job, e.g:

```csharp
var response = client.Post(new QueueTextToImage {
    PositivePrompt = "A serene landscape with mountains and a lake",
    Model = "flux-schnell",
    Width = 1024,
    Height = 1024,
    BatchSize = 1
});

// Poll for Job Completion Status
GetArtifactGenerationStatusResponse status = new();
while (status.JobState is BackgroundJobState.Queued or BackgroundJobState.Started)
{
    status = client.Get(new GetArtifactGenerationStatus { JobId = response.JobId });
    Thread.Sleep(1000);
}

File.WriteAllBytes(saveToPath, status.Results[0].Url.GetBytesFromUrl());
```

### Queued Media Text Generation Request Example

Whilst the Media API Requests that generates text like `SpeechToText` or `ImageToText` would instead call
`GetTextGenerationStatus` to get the text response of a queued job, e.g:

```csharp
using var fsAudio = File.OpenRead("files/test_audio.wav");
var response = client.PostFileWithRequest(new QueueSpeechToText(),
    new UploadFile("test_audio.wav", fsAudio, "audio"));

// Poll for Job Completion Status
GetTextGenerationStatusResponse status = new();
while (status.JobState is BackgroundJobState.Started or BackgroundJobState.Queued)
{
    status = client.Get(new GetTextGenerationStatus { RefId = response.RefId });
    Thread.Sleep(1000);
}

var answer = status.Results[0].Text;
```

### Open AI Chat with Callback Example

The Queued API also accepts a **Reply to Web Callback** for a more reliable push-based App integration
where responses are posted back to a custom URL Endpoint:

```csharp
var correlationId = Guid.NewGuid().ToString("N");
var response = client.Post(new QueueOpenAiChatCompletion
{
    //...
    ReplyTo = $"https://example.org/api/OpenAiChatResponseCallback?CorrelationId=${correlationId}"
});
```

Your callback can add any additional metadata on the callback to assist your App in correlating the response with 
the initiating request which just needs to contain the properties of the `OpenAiChatResponse` you're interested in
along with any metadata added to the callback URL, e.g:

```csharp
public class OpenAiChatResponseCallback : IPost, OpenAiChatResponse, IReturnVoid
{
    public Guid CorrelationId { get; set; }
}

public object Post(OpenAiChatResponseCallback request)
{
    // Handle OpenAiChatResponse callabck
}
```

Unless your callback API is restricted to only accept requests from your AI Server, you should include a 
unique Id like a `Guid` in the callback URL that can be validated against an initiating request to ensure 
the callback can't be spoofed.

## Feedback

Feel free to reach us at [ServiceStack/Discuss](https://github.com/ServiceStack/Discuss/discussions)
if you have any questions about AI Server.