---
title: AI Server - April 2025 Update
summary: New SOTA LLMs added, support for thinking responses, Ollama Vision Models & Generate API
tags: [ai-server,ai,gpt,service-reference,c#,js]
author: Demis Bellot
image: ./img/posts/ai-server/bg.webp
---

AI Server is our Free OSS Docker self-hosted private gateway to manage API access to multiple LLM APIs, Ollama endpoints, Media APIs, Comfy UI and FFmpeg Agents that's designed as a one-stop solution to manage an organization's AI integrations for all their System Apps with its developer friendly HTTP JSON APIs that supports any programming language or framework.

[![](/img/svgs/ai-server-overview.svg)](https://openai.servicestack.net)

:::youtube Ojo80oFQte8
Introducing AI Server
:::

## Built in Analytics & Logging

We've brought [comprehensive Analytics](https://docs.servicestack.net/admin-ui-analytics) added in [ServiceStack v5.7](https://docs.servicestack.net/releases/v8_07) 
into AI Server's Admin UI to provide deep and invaluable insight into your System API Usage, device distribution, 
API Keys and identify IPs where most traffic generates.

[![](/img/posts/ai-server-2025-04/ai-server-analytics.webp)](https://docs.servicestack.net/admin-ui-analytics)

For even finer grained detail of your AI Server's usage we've also surfaced the [SQLite Request Logs](https://docs.servicestack.net/sqlite-request-logs) 
functionality inside AI Server's Admin UI which lets you monitor your API System usage in real-time:

[![](/img/posts/ai-server-2025-04/ai-server-logging.webp)](https://docs.servicestack.net/admin-ui-profiling)

## Support for new Models

This release continues to see a number of improvements to AI Server starting with adding support for popular LLM models added during this release, including:

- **Google:** gemini-pro-2.5, gemini-flash-2.0, gemini-flash-lite-2.0, gemini-flash-thinking-2.0, gemma3
- **OpenAI:** o3, o4-mini, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano
- **Alibaba:** qwen2.5, qwen2.5-coder, qwen-turbo, qwen-plus, qwen-max
- **Meta:** llama-4-scout, llama-4-maverick
- **Microsoft:** phi4
- **Mistral:** mistral-small, mistral-saba

## Custom OpenAI Endpoints

Prior to this release AI Requests were only able to reference LLM Models defined in AI Server's pre-defined 
[ai-models.json](https://github.com/ServiceStack/ai-server/blob/main/AiServer/wwwroot/lib/data/ai-models.json)
or included in your [overrides folder](https://docs.servicestack.net/ai-server/configuration#custom-definitions).

Model definitions facilitate load balancing requests for a specific model across multiple AI Providers
utilizing different model names back to a single model definition.

This approach makes it harder to run custom or fine-tuned models from OpenAI Compatible Chat Endpoints 
which don't map to an existing model definition. To better support these use-cases AI Server now supports
registering a custom OpenAI Chat compatible endpoints with custom models with the new **Custom** AI Provider Type:

![](/img/posts/ai-server-2025-04/custom-openai-provider.webp)

This now lets AI Server connect with other OpenAI Compatible API's like llama.cpp's
[llama-server](https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#llama-server), e.g:

:::sh
CUDA_VISIBLE_DEVICES=0 llama-server --model ./models/phi-4-q4.gguf -ngl 999 --port 8080
:::

Which launches an OpenAI compatible API configured to serve a single model that's run entirely on an 
NVidia GPU at port **8080**. 

As it's only configured to serve a single model you can choose to configure it with any model name you which 
which llama-server ignores but AI Server uses to route any AI requests for that model to the custom AI Provider
instance which you can try in [AI Server's Chat UI](https://docs.servicestack.net/ai-server/chat):

![](/img/posts/ai-server-2025-04/custom-openai-provider-chat.webp)

## Support for Ollama Vision Models

By default [ImageToText](/ai-server/image-to-text) uses a purpose-specific **Florence 2 Vision model** with ComfyUI for its functionality which is capable of generating a very short description about an image, e.g:

> A woman sitting on the edge of a lake with a wolf

But with LLMs gaining multi modal capabilities and Ollama's recent support of Vision Models we can instead use popular
Open Source models like Google's **gemma3:27b** or Mistral's **mistral-small:24b** to extract information from images.

Both models are very capable vision models that's can provide rich detail about an image:

### Describe Image

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="/img/posts/ai-server-2025-04/gemma3-describe.png">
        <img class="p-2" src="/img/posts/ai-server-2025-04/gemma3-describe.png" />
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="/img/posts/ai-server-2025-04/mistral-small-describe.png">
        <img class="p-2" src="/img/posts/ai-server-2025-04/mistral-small-describe.png" />
    </a>
</div>

### Caption Image

Although our initial testing sees gemma being better at responding to a wide variety of different prompts, e.g:

<div class="not-prose mt-8 grid grid-cols-2 gap-4">
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="/img/posts/ai-server-2025-04/gemma3-caption.png">
        <img class="p-2" src="/img/posts/ai-server-2025-04/gemma3-caption.png" />
    </a>
    <a class="block group border dark:border-gray-800 hover:border-indigo-700 dark:hover:border-indigo-700 flex flex-col justify-between" href="/img/posts/ai-server-2025-04/mistral-small-caption.png">
        <img class="p-2" src="/img/posts/ai-server-2025-04/mistral-small-caption.png" />
    </a>
</div>

## New OllamaGenerate Endpoints

To support Ollama's vision models AI Server added a new feature pipeline around
[Ollama's generate completion API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion):

- [ImageToText](https://openai.servicestack.net/ui/ImageToText)
  - **Model** - Whether to use a Vision Model for the request
  - **Prompt** - Prompt for the vision model
- [OllamaGeneration](https://openai.servicestack.net/ui/OllamaGeneration): Synchronous invocation of Ollama's Generate API
- [QueueOllamaGeneration](https://openai.servicestack.net/ui/QueueOllamaGeneration): Asynchronous or Web Callback invocation of Ollama's Generate API
- [GetOllamaGenerationStatus](https://openai.servicestack.net/ui/GetOllamaGenerationStatus): Get the generation status of an Ollama Generate API

## Support for Thinking Responses

With the rise and popularity of **Thinking** Models we've added custom rendering of *thinking* responses in a collapsible
and scrollable container:

[![](/img/posts/ai-server-2025-04/ai-server-thinking.png)](https://docs.servicestack.net/ai-server/chat)

